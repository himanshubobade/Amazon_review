[
	{
		"document": "1 Introduction \nLarge language models (LLMs) (Brown et al., 2020; Smith et al., 2022; Chowdhery et al., 2022; Zhang et al.,2022) have demonstrated unprecedented capabilities in language comprehension and generation, leading tobreakthroughs in reasoning, math, science, and many other tasks (OpenAI, 2023; Anil et al., 2023). However,training LLMs is extremely costly. For example, PaLM takes 6,144 TPUv4 chips to train a 540B model, whileGPT-3 175B consumes several thousand petaflop/s-days of compute for pre-training (Chowdhery et al., 2022;Brown et al., 2020). This motivates the needs of reducing the training costs of LLMs, especially for the scalingof next-generation super-intelligent models.Low-precision training is one of the most promising directions to reduce the costs, as it can provide highspeed, small memory footprint, and low communication overhead. Most existing training systems, e.g.,Megatron-LM (Shoeybi et al., 2019), MetaSeq (Zhang et al., 2022), and Colossal-AI (Li et al., 2023a), trainLLMs with either FP32 full-precision or FP16/BF16 mixed-precision by default. This is not essential, however, to achieve full accuracy for large models. With the release of Nvidia H100 GPU, FP8 is becoming the nextgeneration datatype for low-precision representation (Nvidia, 2022a; Micikevicius et al., 2022). Theoretically,FP8 can achieve 2x speed-up, 50% - 75% memory cost savings, and 50% - 75% communication savingscompared with current 16-bit and 32-bit floating point mixed-precision training, which is very promising forscaling-up next-generation foundation models.Unfortunately, the current support for FP8 training is rare and limited. The only usable framework is theNvidia Transformer Engine (TE) (Nvidia, 2022b), but it applies FP8 solely for GEMM computation and stillretains master weights and gradients using high precision, e.g., FP16 or FP32. As a result, the end-to-endspeed-up, memory and communication cost savings are very limited, which does not fully unveil the powerof FP8. To address this issue, we propose an extremely optimized FP8 mixed-precision framework for LLMtraining. The core idea is to infiltrate FP8 compute, storage, and communication into the whole progress oflarge model training, making the forward and backward pass all used the low-precision FP8, thus largelyreducing system workloads compared to previous frameworks (Micikevicius et al., 2017; Nvidia, 2022b;Micikevicius et al., 2022). Specifically, we design three optimization levels that utilize FP8 to streamline mixedprecision and distributed training. The three levels gradually incorporate 8-bit collective communication,optimizer, and distributed parallel training in an incremental manner. The higher optimization level indicatesusing more FP8 during LLM training. Moreover, for large-scale training, such as GPT-175B trained onthousand of GPUs, our framework provides FP8 low-bit parallelism, including tensor, pipeline, and sequenceparallelism, paving the way to next-generation low-precision parallel training.Training LLMs with FP8 is non-trivial. The challenges stem from issues such as data underflow or overflow,coupled with quantization errors arising from the narrower dynamic range and reduced precision inherentin FP8 data formats. These challenges cause numerical instabilities and irreversible divergences throughoutthe training process. To tackle them, we propose two techniques: precision decoupling and automatic scaling forpreventing the loss of critical information. The former one involves decoupling the influence of data precisionon parameters such as weights, gradients, optimizer states, and assigning reduced precision to componentsthat are not precision sensitive. The latter one is to preserve gradient values within the representation rangeof FP8 data formats through the dynamic adjustment of tensor scaling factors, thereby alleviating underflowand overflow occurrences during all-reduce communication.To validate the proposed FP8 low-precision framework, we apply it to GPT-style model training, encompassingboth pre-training and supervised fine-tuning (SFT). The experimental results demonstrate the effectivenessof our FP8 methodology, yielding substantial benefits including a 27% to 42% reduction in real memoryusage (e.g., 27% reduction for GPT-7B while 42% for GPT-175B ) and a notable 63% to 65% decrease in weightgradient communication overhead compared to the prevalent BF16 mixed-precision training approach.Without changes to any hyper-parameters, such as learning rate and weight decay, the models trained usingFP8 exhibit performance equivalency to those employing BF16 high precision, both in pre-training and downstream tasks. It is noteworthy that during the training of GPT-175B model, our FP8 mix-precisionframework reduces training time by 17% compared to TE (Nvidia, 2022b), while consuming 21% less memoryon H100 GPU platform. More importantly, the reduction in costs achieved through the utilization of lowprecision FP8 can be further increased, as the scale of models continues to expand, which is presented inFig. 1.For fine-tuning, we employ FP8 mixed-precision for instruction tuning and reinforcement learning withhuman feedback (RLHF) to better align pre-trained LLMs with end tasks and user preferences. Specifically,we fine-tune pre-trained models on publicly user-shared instruction-following data (ShareGPT, 2023). Themodels tuned with our FP8 mixed-precision demonstrate comparable performance to those utilizing thehalf-precision BF16 (Zheng et al., 2023) on the AlpacaEval (Li et al., 2023b) and MT-Bench (Zheng et al.,2023) benchmarks, while achieving 27% improvements in training speed. Moreover, FP8 mixed-precisionexhibits considerable potentials in RLHF, a process that necessitates loading multiple models during training.Through the utilization of FP8 in training, the prevalent RLHF framework AlpacaFarm (Dubois et al., 2023)can yield a 46% reduction in model weights and a 62% reduction in optimizer states’ memory consumption.This further demonstrates the versatility and adaptability of our FP8 low-precision training framework.We are making the following contributions to drive the design of next-generation FP8 low-precision trainingfor LLMs.• A new FP8 mixed-precision training framework. It unlocks 8-bit weights, gradients, optimizer, anddistributed training gradually in an add-on fashion, which is convenient in use. This 8-bit frameworkcan be used as a simple drop-in replacement for existing 16/32-bit mixed-precision counterparts,without requiring any changes to the hyper-parameters and training receipts. Additionally, weprovide a Pytorch implementation that enables 8-bit low-precision training in a few lines of code.• A new family of GPT-style models trained with FP8. We apply the proposed FP8 scheme to GPT pretraining and fine-tuning (i.e., SFT and RLHF), and demonstrate its potentials on a variety of modelscales ranging from 7B to 175B parameters. We equip prevalent parallel computation paradigmswith FP8 supports, including tensor, pipeline, and sequence parallelisms, enabling the utilization ofFP8 to train large foundation models. We open-source the first FP8 GPT training codebase basedupon Megatron-LM (Shoeybi et al., 2019) implementation.We expect the release of our FP8 framework will establish a new paradigm for next-generation low-precisiontraining system dedicated to large foundation models. \n2 FP8 LLMs \nMixed-precision (Micikevicius et al., 2017) has been widely used in LLM training to improve compute andmemory efficiency. The most popular mixed-precision schemes are FP16-FP32 and BF16-FP32. Because ofthe restricted numerical range of FP16, FP16-FP32 scheme has been known instabilities for training largemodels (Rae et al., 2021; Zeng et al., 2022). Consequently, the community now commonly adopts BF16-FP32for training LLMs, such as Megatron-Turing NLG-530B (Smith et al., 2022), Bloom-175B (Scao et al., 2022)and Gopher (Rae et al., 2021). The underlying reason is that BF16 has a wide dynamic range to maintainnumerical stability while matching the performance of the full-precision FP32. Moreover, BF16 employs halfthe number of bits as compared to FP32, thus reducing considerable memory footprints while improvingcompute efficiency.FP8 is a natural evolution from 16-bit data formats to further reducing computing costs. However, trainingLLMs with reduced-precision FP8 poses new challenges. The dynamic range and representation precisionof FP81 are much lower than BF16 and FP16, which inevitably induces more training collapses, such asloss spikes or even NaNs. To address the issues, tensor scaling techniques are proposed (Sun et al., 2019;Micikevicius et al., 2022). The core idea is multiplying higher precision values with a scaling factor prior totheir casting to FP8 in order to move them into a range that better overlaps with the representable range of a corresponding FP8 format2(Micikevicius et al., 2022). Such a per-tensor scaling technique reduces dataquantization errors while improving numerical stability and accuracy, thus enabling the utilization of thelower-precision FP8 for training large models.Unfortunately, the current support for FP8 low-precision training is restricted. Nvidia TE (Nvidia, 2022b) onlysupports FP8 compute for linear layers in Transformer (Vaswani et al., 2017), while leaving all other operations,such as weight update and gradient synchronization, still using higher precision. In this work, we present anextremely optimized FP8 mixed-precision strategy for LLM training. The new FP8 optimization includesthree key perspectives: FP8 communication, FP8 optimizer, and FP8 distributed training. By integratingthese aspects, the training of LLMs such as the 175B GPT-3 model can fully harness the advantages of FP8low-precision and improve training efficiency. \n2.1 FP8 Gradient and All-Reduce Communication \nExisting mixed-precision training methodologies (Micikevicius et al., 2017; Nvidia, 2022b) typically employ 16-bit or 32-bit datatype for the computation and storage of gradients, resulting in a high bandwidth requirementfor collective communication throughout the training process. We found that directly applying FP8 togradients leads to a decrease in accuracy. The fundamental issue lies in the underflow and overflow problemsarising from the low-bit all-reduce operation. Specifically, there are two standard methods aggregatinggradients across GPUs during all-reduce: pre-scaling and post-scaling. Pre-scaling divides the gradient gicalculated on the i-th GPU by the total number of GPUs (i.e., N) before being summed, which is formulatedas: g = g1/N + g2/N + · · · + gN /N. When N is large, this division can cause data underflow, especially for FP8 low-precision representation ofgradients. To mitigate this issue, post-scaling performs the gradient summation first, followed by the divisionscaling during the gradient collection process: g = (g1 + g2 + · · · + gN )/N. This post-scaling approach keeps the gradients close to the maximum value of the FP8 datatype, effectivelyalleviating the underflow issue. However, this approach encounters overflow issues when aggregatinggradients.In contrast, we propose an automatic scaling technique to resolve both the underflow and overflow issuesin the pre-scaling and post-scaling approaches. To be specific, we introduce an auto-scaling factor µ, thatchanges on the fly during the training, to reduce the occurrences of overflow and underflow in gradients: g′i = µ · gi. A statistical analysis is conducted on the gradient values of g′i, with the objective of quantifying the proportionof values that attains the maximum feasible value within the FP8 representation range. If the ratio of themaximum value exceeds a specified threshold, i.e., 0.001%, µ is set to 1/2 in the subsequent training step,thereby mitigating the risk of overflow. Conversely, when the ratio consistently remains the threshold, weopt to exponentially increase µ to 2 over the span of 1,000 training steps, thereby effectively mitigating therisk of underflow occurrences.Another key obstacle of FP8 collective communication lies in devising an effective strategy to manage thetensor-wise scaling factors that are associated with each gradient tensor. The current NCCL implementation(Nvidia, 2020) lacks the capability of performing all-reduce operation considering the additional tensor-wisescaling factors. Meanwhile, efficient implementation is also very challenging, especially considering thatthe NCCL gradient summation operates at sub-tensor level. This complexity increases significantly whenincorporating updates for tensor-wise scaling factors. To overcome this issue, we propose a new mechanismthat scales FP8 gradients across GPUs using a single shared scalar. To be specific, let (g′i, s′i) denote a scalingtensor which stores the weight gradient in the i-th GPU, where g′i is a FP8 tensor and s′i is the correspondingscaling factor. The actual weight gradient is g′i/s′i \n 2.2 FP8 Optimizer \nIn the training of LLMs, Adam and its variants (Kingma and Ba, 2015; Loshchilov and Hutter, 2018) are themost frequently-used optimization methods, that maintain copies of model weights, gradients, first-orderand second-order gradient moments for model updates. Mixed-precision training (Micikevicius et al., 2017)with Adam optimizer typically stores master weights, gradients and gradient moments in 32-bit float formatfor numerical stability (Shoeybi et al., 2019; Rajbhandari et al., 2020; Zhang et al., 2022; Scao et al., 2022).Consequently, the Adam optimizer consumes 16 bytes of memory per parameter during training. When model size is large, the memory consumption of the variables in Adam will become a bottleneck.Previous work (Rae et al., 2021; Zeng et al., 2022; Liu et al., 2022) has revealed that reducing precision ofthe variables in optimizer to 16-bit leads to accuracy degradation when training billion-scale models3. Thisprompts an evaluation of which variables in the optimizer should be allocated high precision and which canbe accommodated with low-precision.To clarify, we decouple the influence of data precision on the variables in the optimizer and investigatewhich one can be assigned lower precision, i.e., precision decoupling. We find a guiding principle: the gradientstatistics can use lower precision, while the master weights necessitate high precision. More concretely, thefirst-order gradient moment can tolerate a high quantization error and can be assigned with low-precisionFP8, while the second-order moment requires a higher precision, as analyzed in Sec. 3.3. This stems from thefact that, during model updates in Adam, the direction of the gradient holds greater significance than itsmagnitude. FP8 with tensor scaling can effectively preserve the distribution of the first-order moment as thehigh-precision tensor, though it introduces precision degradation to some extend. Calculating the square ofgradients for the second-order gradient moment might lead to data underflow due to the typically smallgradient values. Therefore, allocating a 16-bit higher precision is necessary to preserve numerical accuracy.On the other hand, we find that it is crucial to keep the master weights using high precision. The underlyingreason is that weight updates can sometimes become extremely small or large during training, higher precision for the master weights helps prevent loss of information when updating weights, ensuring morestable and accurate training. In the implementation, the master weights have two viable options: utilizingeither FP32 full-precision or FP16 with tensor scaling. FP16 with tensor scaling offers the advantage ofconserving memory without compromising accuracy. Consequently, our default choice is to employ FP16with tensor scaling for storing master weights in the optimizer. Our FP8 mixed-precision optimizer consumes6 bytes of memory per parameter during training. This new low-bit optimizer reduces memory footprints by 2.6x in comparison to the previous solution, asexemplified in Eq. (7). Noteworthily, this is the first FP8 optimizer for LLM training. The experiments inSec. 3.2 show that FP8 optimizer can preserve model accuracy at various scales, ranging from 125M to 175Bparameters. \n 2.3 FP8 Distributed Parallel Training \nTraining LLMs like GPT-3 requires distributed learning strategies for parallelizing across GPUs. Thefrequently-used strategies include data parallelism, tensor parallelism, pipeline parallelism, and sequenceparallelism. Each parallelism has its own merits and has been used in a complementary fashion in existingsystems (Smith et al., 2022; Shoeybi et al., 2019; Zhang et al., 2022; Scao et al., 2022; Li et al., 2023a). ForFP8 supports of these strategies, neither data parallelism nor pipeline parallelism necessitates any specificmodifications, because they do not involve additional FP8 compute and communication when splitting databatches or model layers into segments across devices.Tensor parallelism partitions individual layers of a model across multiple devices, such that the shards ofweight, gradient and activation tensors are placed on separate GPUs, instead of a single one. To equip tensorparallelism with FP8, we convert the sharded weight and activation tensors to FP8 format for linear layercomputation, enabling the forward compute and backward gradient collective communication all using FP8.On the other hand, sequence parallelism splits input sequences into multiple chunks and the sub-sequencesare fed to different devices to save activation memory. As shown in Fig. 2, sequence and tensor parallelismare performed in parallel to different parts of a Transformer model to make the best use of the availablememory and improve training efficiency. There is a converter g between sequence and tensor parallel regionsto all-gather sequence partitions in the forward pass (or reduce-scatter tensor segments in the backwardpass). We add an FP8 datatype conversion prior to g, such that the all-gather (or reduce-scatter) operationuses FP8 low-bit activation to save communication cost across GPUs. In addition, Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) is another frequently-useddistributed learning technique in large model training. The core idea of ZeRO is to shade model states overdevices, such that each device only hold a fraction of data (e.g., master weights, gradients, and optimizerstates) required for a training step. To reduce memory consumption, ZeRO method generally splits a singletensor into multiple partitions and distributes them to different devices. Directly applying FP8 to ZeRO isinfeasible, because it is difficult to handle the scaling factors associated with the FP8 partitions. The per-tensorscaling factors should be distributed along with FP8 partitions. To address this issue, we implement a newFP8 distribution scheme that distributes each tensor as a whole across devices, rather than partitioning it intomultiple sub-tensors as in ZeRO (Rajbhandari et al., 2020). The distribution of FP8 tensors is processed in agreedy manner, as outlined in Alg. 1. Specifically, our method first sorts the tensors of model states accordingto their sizes, and then distributes the tensors to different GPUs based upon the remaining memory size ofeach GPU. The distribution follows the principle that the GPUs with larger remaining memory get a higherpriority in receiving new distributed tensors. In this way, the tensor scaling factors can be distributed alongwith the tensors smoothly, while reducing communication and compute complexity. Figure 3 presents avisual illustration of the difference in ZeRO tensor partitioning between scenarios with and without scalingfactors. \n 3 Experiment \nIn this section, we assess the effectiveness of the proposed FP8 mixed-precision training approach on GPT-styleLLMs, including a wide range of model scales, from 125 million to 175 billion parameters. For performance ablation, we compare GPT models trained with FP8 against those trained with half-precision BF16 andfull-precision FP32. For generality evaluation, we conduct experiments encompassing both FP8 low-bitpre-training and fine-tuning, considering instruction tuning and human preference alignment. \n3.1 Experimental Setup \n3.1.1 Training Dataset \nOur pre-training data is constructed using open-sourced language collections from several sources, includingCommonCrawl4, The Pile (Gao et al., 2020), C4 (Raffel et al., 2020), OpenWebText (Radford et al., 2019;Gokaslan and Cohen, 2019), CC-NEWS (Liu et al., 2019), CC-Stories (Trinh and Le, 2018), Redpajama(Redpajama, 2023), and Wikipedia5. We apply fuzzy deduplication (Lee et al., 2022) across CommonCrawlsnapshots to enhance data quality. Tab. 10 in Appendix A.3 provides details of our pre-training data,including information such as the number of tokens from each source and associated sampling weights. Fora more comprehensive understanding of the data and its cleaning pipeline, readers are encouraged to referto Appendix A.3.Moreover, for instruction tuning, we follow the same settings as Vicuna-v1.1(VicunaTeam, 2023), whichuses a publicly user-shared instruction following data (ShareGPT, 2023). For reinforcement learning withhuman feedback, the training data we used is a combination of the Anthropic’s Helpful and Harmlessdataset (Bai et al., 2022) and Open-Assistant dataset (Köpf et al., 2023). The training framework andassociated configurations align with the publicly available AlpacaFarm (Dubois et al., 2023).3.1.2 Model Configuration \nThe model architecture we used is a decoder-only Transformer (Brown et al., 2020), which has been widelyused in recent generative LLMs like PaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), and LLaMA(Touvron et al., 2023). In addition to the base architecture, we integrate several modifications proposedrecently to improve model efficiency and effectiveness. 1) Rotary Positional Embedding: Drawing inspirationfrom recent successful experiments (Black et al., 2022; Touvron et al., 2023), we incorporate rotary positionalembeddings (RoPE) (Su et al., 2021) into our approach. This addition enables us to capture both absoluteand relative positions information, enhancing performance especially when extrapolating to larger contextwindows. 2) Flash Attention: The standard attention implementation is bottlenecked by memory access(Ivanov et al., 2021). Flash Attention (Dao et al., 2022) proposed an IO-aware exact attention algorithmwhich uses tiling to reduce the amount of HBM accesses, achieving substantial acceleration.We train the models using the proposed FP8 optimizer, which is built upon Adam (Kingma and Ba, 2015)with decoupled weight decay (Loshchilov and Hutter, 2018), following the common practise with the decayrates β1 = 0.9, β2 = 0.95, and weight decay = 0.1. The learning rate schedule is cosine-like, and the finallearning rate is 10% of the maximal learning rate. We train the models for 100B tokens in total with a batchsize of 4M tokens, and the input sequence length is set to 2048. The model warm-up is conducted for 1,000 iterations. Tab. 1 presents the details of model configurations and the corresponding training settings. Thetraining is conducted on Azure NDv5 H100 GPU platform (Microsoft, 2023).\n3.2 Main Results\n3.2.1 Model Performance\nWe first compare the performance of models trained using FP8 mixed-precision with those trained using BF16.In Fig. 4, the pre-training loss over tokens is displayed for GPT models of 7B, 13B, and 175B parameters. Thetraining configurations and hyper-parameters remain consistent across models trained with FP8 and BF16.The only difference lies in the mixed-precision schemes utilized. As shown in Fig. 4, the loss curves almostoverlap with each other. The results unequivocally demonstrate that the proposed FP8 mixed-precisionscheme can achieve equivalent performance to the prevalent higher-precision BF16 scheme (Shoeybi et al.,2019; Rae et al., 2021; Hoffmann et al., 2022) across a diverse array of model scales. Also, we evaluate thepre-trained models on a wide range of downstream tasks, including HellaSwag (HS) (Zellers et al., 2019),Lambada (Paperno et al., 2016) BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), COPA (Roemmele et al.,2011), Winogrande (Sakaguchi et al., 2021), Arc (Clark et al., 2018), and OpenbookQA (ObQA) (Mihaylovet al., 2018). As reported in Tab. 2, the FP8 pre-trained models exhibit comparable zero-shot performance incomparison to their BF16 counterparts. This result provides further validation that models pre-trained withFP8 low-precision maintain both accuracy and intrinsic in-context learning capabilities at a level comparableto their high-precision counterparts.Furthermore, we leverage the proposed FP8 mixed-precision approach for fine-tuning LLMs in instructionfollowing. For a fair comparison, we follow the same instruction tuning settings as Vicuna-v1.1 (VicunaTeam,2023), which adopts the open-sourced LLaMA-7B (Touvron et al., 2023) as the base model for fine-tuning.Fig. 5 presents the fine-tuning loss, where the curves corresponding to BF16 and FP8 display a notable degreeof overlap. Meanwhile, the win-rate of our FP8 fine-tuned models against Davinci-003 (OpenAI, 2022) is alsocomparable to that of Vicuna-v1.1, which is fine-tuned using BF16 half-precision, as reported in Tab. 3. Thisindicates that our FP8 low-bit training scheme is versatile, as it is applicable not only to pre-training phasebut also to downstream fine-tuning tasks. In addition, we further apply the proposed FP8 mixed-precision scheme to reinforcement learning fromhuman feedback (RLHF), a more complex process to align LLMs with user preferences. Following thesame training setting as AlpacaFarm (Dubois et al., 2023), a recent RL framework for LLM alignment, weoptimize policy models with PPO algorithm (Schulman et al., 2017). The solely difference lies in the choiceof mixed-precision training schemes, i.e., BF16 v.s. FP8. From the results reported in Fig. 6 and Tab. 4,we observe a notable reduction in memory utilization, for instance, a 31.8% memory reduction concerningmodel weights and a 62.5% reduction concerning optimizer states. Consequently, it can be inferred thatFP8 is capable of replicating the BF16 mixed-precision for RLHF training. This underscores the broaderapplicability and versatility of our FP8 low-bit training solution.\n3.2.2 System Performance\nIn this section, we evaluate system-level performance of FP8 mixed-precision, considering communicationefficiency, memory utilization, and the overall speed, with an emphasis on cost savings. Our methodemploys 8-bit gradients for all-reduce collective communication among GPUs. Theoretically, this resultsin a 75% reduction in communication costs when compared to the mainstream 32-bit scheme (DespiteBF16 mixed-precision computing gradients using 16-bit precision, it still employs 32-bit precision for allreduce communication (Shoeybi et al., 2019)). Due to the impact of system transmission loss, the observedpractical reduction during GPT model training falls within the range of 63% to 65%, as indicated in Table 5.Furthermore, it is worth noting that the recent Nvidia Transformer Engine (TE) (Nvidia, 2022b) still relieson full-precision FP32 for collective communication, resulting in the same level of reduction for our FP8solution.When training GPT models with identical batch sizes, FP8 mixed-precision can lead to a reduction in memoryfootprint ranging from 27% to 42% when compared to BF16, as reported in Tab. 5. These reductions inmemory consumption are attributed to the FP8 gradient and FP8 optimizer techniques we have introduced.Moreover, compared with TE (Nvidia, 2022b), our solution is also very competitive, obtaining 34.2%, 35.4%,and 44.8% additional memory reductions for different model sizes, i.e., GPT-7B, 13B, and 175B. Although TEemploys FP8 for compute, it still uses high-precision optimizer and gradients, which consumes much morememory than our solution. In addition, the saved memory in our method can be used to train larger batchsize or longer sequence. For example, when employing 32 H100 GPUs with a memory capacity of 80G, ourapproach enables the training of models with a context of 4,096 tokens, accommodating up to 175 billion parameters. In contrast, TE can only accommodate models with a context of 2,048 tokens. This showcasesthe potential of integrating our FP8 mixed-precision training into existing LLMs, empowering them to trainlonger sequences with the same GPU resources.Moreover, our FP8 mixed-precision scheme shows a superior training throughput compared to the prevalentBF16 scheme, achieving a notable speed-up of 64% when applied to GPT-175B model. The model FLOPSutilization (MFU) of FP8 mixed-precision training is 32.0% on H100 GPUs, being 17.2% superior to TE.These findings provide substantial evidence that our FP8 scheme effectively conserves memory, reducescommunication costs during the training of large models, and ultimately enhances system utilization efficiencyon the latest H100 GPU platform.\n3.3 Ablation Study\nWe ablate various design choices of FP8 mixed-precision training strategy for LLMs and report the performance in Tab. 6 – 8 and Fig. 7 – 8. The ablation experiments are conducted on GPT models, whosearchitectures and training settings are elaborated in Tab. 1. Importantly, our ablation study yields severalguidelines for the effective utilization of 8-bit datatype in LLM training, which can facilitate future researchon low-bit model training.Communication. We first analyze the limitations of the conventional pre-scaling and post-scaling methodswhen aggregating low-bit gradients during the all-reduce communication process. As shown in Fig. 7, weconduct a statistical analysis on SNR, underflow rate, and overflow rate of weight gradients across differentTransformer blocks. It is observed that the pre-scaling method has relative larger underflow rate whenquantifying gradients from 32-bit to 8-bit, while the post-scaling method has higher overflow rate. In contrast the proposed auto-scaling technique can diminish both the underflow ratio and the overflow ratio, whilegetting much better SNR, as shown in Fig. 7 (a). This demonstrates the effectiveness of auto-scaling methodin reducing quantization errors when utilizing 8-bit datatype for gradient all-reduce. Optimizer. We further ablate the impact of reducedprecision for the variables in the AdamW optimizer. We set the BF16 mixed-precision optimizeras the baseline, since it has been widely used inexisting LLM training frameworks (Micikeviciuset al., 2017; Shoeybi et al., 2019; Nvidia, 2022b).Tab. 6 presents the settings of reduced precisionfor the variables, while Fig. 8 plots the corresponding training losses. We observe that: 1) FP8 masterweight induces performance degradation (see the#2 vs. #3 lines in Fig. 8), while FP16 can maintain accuracy as FP32 (see #2 vs. #0 and #1) butrequiring using tensor scaling. It reveals that themaster weight is precision-sensitive. This can beattributed to the master weight’s role in updating weights, which tend to exhibit small magnitudes, necessitating high precision to maintainaccuracy. 2) The second-order gradient momentis more precision-sensitive than the first-order one,because the square calculation is easy to cause underflow and leads to accuracy degradation. Utilizing FP8 for the second-order gradient momentcan lead to divergent training loss (see the #4 dotin Fig. 8).Parallelism. In our FP8 LLM training framework,we introduce FP8 low-bit convertors into sequenceparallelism and tensor parallelism to reduce activation communication costs across GPUs. Herewe conduct an analysis experiment to count the activation-related communication volume during GPTmodel training, and report the numbers in Tab. 7. It is observed that our FP8 parallel scheme results in asubstantial reduction of 33% in activation-related communication costs compared to the original methodutilizing BF16. Furthermore, in ZeRO distributed training, our method distributes each FP8 tensor alongwith its associated scaling factor as a whole, rather than partitioning the tensor into splits across GPUs. Thisstrategy not only results in more GPU memory savings but also maintains a balanced memory load acrossGPUs, as demonstrated in Tab. 8.\n4 Related Work\nMixed-precision Training. Efficient training through reduced mixed-precision has been widely used inmodern deep learning to save computing costs. While some works have taken bit-reduction to the extreme,i.e. 1-bit binary networks (Hubara et al., 2016; Rastegari et al., 2016), they have not been successful inmaintaining model accuracy (Micikevicius et al., 2022). The most practical scheme now is the FP16 halfprecision method (Micikevicius et al., 2017), which can maintain accuracy while improving training efficiency.The computations during forward pass and back propagation use FP16 while the master weights use FP32.Since FP16 has a narrower dynamic range, FP16 mixed-precision entails loss scaling (Micikevicius et al., 2017)to prevent loss of accuracy. Fortunately, the need for loss scaling can be avoided by using BF16 datatype,because BF16 maintains the same dynamic range as the full-precision FP32. This results in that large modeltraining now prefers to use BF16 mixed-precision scheme, which is more stable during training (Smith et al.,2022; Scao et al., 2022; Zeng et al., 2022).FP8 is a natural progression from 16-bit data formats to further reducing computing cost. Early pioneeringefforts in FP8 low-bit model training (Wang et al., 2018; Sun et al., 2019; Dettmers et al., 2021) have largelyremained at the simulation stage. Consequently, there exists a notable gap between the projected capabilitiesof these approaches and their actual performance on hardware (Micikevicius et al., 2022). With the adventof Nvidia Hopper GPU architecture (Nvidia, 2022a), FP8 is emerging as a viable and practical data type forthe next-generation low-precision training, as discussed in (Micikevicius et al., 2022). At present, the NvidiaTransformer Engine (TE) (Nvidia, 2022b) serves as the primary framework for FP8 mixed-precision training.However, its support for FP8 usage remains somewhat constrained. TE’s current implementation restrictsFP8 usage solely to weight computation, retaining the storage of model weights and gradient calculationswith 16-bit data types. Consequently, the end-to-end speed-up, memory and communication cost savingsare limited. In contrast, our work infiltrates FP8 gradient, optimizer, and distributed training into the wholeprogress of model training, fully unveiling the capabilities of FP8.Large Language Models. Recent years have witnessed a substantial evolution in the field of LLMs. Autoregressive language modeling – predicting the future of a text sequence from its past – provides a simple yetpowerful objective that admits formulation of numerous tasks. While there exist alternative methodologies,such as masked language modeling (Devlin et al., 2019) and permutation language modeling (Yang et al.,2019), the autoregressive method now is more promising because of its strong performance. Following thescaling laws (Brown et al., 2020) and the refined laws (Hoffmann et al., 2022), various LLMs are have beenproposed, including dense models: GPT-3 (Brown et al., 2020), Jurassic-1 (Lieber et al., 2021), Gopher (Raeet al., 2021), Chinchilla (Hoffmann et al., 2022), Bloom (Scao et al., 2022), OPT (Zhang et al., 2022) MegatronTuring NLG (Smith et al., 2022), PaLM (Chowdhery et al., 2022), LaMDA (Thoppilan et al., 2022), LLaMA(Touvron et al., 2023), and sparse models: GLaM (Du et al., 2022), and Switch transformers (Fedus et al.,2022). Each of them has demonstrated remarkably competitive few-shot performance across a wide range oftasks at the time of their respective releases. Nonetheless, these models still encounter challenges, such asoverwhelming computational requirements and the need for acquiring more high-quality training data. Inthis work, we delve into the utilization of low-precision techniques to mitigate the training costs, which is acrucial step for the continued expansion of language models.Low-precision training has been widely used in LLM training to reduce compute cost. OPT (Zhang et al.,2022) and GLM (Zeng et al., 2022) utilize FP16 for forwards and backwards and FP32 for optimizer states andmaster weights, to reduce the GPU memory usage and improve training efficiency. Bloom (Scao et al., 2022)find that FP16 can cause numerical instabilities and irreversible divergences, especially when training modelslarger than 100B parameters, because FP16’s dynamic range is limited. Consequently, Bloom and other LLMs,such as Gopher (Rae et al., 2021) and Chinchilla (Hoffmann et al., 2022), adopt BF16 mixed-precision, becauseBF16 has a wide dynamic range that is the same as FP32. LLM training and tuning with 8-bit low-precisionwere not well-explored in previous works, because the hardware support for FP8 is not available beforethe release of Nvidia Hopper infrastructure. This work presents the first exploration of FP8 pre-training and fine-tuning for LLMs, while proposing an extremely-optimized FP8 mixed-precision scheme. We hopethis work could facilitate future research in FP8 and, potentially, extend to exploring even lower precisiontraining, such as 4-bit and 1-bit.\n5 Conclusion\nIn this work, we explore 8-bit training for LLMs. We introduce a new FP8 mixed-precision training framework, which incorporates 8-bit collective communication, optimizer, and distributed parallel training in anincremental manner. To our best knowledge, this is the first work infiltrating FP8 compute, storage andcommunication into the whole progress of large language model training. Extensive experiments demonstratethe proposed method effectively diminishes communication overhead and curtails memory utilization in thecontext of GPT model training at various scales. In future work, we plan to scale up the size and trainingsteps of the FP8 GPT models and further train them with our 8-bit mixed-precision scheme. Moreover, wewill also use the proposed FP8 scheme to train multi-modal large models, and explore low-bit deployment ofLLMs on various edge devices, such as smart phones.",
		"summary": "In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at aka.ms/MS.AMP.",
		"id": "UUID1"
	},
	{
		"document": "1 Introduction Federated learning (FL) is a technique for training machine learning models without sharing data,introduced by McMahan et al. [2017] and Konecnˇ y` et al. [2015, 2016], and steadily gaining momentum. Federated learning involves a possibly varying set of parties participating in a parallel trainingprocess through a centralized aggregator that has access only to the parties’ model parameters orgradients but not to the data itself. Compared to parallel stochastic gradient descent (SGD), federatedlearning aims at minimizing communication by parties performing a number of iterations locallybefore sending parameters to the aggregator. Federations tend to be diverse, leading to non-IID dataacross parties, and often include parties whose data can be considered to be outliers with respect to theothers. Most algorithms, however, can trigger a failure of the training process itself when parties aretoo heterogeneous in precisely the settings where federated learning could have the greatest benefit.Personalization of federated model training, when judiciously performed, is one means of avoidingsuch training failure. In addition, personalization of federated training allows for greater accuracyon the data that matters most to each party. The majority of federated learning fusion algorithmsare designed to produce a common solution for all parties. However, this is seldom the setting thatmotivates the use of federated learning. As also noted by Mansour et al. [2020], an application (e.g.of sentence completion) for a user should be optimized for that user’s needs and not be identicalacross all users. We propose Fed+ (pronounced as FedPlus) to address the issues of avoiding training failure, increasingrobustness to outliers and stragglers, and improving performance on the applications of interest whereparty-level data distributions need not be similar across parties. Fed+ unifies many algorithmsand offers provably-convergent personalization and robustness; this is achieved through a problemformulation that allows the central server to employ robust ways of aggregating the local modelswhile keeping the structure of local computation intact.Fed+ does not make explicit assumptions on the distributions of the local data, which are assumedprivate to each party. Instead, we assume a global shared parameter space with locally computedloss functions. Like some personalized methods, Fed+ allows for data heterogeneity by relaxingthe requirement that the parties must reach a full consensus. The Fed+ theory is equipped to handleheterogeneous computing environments, including stragglers, without making additional assumptions;specifically, the convergence results cover the general setting where the number of local update stepsacross parties can vary.To evaluate the performance of a federated learning aggregation method, it is important to assess iton the types of datasets on which it would be ultimately used. On the one hand, parties involved infederated learning training wish to enjoy improved accuracy on data from their own data populations.In addition, parties involved in federated model training also aim to train models that will transferwell. As such, it is crucial to evaluate algorithms on test sets that include some data from outside theparty-specific training data. We thus illustrate the benefits of Fed+ on the synthetic dataset created forFedProx by [Li et al., 2020a] as well as on the LEAF datasets of Caldas et al. [2018] to represent theparty-specific dataset scenario. We also construct personalized FL datasets on a synthetic regressionproblem and from the well-known MNIST dataset to provide an assessment of transfer quality withina party-specific setting.The contributions of this work are (i) the definition of a unified framework for robust, personalizedfederated learning, called Fed+; (ii) a convergence theory that covers the most important variantsof the Fed+ algorithm, including convex and nonconvex loss functions, robust aggregation andstragglers; and (iii) a comprehensive set of numerical experiments on party-specific datasets withand without with transfer requirements, thus illustrating the benefit of Fed+ with respect to otherfederated learning algorithms, personalized and non-personalized.\n2 Related Work\nLi et al. [2020b] showed that FedAvg defined by McMahan et al. [2017] can converge to a point thatis not a solution to the original problem and proposed to add a decreasing learning rate; with that,they provide a theoretical convergence guarantee, even when the data is not IID, but the resultingalgorithm is slow to converge. To handle non-IID data, Li et al. [2020a] introduced a regularizationterm in their FedProx algorithm. Li et al. [2020b]; Karimireddy et al. [2019] seek to explain thenon-convergence of FedAvg while proposing new algorithms. Pathak and Wainwright [2020]; Charlesand Konecný [2020]; Malinovsky et al. [2020] propose FedSplit and LocalUpdate, and Local FixedPoint, resp., and obtain tight bounds on the number of communication rounds required to achievean  accuracy. However, these algorithms all require the convergence of all parties to a commonmodel. Others have sought to increase robustness to corrupted updates and outliers. Pillutla et al.[2019] proposed Robust Federated Aggregation (RFA) by replacing the weighted arithmetic meanaggregation with an approximate geometric median. Yin et al. [2018] proposed a Byzantine-robustdistributed statistical learning algorithm based on the coordinate-wise median. Both RFA [Pillutla etal., 2019] and coordinate-wise median [Yin et al., 2018] involve training a single global model, andneither is robust to non-IID data, leading in some cases to failure of the learning process.Several recent works advocate, as we do, for a fully personalized approach whereby each client trainsa local model while contributing to a global model. Mansour et al. [2020] proposed clustering partiesand solving an aggregate model within each cluster. While this would likely eliminate the trainingfailure we observe in practice, it adds considerable overhead. Hanzely and Richtárik [2020] proposeda local-global mixture method focused on reducing communication overhead for the smooth convexsetting. Deng et al. [2020] proposed a method similar to our FedAvg+. T. Dinh et al. [2020] proposeda procedure for mean aggregation where each party optimizes its local loss and a (local version of)the global parameters. Hanzely et al. [2021] provided a unification of mean personalized aggregationfor smooth and convex loss functions. Li et al. [2021] proposed a bilevel programming framework, that alternates between solving for the mean aggregate solution and the local solutions. The overallproblem, however, is non-convex, even when parties have convex loss functions, and could be solvedin two separate phases. Zhang et al. [2021] suggested personalizing the mean aggregate solution as aset of weighted average aggregate solutions. The most important difference between Fed+ and theabove methods is that only Fed+ allows for robust aggregation, both in the definition of the algorithmand in the convergence theory, handling the resulting non-smooth optimization problem.3 Illustration of Training Failure in Federated LearningHere, we illustrate the training failure that can occur in real-world federated learning settings on afederated reinforcement learning-based financial portfolio management problem. The key observation,see Figure 1, is that replacing the local party models with a common, aggregate model at each roundcan lead to large spikes in model changes, triggering training failure for the federation as a whole.The figure shows the mean and standard deviation of the change in neural network parameter valuesbefore and after a federated learning aggregation step. FedAvg, RFA using the geometric median,coordinate-wise median, and FedProx are shown, as well as the no fusion case where each partytrains independently on its own data, and the FedAvg+ version of Fed+. All the standard FL methodscause large spikes in the parameter change that do not occur without federated learning or with Fed+.Such dramatic model change can lead to a collapse of the training process. The large spikes coincideprecisely with training collapse, as shown in Figure 2 (bottom four figures). Note that this exampledoes not involve adversarial parties or party failure, as evident from the fact that single-party training(top curve) does not suffer failure. Rather, it shows a real-world problem where parties’ data are notdrawn IID from a single dataset. It is conceivable that federated training failure may be a commonoccurrence in practice when forcing convergence to a common solution across parties.A deeper understanding of the training failure can be gleaned from Figure 3, which shows whatoccurs before and after an aggregation step and motivates the Fed+ approach. A local party updateoccurs in each subplot on the left side, at λ = 0. Values of λ ∈ [0, 1] correspond to moving towards,but not reaching, the common, aggregate model. A right-hand side lower than the left-hand sidemeans that a full step towards averaging (or using the median for) all parties, i.e., λ = 1, degradeslocal performance. Dashed lines represent the aggregated model in the previous round. Observe thatlocal updates improve the performance from the previous aggregation indicated by the dashed lines.However, performance degrades after the subsequent aggregation, corresponding to the right-handside of each subplot, where λ = 1. In fact, for FedAvg, RFA, and FedProx, the performance of thesubsequent aggregation is worse than the previous value (dashed line). \n 4 The Fed+ Framework\nWe design the Fed+ framework to handle real-world federated learning settings better, includingnon-IID data across parties, parties having outlier data with respect to other parties, stragglers, in thatupdates are transmitted late, and an implicit requirement for the final trained model(s) to performwell on both each party’s own datasets as well as datasets whose distributions differ from the party’straining data. To accomplish these goals, Fed+ takes a robust, personalized approach to federatedlearning and, importantly, does not require all parties to converge to a single central point. Fed+ thusrequires generalizing the objective of the federated learning training process, as follows.\n 5 Experiments\n5.1 Performance Comparison on Personalized Datasets\nTo test the robustness as well as the personalization quality of Fed+ and the baseline methods, wecreate two variants of the MNIST dataset: MNIST-robust and MNIST-personal, each with twodifferent federation sizes, N = 10 and N = 50, as well as a personalized synthetic regressionproblem. To create the robust and personal variants, we first partitioned the MNIST dataset equallyinto N parties. Then we transform the input distribution for 10% (20% for N = 50) of the parties bytaking negative of the images. Further, for every party we choose 2 different class labels and addLaplacian noise to the images corresponding to those classes to create the personalized dataset. Inthe synthetic regression dataset (N = 10), a sample (x, y) for a party k is generated through model:are generated by adding Laplacian noise (with scale =0.5) to a fixed w¯ ∼ N (0, 5I1000). We generatewN similarly but with a different w¯ ∼ N (0, 50I1000) to make the setting robust.We train logistic regression classifiers on the robust and personalized variants of the MNIST datasetand a linear regression model on the Synthetic one. Data is randomly split for each local party intoan 50% training set and a 50% test set. We report average performance on the test set after runningeach method 5 times with different random seeds. Each party’s dataset consists of 100 samples inthe Synthetic-regression case. The number of selected parties per round is K = 10; the batch sizeis 20 for MNIST datasets and 10 for the synthetic-regression dataset. We set the learning rate η to0.0001 for synthetic and 0.02 for MNIST variants. The regularization constant σ is chosen to be 15(σ = 1 for synthetic) for the robust and personalized MNIST datasets. For all experiments, we fixthe number of local iterations per round by setting Ek = 20 and report performance after T = 500rounds of training.We compare the performance of Fed+ with FL methods such as the non-personalized Scaffold[Karimireddy et al., 2019], as well as the personalized FL algorithms pFedMe [T. Dinh et al., 2020],perFedAvg [Fallah et al., 2020] and APFL [Deng et al., 2020]. The results are compiled in Table 2.Note that the MNIST problems measure accuracy and as such higher is better, while the regressionproblem measures error and thus lower is better.We observe that the robust FedGeoMed+ outperforms both the non-personalized and meanaggregation personalized methods by a significant margin. Our FedCoMed+, while performingpoorly on the personalized MNIST-robust datasets, is a close second place on the synthetic regressionproblem.5.2 Results on Standard Federated Learning Datasets\nWe also test our methods and the main non-personalized methods on a set of synthetic and nonsynthetic datasets from Li et al. [2020a] and the LEAF set of Caldas et al. [2018]. Since our FedAvg+is comparable to several of the recent personalized FL methods, it serves as an indicator of how meanaggregation-based personalized models fare on these standard datasets. The hyperparameters are the same as those of Li et al. [2020a] and use their reported best µ for theiralgorithm FedProx. MNIST [LeCun et al., 1998] with multinomial logistic regression. To imposestatistical heterogeneity, we distribute the data among N =1,000 parties such that each party hassamples of only one digit and the number of samples per party follows a power law. The input is aflattened 784-dimensional (28 × 28) image, and the output is a class label between 0 and 9. We alsoinclude the 62-class Federated Extended MNIST [Cohen et al., 2017; Caldas et al., 2018] (FEMNIST)of Li et al. [2020a]. Heterogeneous data partitions are generated by subsampling 10 lower casecharacters (‘a’-‘j’) from EMNIST and distributing only 5 classes to each party, with N =200 partiesin total. The input is a flattened 784-dimensional (28 × 28) image, and the output is a class labelbetween 0 and 9. To address non-convex settings, we consider sentiment analysis on tweets fromSentiment140 [Go et al., 2009] (Sent140) with a two layer LSTM binary classifier containing 256hidden units with pretrained 300D GloVe embedding [Pennington et al., 2014]. Each twitter accountcorresponds to a party with N =772 in total. The model takes as input a sequence of 25 characters,embeds each into a 300-dimensional space using Glove and outputs one character per training sampleafter 2 LSTM layers and a densely-connected layer. We consider the highly heterogeneous settingwhere there are 90% stragglers; see Li et al. [2020a] for details.Data is randomly split for each local party into an 80% training set and a 20% testing set. The numberof selected parties per round is 10 and the batch size is 10 for all experiments on all datasets. Theneural network models for all datasets are the same as those of Li et al. [2020a]. Learning rates are0.01, 0.03, 0.003 and 0.3 for synthetic, MNIST and FEMNIST and Sent140 datasets, respectively.The experiments used a fixed regularization parameter σ = 0.01 for each party’s Local-Solve andthe parameter δ is set to 0.001, 0.1 and 0.1 for FedAvg+, FedGeoMed+ and FedCoMed+ methods,respectively. On the Sent140 dataset, we found that initializing the local model to a mixture model(i.e. setting λ = 0.001 instead of the default λ = 0) at the beginning of every Local-Solve subroutinefor each party gives the best performance. We simulate the federated learning setup (1 aggregator Nparties) on a commodity-hardware machine with 16 Intel® Xeon® E5-2690 v4 CPU and 2 NVIDIA®Tesla P100 PCIe GPU.In Figure 4, we illustrate the test performance of the baseline algorithms FedAvg, FedProx, RFA,coordinate-wise median and Fed+. FedAvg+ is comparable and to and thus represents the performanceof the recent personalized methods. The baseline robust algorithms perform the worst on these nonIID data sets. Fed+ often speeds up the learning convergence, as shown in Figure 4 and improvesperformance on these datasets by 28.72%, 6.24%, 11.32% and 13.89%, resp. In particular, the bestFed+ algorithm can improve the most competitive implementation of the baseline FedProx on thesefour datasets by 9.90% on average. FedAvg+, and hence many standard personalized FL methods,achieves similar performance to FedGeoMed+ on MNIST and FEMNIST, but but fails to outperformthe robust variants of Fed+, FedGeoMed+ and FedCoMed+, on the synthetic and Sent140 datasets,highlighting the benefit of robust statistics. We also evaluate the impact of increasing the number of parties in training on test accuracy. Onthe synthetic dataset, average test accuracy improves from 70.22% to 90.73% to 98.03% when thenumber of parties participating in training goes from N = 3 to N = 15 to N = 30. The average istaken over the three Fed+ variants: FedAvg+, FedGeoMed+ and FedCoMed+. On MNIST, averageaccuracies of Fed+ are 69.80%, 81.34%, and 83.36% when the number of parties in training goesfrom N = 100 to N = 500 to N = 1000, resp. On FEMNIST, average accuracies of Fed+ are25.16%, 68.71%, and 78.66% when the number of parties in training goes from N = 20 to N = 100to N = 200, resp. On the Sent140 dataset, average accuracies of Fed+ are 57.13%, 60.77%, and65.43% when the number of parties goes from N = 77 to N = 386 to N = 772, resp. This showsthat the benefit of using Fed+ increases as the number of parties increases.\n6 Conclusion\nFed+ has been designed to better handle the heterogeneity inherent in federated settings: the lack ofIID data, the need for robustness to outliers and stragglers, and the requirement to perform well onparty-specific data. The Fed+ class of methods unifies numerous algorithms through a formulationthat allows for robust ways of aggregating the local models whilst keeping the structure of localcomputation intact. We provide convergence guarantees for Fed+ for convex and non-convex lossfunctions, robust aggregation, and for the case of stragglers. Probably the most promising extensionof this work would be an in-depth exploration of neural network layer-specific aggregation functionsas made possible through the Fed+ formulation.""summary": "We present a class of methods for robust, personalized federated learning, calledFed+, that unifies many federated learning algorithms. The principal advantageof this class of methods is to better accommodate the real-world characteristicsfound in federated training, such as the lack of IID data across parties, the needfor robustness to outliers or stragglers, and the requirement to perform well onparty-specific datasets. We achieve this through a problem formulation that allowsthe central server to employ robust ways of aggregating the local models whilekeeping the structure of local computation intact. Without making any statisticalassumption on the degree of heterogeneity of local data across parties, we provideconvergence guarantees for Fed+ for convex and non-convex loss functions underdifferent (robust) aggregation methods. The Fed+ theory is also equipped to handleheterogeneous computing environments including stragglers without additional assumptions; specifically, the convergence results cover the general setting where thenumber of local update steps across parties can vary. We demonstrate the benefitsof Fed+ through extensive experiments across standard benchmark datasets.",
		"id": "UUID2"
	},
	{
		"document": "1. Introduction \nCardiovascular disease (CVD), which is the leading cause of deathglobally, has become a significant problem in public health all over theworld. As a result, patients, their families, and the governments ofthese countries have incurred substantial socioeconomic expenses.Patients at high risk for CVD can be identified by prediction modelsthat use risk stratification. After that, measures that are tailored to thisgroup, such as dietary changes and the use of statins, can help reducethat risk and contribute to the primary prevention of CVD (1).Several guidelines for the evaluation and management of CVDhave suggested using predictive models as a means of identifyingpatients at high risk and assisting with clinical decision-making. ThePooled Cohort Equations and the Framingham CV risk equation6, forexample, have both been subjected to independent evaluations in avariety of populations; however, the findings indicated that both ofthese equations were only weakly discriminating and had a poor levelof calibration (2).As a direct consequence of this, the predictive power of themajority of the models that are now in use is restricted, and there isroom for advancement. For instance, the assumption of linearity isnecessary for logistic regression, while the assumption of predictorindependence is necessary for the Cox proportional hazard model (3).In the area of study pertaining to the cardiovascular system,machine learning (ML) algorithms have been demonstrated tobe extremely helpful predictors. They are more adept than standardstatistical models at capturing the complex interactions and nonlinearlinkages that exist between the variables and the results (4). Severaldifferent investigations (5–15) came to the conclusion that randomforests (RF) and support vector machines (SVM) performed betterthan traditional models.Cardiovascular diseases such as coronary artery disease (CAD),atrial fibrillation (AF), and other cardiac or vascular ailments continueto be the leading cause of death in the world (10). As people livingstandards improve and their stress levels continue to rise, the numberof people who suffer from CVD is growing at an alarming rate.According to the most recent estimations (16, 17), CVD willbe responsible for the deaths of about 23 million people by the year2030. Infarction of the myocardium, atrial fibrillation, and heartfailure are all instances of different types of CVD (18, 19). Theincidence of cardiovascular disease can be influenced by a number offactors, including racial or ethnic background, age, gender, body massindex (BMI), height, and length of torso, as well as the outcomes ofblood tests that evaluate factors such as renal function, liver function,and cholesterol levels (20, 21) which is shown in Figure 1.The development of a wide variety of health problems canbe influenced by the complex interactions that take place betweenthese factors. Standard statistical approaches are incapable ofaccounting for all of the intricate causal links that exist between riskfactors because there are so many of them (22, 23). In this day and ageof big data, the Internet of Things (IoT) has been shown to be ofcritical importance. It has made it possible for patients to use smartdrugs and smart bracelets to monitor and collect accurate data duringa pandemic (24).Researchers are employing artificial intelligence (AI) in an effortto mine new medical information that can be used by clinicians tobetter understand the symptoms of various diseases and, as a result,make more informed decisions for patients (25). This comes as theprevalence of data from the internet of things (IoT) grows withinhealthcare systems. In order to investigate previously unknown riskfactors, current efforts to standardise medical data, and efforts toorganise national health screening data (26–28), we  will firststandardise medical data. These risk variables may have a correlationwith the occurrence of the disease, which means that they could offerinsights into the mechanisms underlying the disease. Furthermore,accurate disease incidence prediction models necessitate the analysisof large amounts of data (29, 30). The use of artificial intelligence (AI)and massive amounts of data in the prediction of CVD models isbecoming increasingly common.The main contribution and novelty of this research ismentioned below:• To extract a total of 11 distinct characteristics from the dataset.• After that, we started by normalising the data and then proceededto divide the Heart dataset into training and testing sets using an8:2 split.• Afterwards, the incorporated GBDT is utilised in the SHAPmethod for the purpose of selecting features.• It helps to construct a stacking model consisting of a base learnerlayer in addition to a meta learner layer.• Finally, we  will achieve the results over several performancemetrics analyses and method in the stacking model.\n2. Background\nWeng et al. (31) tested four different models using clinical datafrom over 300,000 homes in the United Kingdom. According to thefindings, NN was the method that produced the most accurate CVDprediction results for the larger amount of data that were analysed.The three traditional machine learning models that were testedand evaluated by Dimopoulos et al. (32) based on ATTICA data with2020 samples for the little CVD dataset were K-Nearest Neighbour(KNN), Random Forest (RF), and Decision Tree. When compared, RFwas shown to have produced the best results by using theHellenicSCORE tool, which is a calibration of the ESC Score.In view of the growing popularity of machine learning techniquesin IoT applications, Mohan et al. (15) have proposed a hybrid HRFLMstrategy as a means of further improving the accuracy of the modelpredictions in light of the aforementioned popularity of machinelearning methods.An IoT-ML method was investigated by Akash et al. (33) with thegoal of predicting the condition of the cardiovascular system in thehuman body. The algorithm model uses machine learning (ML)techniques to compute and predict the patient cardiovascular healthafter it has obtained essential data from the human body. This datainclude the patient heart rate, ECG signal, and cholesterol.Within the framework of Yang et al. (34) examination of locallocations with separate prediction models, LR was utilised to evaluate30 cardiovascular disease-related characteristics utilising more than200,000 high-risk participants in eastern China. The results of theexperiments led to the development of an RF model that is moresuited to eastern China. For the first time in the study of CVDs, the idea of a stackingmodel was presented for the very first time by Yang et al. (35). The dataon air pollution and weather were considered in order to have a betterunderstanding of how the stacking model influences the dailyhospitalisation rate for CVDs. In order to assist in the construction ofthe stacking model, a grassroots level of five basic learners wasfirst constructed.During this period, digital, fully automated ecosystems as well ascyber-physical systems are fast growing and finding applications allover the world. The creation of smart healthcare, which offers toolsand processes for early diagnosis of life-threatening disorders, is oneexample of the innovative concepts and technical compositions thatare being implemented in nearly every business. As the fourthindustrial revolution moves towards a society that is moretechnologically advanced, there is an urgent requirement foradditional research into CVD Zheng et al. (36).\n3. Proposed method\nThe first thing that needs to be done is to combine the data fromthe Heart Dataset, which already contains information fromCleveland, Hungarian, and Swizerlang, as well as data from LongBeach VA and Stalog (Heart). From the five sources, we extracted atotal of 11 distinct characteristics. After that, we started by normalisingthe data and then proceeded to divide the Heart dataset into trainingand testing sets using an 8:2 split. Afterwards, the incorporated GBDTis utilised in the SHAP method for the purpose of selecting features.In the following stage, we  will construct a stacking modelconsisting of a base learner layer in addition to a meta learner layer.The study uses RF, LR, MLP, ET, and CatBoost classifiers to serve asour base learners. LR is utilised in the role of the meta learner. Finally,the suggested stacking model is assessed with regard to its accuracy,precision, recall, F1 score, and area under the curve (AUC). In orderto evaluate the model adaptability to new contexts, we made use of apublicly available dataset known as the Heart Attack Dataset.The Cleveland, Hungarian, Swizerlang, Long Beach VA, andStalog (Heart) datasets, together with others from the machinelearning repository at the University of California, Irvine (UCI), werecombined to form the Heart Dataset. We began with a total of 1,190samples, and after deleting 272 duplicates, we  were left with 918unique sample datasets. We started with 1,190 samples. The wholeHeart dataset is displayed in Table 1, and it consists of 11 features thatwere taken from five different datasets that contained significantrelevant features.\n3.1. Feature select and analysis\nIt is feasible to increase model performance and save aconsiderable amount of runtime by selecting the ideal subset offeatures that have a significant impact on the prediction outcomes.This process is referred to as feature selection, and it is possible toaccomplish both of these goals.The three most common methods for picking characteristics arecalled filters, wrappers, and embedding. The research we conductedutilised the embedded approach known as GBDT as a means ofselecting feature variables. This was due to the fact that embeddedtechniques offer superior prediction performance compared to filtermethods and are noticeably quicker than wrapper methods.GBDT makes use of an additive model and a forward stepwisealgorithm in order to achieve learning. These two components worktogether to accomplish this. For non-leaf nodes, the significance of thefeatures increases proportionately with the magnitude of the reductionin weighted impurity that occurs during splitting.Because of this, it is not possible to provide a detailed explanationof the role that each attribute plays in determining the overall accuracyof the predictions made by the integrated GBDT. In order to find asolution to this issue, we make use of a technique known as featureimputation, in which the explanatory model is a linear function of thevalues produced by feature imputation.\n3.2. Model building\nTo the extent that the model predictions are accurate, eachindividual in the base population has a stronger capacity for learning,and the degree of correlation between them decreases. When theindividual learners are already more accurate, a fusion of models willbe more successful if the individual learners come from a diverserange of backgrounds. This is the foundation upon which the conceptof error-ambiguity decomposition is built.This suggests that when picking the foundation learners for ourorganisation, we  should take into account the performance ofindividual learners while also taking into account the distinctivenessof each individual learner. Theoretically, it is conceivable to buildlayers of the stacking model indefinitely as long as their fundamentalclassifier is operational. This, of course, results in an increase in thelevel of complexity represented by the model.To ensure accuracy while reducing the level of complexityexhibited by the model, we solely employ the stacking model, whichis comprised of a two-tiered structure consisting of base learners andmeta-learners. As a direct consequence of this, SVM, KNN, LR, andET were decided upon as the possible models for base learners toutilise in the prediction of CVDs. XGBoost, LightGBM, CatBoost, andMLP were some of the other options that were thought about.Following the selection of the most reliable models as the foundationfor our education, we restricted the pool of potential candidates to fivepeople who exemplified a comprehensive representation of thepopulation as a whole. The optuna framework was used in order to getthe optimal values for the model parameters.After running a 5-fold CV, this model may generate a largenumber of features. 5-fold CV is a technique that is frequently utilisedin the first layer of a stacking framework to collect input features forthe second layer. This paper employs linear regression (LR) as the classifier for the fusion model predictions since generalised linearregression, also known as GLR, has historically been employed in thesecond layer of the stacking architecture. Because adjusting thecomplexity of the output layer of a neural network does not requirethe employment of more complex functions, this example makes useof functions that are simpler in nature.The primary learners are the LR, RF, DT, MLP, and CatBoostprotocols. At the beginning, we give the training sets eight times asmany points as the testing sets. Within the training package thatwe provide for each of the five foundational learners, we utilise a5-fold CV. A single base learner is capable of producing fivepredictions, and each of these five predictions is arranged in a verticalcolumn within a one-dimensional matrix. It possible that the secondstage of training will be based on a five-dimensional matrix that beendeveloped using the data of five different learners as its foundation.When applied to the testing set, the 5-fold CV model is utilisedonce more to make predictions about our initial testing set, whichresults in the production of five predictions once more. The baselearners can be  concatenated into a matrix for the stage seconditeration. We use LR on the meta-learner so that it does not becometoo good at its job. In the second step of the process, we use thesepredictions to build the final results.\n4. Results and discussion\nThe outcomes of the experiments will be discussed here in orderto illustrate the benefits of the stacking paradigm that wasrecommended by us. Python version 3.9.7 was used throughout eachand every test. In this investigation, the sklearn 1.0.2 toolbox is usedfor model prediction. The SHAP  40.0 toolbox is used for featureselection, and the Optuna 2.10.0 framework is used to determine theoptimum values for the model parameters which is shown in Table 2.We executed 10 splits of the data set using various random seeds inorder to account for the small sample size of this study and theaforementioned randomisation. After doing so, we  averaged theresults of all 10 experiments.Before we  started the feature selection process, our datasetcontained a total of 11 features. Using the Tree SHAP approach,you are able to determine the contribution value that corresponds toeach feature that is contained inside the sample dataset. The rankingof the feature contributions is determined by using the averageSHAP value for all of the samples. In accordance with the GBDTmodel, the contributions of the global features are depicted. The STSlope and Chest Pain Type have a significant influence on the patientcondition (CVD) in patients with cardiovascular disease. In order tocut the model operating time even more, some features that aren’tnecessary will have to be eliminated. We chose to adopt a cutoff of0.02, which led to the elimination of the Resting ECG characteristicwhile permitting the retention of the other 10 features. We used theAUC to evaluate the performance both before and after the featureselection process. Even though the AUC values of GBDT went down,the drop wasnot substantial at all, and there was not any differencethat could be  considered statistically significant by performingmetrics such as AUC, Threshold, Sensitivity, Specificity which isshown in Figures 2–5.The incidence of CVD was quite low in this experiment,resulting in poor PPV and NPV values for each of the sevendifferent ML models. Because of this, their therapeutic value maysuffer as a result of an increase in the number of false-positiveresults. The probabilities that were predicted by each machinelearning model were unique, and the risk distribution for LR wascomparable to that of SVM. Patients who had a CVD episode hadestimated risks that were greater, across all ML models, thanthose patients who had not had a CVD episode. The plots alsodemonstrated that all ML models overestimated the risks of thoseindividuals who had not experienced any CVD events. Thisfinding suggests that this variable may also affect how well amodel predicts what will happen.It is necessary to have a risk model in order to determinewhether individuals have a high probability of developingCVD. We intended to test seven machine learning (ML)-basedmodels to evaluate how correctly they could predict the risk ofCVD. The findings demonstrated that each one of them had goodto excellent discrimination and that they were all accuratelycalibrated. When it came to forecasting the risk of CVD, penalisedLR performed better than other machine learning models, justlike SVM did. The specificity of the SVM was higher than that ofthe LR, while the LR had a lower level of sensitivity. It is possiblethat a higher level of specialisation was favoured in this KazakhChinese group because the majority of the population wasnomadic and there were few medical services available. Inaddition to this, when taking calibration and DCA intoconsideration, SVM fared marginally better than LR. Because ofthis, SVM and LR can be used to find people in this group whoare at a higher risk of CVD and to find out if putting riskmitigation interventions in place for these people will improvetheir CVD outcomes during the clinical decision-making process.Linear regression has been widely used in the clinic toconstruct predictive models due to the ease with which it maybe  interpreted and its general straightforwardness. In a studyaimed at predicting myocardial ischemia, both LR and SVM wereshown to have the same level of predictive ability, which wasconsistent with our findings. A recent and exhaustive studyconcluded that there is no performance benefit to be gained fromusing ML in clinical prediction models over using LR. It wasdetermined that when machine learning algorithms were appliedto small datasets with a limited number of predictors, LR modelsmight perform better than ML models in terms of overallperformance. It is possible that the small sample size and the L1penalised technique used in this work are to blame for the superiorperformance of LR in comparison to other machine learningmodels, with the exception of SVM.The Support Vector Machine (SVM) is a well-known supervisedmachine learning approach that has found applications in a widevariety of business sectors. The fundamental idea behind supportvector machines (SVM) is to locate the hyperplane that has the capacity to effectively classify the data while also providing thebiggest geometric margin. In addition to this, it possesses significantkernel capabilities, which simplify the process of dealing withnonlinear classification issues. The outstanding performance of SVMdemonstrates that it is a great tool for tackling classification challengeson small, non-linear, and high-dimensional datasets. Thisdemonstrates that SVM is an excellent tool. In our experiment,we observed that the SVM performed significantly better than othermachine learning models.When it comes to classification, RF is among the most successfulensemble learning strategies that may be used. Its predictions were notnearly as accurate as those generated by the LR and SVM algorithms,which were the other two options. It is likely that the limited numberof participants in this study will prevent RF from achieving its fullpotential as a prediction tool. The concept of variable importance wasutilised in order to locate potential indicators of CVD. Some studiessuggest that RF may be  capable of revealing crucial butundisclosed predictions.According to the results of feature selection that was based on RF,the age of the patient was the most significant predictor in theclassification of CVD. In this study, it was discovered that certain riskfactors, such as smoking and alcohol intake, were not as predictive aspreviously believed. Previous studies have shown that the syntheticindices BAI and LHR are both very good indicators of cardiovasculardisease. Inflammation plays a significant part in the formation ofatherosclerotic plaques as well as1 the progression of cardiovasculardisease is shown in Figures 6–11.There is evidence that inflammatory cytokines, such as highsensitivity CRP and interleukin-6, are associated with an elevated riskof cardiovascular disease. The Hs-CRP inflammatory marker wasincluded in the Reynolds Risk Score in order to account for its role asa potential contributor to cardiovascular disease. hs-CRP has beenshown in a number of other epidemiological studies to be  animportant predictor of CVD. These studies have also shown thaths-CRP acts as a mediator in the pathogenesis of vascular disease andis a marker of endothelial dysfunction. These findings are consistentwith the findings of the aforementioned studies. It was discovered thatHs-CRP increases the risk of atherosclerotic plaque rupture in addition to destabilising atherosclerotic plaques via NO, IL-6,and prostacyclin.In addition, hs-CRP has been demonstrated to enhancethrombosis and cardiomyocyte apoptosis in response to hypoxia,which provides more support for its position as a risk factor forcardiovascular disease. It has been demonstrated that IL-6 is afactor in the course of atherosclerosis and that it promotes thecreation of atherosclerotic plaques. This factor may have contributedto the increase in the number of cases of CVD. Taking statins,which can reduce a person chance of acquiring CVD, is beneficialfor a great number of people and can help them avoid developingthe condition. In clinical practise, Hs-CRP and IL-6 can be used asbiomarkers for the early diagnosis of patients who have an increasedlikelihood of developing cardiovascular disease.According to the findings of our study, a decrease in ADP wasassociated with an increased risk of developing cardiovasculardisease. The adipose hormone ADP, which is secreted by adipocytes,has anti-inflammatory effects. These effects manifest themselves asa reduction in the levels of CRP and lymphocyte recruitment inatherosclerotic lesions, a reduction in the expression of TNF-, andan increase in the production of cytokines that are protectiveagainst inflammation. On the other hand, there is evidence from asmall number of studies that suggests an increase in ADP may assistin avoiding an ischemic stroke. Increased NEFA concentrationshave been associated with an increased risk of cardiovasculardisease in previous research, and our study came to the sameconclusion. The possible effects of NEFA on cardiovascular diseaseinclude the potential to exacerbate or worsen a number of illnesses,including type 2 diabetes, hypertension, the metabolic syndrome,and endothelial deterioration, to name a few. Patients can have alower chance of developing cardiovascular disease if they are treatedto have a lower ADP (CVD).The risk prediction models that are currently being used inCVD domains were built using traditional statistical methodologies,as many studies have revealed. Nevertheless, these models havebeen proven to be erroneous in external populations. In the field ofcardiology, machine learning algorithms have proven to be superiormethods for deriving predictions from big datasets that arenotoriously difficult to understand. No prior assumptions are madeby machine learning algorithms, which means that any data canbe used to develop accurate and resilient models. Because of this,ML is able to model more complex relationships between outcomesand predictors, which are typically more challenging to expressusing standard statistical methods. Discovering interactionsbetween marginal predictors can help improve risk-managementstrategies when using ML.Machine learning has the potential to identify new genotypesand phenotypes for a variety of CVDs, as well as novel risk factorsfor CVDs. All aspects of medical picture recognition, diagnosis,outcome prediction, and prognosis evaluation can be improvedwith the application of more sophisticated machine learningtechniques such as deep learning and artificial neural networks(ANN). It possible that in the future, cardiologists will makebetter clinical decisions if they use machine learning modelsrather than the CVD risk stratifications that are currently used.On the other hand, most ML models may be hard for medicalprofessionals to understand and use, which may limit how oftenthey can be used in clinical settings.\n5. Conclusion\nAccording to the findings of this research, a stacking fusionmodel-based classifier performs better than individual models on allassessment criteria. This finding suggests that stacking models cancombine the benefits of a variety of model types to achieve superiorprediction performance. The recommended stacking approach offersimproved prediction performance, increased resilience, and increasedutility for individuals who are at high risk of developingcardiovascular disease. Hospitals can utilise this information toidentify patients who are at a high risk of developing cardiovasculardisease and provide them with early clinical intervention in order toreduce that risk. Research in the field of deep learning will benefitfrom additional data from a large number of medical institutions,which may be used for the development of artificial neural networkstructures or for the usage of deep learning frameworks in the future.In future work, the other deep learning techniques algorithms canbe incorporated into Internet of Things (IoT) environments whichhelps to achieve more accuracy in terms of result and it can be usefulto the hospitals and saving several human life.",
		"summary": "It is yet unknown what causes cardiovascular disease (CVD), but we do know thatit is associated with a high risk of death, as well as severe morbidity and disability.There is an urgent need for AI-based technologies that are able to promptlyand reliably predict the future outcomes of individuals who have cardiovasculardisease. The Internet of Things (IoT) is serving as a driving force behind thedevelopment of CVD prediction. In order to analyse and make predictions basedon the data that IoT devices receive, machine learning (ML) is used. Traditionalmachine learning algorithms are unable to take differences in the data intoaccount and have a low level of accuracy in their model predictions. This researchpresents a collection of machine learning models that can be used to addressthis problem. These models take into account the data observation mechanismsand training procedures of a number of different algorithms. In order to verify theefficacy of our strategy, we combined the Heart Dataset with other classificationmodels. The proposed method provides nearly 96 percent of accuracy result thanother existing methods and the complete analysis over several metrics has beenanalysed and provided. Research in the field of deep learning will benefit fromadditional data from a large number of medical institutions, which may be usedfor the development of artificial neural network structures.",
		"id": "UUID3"
	},
	{
		"document": "I. INTRODUCTION \nDuring the last years, there has been a great increase inthe number of applications in which image classificationis useful. Helping people organise their photo collections,analysing medical images or identifying what’s around selfdriving cars are just a few examples. These tasks requireprecisely labeled large-scale datasets, and most of theminclude a huge variety of image types, from dogs or cats,to landscapes, roads, and so on.In image classification, given an input image, the goal isto predict the class which it belongs to. This is not a bigdeal for humans, but teaching computers to see is a difficultproblem that has become a broad area of research interest,and both classic computer vision and Deep Learning (DL)techniques have been developed. Classic techniques use localdescriptors to try to find similarities between images, buttoday, advances in technology allow the use of DL techniquesto automatically extract representative features and patternsfrom each image. However, to understand these concepts itis first necessary to review traditional techniques.In this report, we present different image classificationsystems trained on a specific dataset detailed in Section III.We first explore the Bag of Visual Words (BoVW) approach,which consists on extracting local features from the imagesand clustering them to create visual words. For each image,its histogram of visual words is used as a global featureto classify it. On the other hand, we use a MultilayerPerceptron (MLP) as a first step towards DL. To improvethe performance of the system, we use CNNs, which are more suitable for image classification. Specifically, we firstrefine an existing architecture, InceptionV3 [1], and finallydesign a CNN from scratch and analyze the impact of eachlayer, parameters, activation function, and more.\nII. PROBLEM DEFINITION\nGiven a dataset divided into 8 different classes, for eachimage in the dataset, the goal is to predict the class itbelongs to. To do so, we implement and evaluate fourdifferent systems: BoVW approach, MLP based, and CNNsarchitectures: fine-tuning an existing one and designing onefrom scratch. In each case, the model is trained with a subsetof images, and tested with unseen images to validate theperformance by means of the accuracy and loss. In Fig. 1we can see a simplified scheme of the system.\nIII. DATA\nThe dataset contains 2688 images from 8 different classes:coast, forest, highway, inside_city, mountain, open_country,street and tall_building. In Fig. 2 a sample image fromeach class is presented. To properly train and evaluate theimplemented systems, the dataset is divided into a trainingset of 1881 images (70%) and a test set, that contains 807images (30%).Before start developing our system, it is important toanalyze the dataset. For example, if the number of samples ofeach class is unevenly distributed (i.e. unbalanced dataset),using accuracy as the evaluation metric is not a good idea.In Fig. 3 it can be observed that the data samples are moreor less equally distributed across the different classes, so thedataset is balanced.\n IV. BAG OF VISUAL WORDS\nThe Bag of Visual Words (BoVW) approach consists on,given some training data, extract some local descriptors,cluster them in the multidimensional feature space to createvisual words and count the number of words each imagehas (i.e. histogram of visual words). Therefore, a histogramis generated for each labeled image, and used to train aclassifier such as Support Vector Machines (SVM). A toyscheme is presented in Fig. 4. In this section, the methodsused to implement the BoVW system are explained in detail,and the results obtained with each configuration are presentedand analyzed. For that purpose, we compute the accuracywith 8 (stratified) fold cross-validation in all cases.A. Keypoints and descriptorsIn the BoVW approach a feature detection algorithm isused to detect keypoints and extract local descriptors fromeach image, so the first step is to find which is the one thatworks best in our case using a k-Nearest Neighbors (KNN)classifier. The descriptors tested are: SIFT [2] (vanilla anddense), SURF [3] (vanilla and dense) and DAISY [4] (onlydense).1) Vanilla descriptors: In this scenario, keypoints areextracted using the detection algorithm of the correspondinglocal descriptor: SIFT or SURF (DAISY does not have anykeypoint detector). An example of this detection is presentedin Fig. 5b.2) Dense descriptors: Instead of using the detection algorithm to extract keypoints, we create a grid of spatiallyequidistant keypoints, which is translated into a more denserepresentation of the image. This is shown in Fig. 5c. The results are presented in Tab. I. As observed, thedense descriptors outperform vanilla SIFT and SURF. Themain reason is that vanilla SIFT and SURF depend on theperformance of the keypoint detector: if this detection ispoor, the classification fails. When using dense keypoints,we get a representative descriptor of the images even if thecontent of the image is plain (low textures) or has repetitivepatterns. In this case, the best results are obtained with denseSIFT, so when the descriptor is mentioned in this paper, itwill be referring to dense SIFT.The hyperparameters used to create the dense keypointsare also fine-tuned, and the results show that the best performance is obtained when the number of keypoints is larger. Onthe other hand, vanilla SIFT is scale-covariant, as it computeskeypoints at different scales, so we tried to emulate this fordense SIFT with another parameter. However, there is nota substantial improvement in the results in this case, so thescale is not an important factor in this dataset. For this reason,we can conclude that losing the scale-covariance property ofvanilla SIFT is not a problem when using dense SIFT.B. ClassifiersThe presented results are obtained using a k-NN classifier,which might be too simple in some tasks such as imageclassification. For this reason, we analyze the performance of the system using other classifiers: a logistic regressionmodel and a SVM. As aforementioned, the descriptor usedin each case is dense SIFT.The corresponding results, presented in Tab. II, showthat SVM outperforms both k-NN and logistic regressionclassifiers. For this reason, we conclude that SVM is bettersuited for our problem, and thus it will be the one used inthe rest of the experiments.1) Fine-tuning SVM kernel: To further improve the results, we compute the accuracy (mean and standard deviation) for different SVM kernels. In addition to the the typicalkernels (linear, poly, RBF and sigmoid), we create our own:the histogram intersection kernel, defined in Eq. 1. The results are shown in Tab. III. The worse results areobtained using the linear kernel. The reason is that, in thisdataset, images of different classes share visual words (e.g.trees in both forests and open country classes), so they arenot linearly separable. On the other hand, the performanceusing the histogram intersection kernel is acceptable, as thiskernel is useful in our specific problem, where the featuresare histograms. However, it is recommended to use the RBFkernel, which creates non-linear combinations of the featuresto uplift the samples onto a higher-dimensional feature spacewhere a linear decision boundary can be used. Indeed, weobtain the best accuracy with this kernel.C. Spatial pyramidsThe BoVW system efficiently aggregates local featuresinto a single global vector but it completely ignores theinformation about the spatial layout of the features. To tacklethis, we compute the keypoints and descriptors at differentpyramidal levels. Spatial pyramids work by partitioning the image into increasingly fine sub-regions and computinghistograms of the visual words inside each sub-region.1) Square sub-regions: The first approach is to divideeach region in 4 square sub-regions for each level (Fig. 6a,b).For example, using three levels, we first compute the descriptors for the whole image. Then, the image is dividedin 4 blocks and the descriptors are computed for each ofthem. Each sub-block is divided into 4 blocks (16 in total)and the descriptors are computed for each of them. Finally,the descriptors computed in each level are concatenated (inthis case, 1 + 4 + 16 = 21 descriptors). By computing thedescriptors of the different sub-regions of the image, we canlater compute the histograms of visual words of each of theregions.2) Horizontal sub-regions: As this specific dataset isformed by a large number of landscape images, we thoughtit might be interesting to divide the image in horizontal subregions, as shown in Fig. 6c,d. In this case, for each level theimage is divided into 3 more sub-regions. For example, usingthree levels, the resulting descriptor will have (1 + 3 + 6 =)10 concatenated histograms.D. NormalizationIt is a good practice to normalize the data to avoid differentscales of the feature vectors and thus improve data integrity.In this case, the histograms of visual words are normalizedusing L2 norm, Power norm or StandardScaler. The normalization of the histograms of visual wordsplay an important role in the spatial pyramid algorithm.If the image is divided in 4 blocks, each block has 1/4of the information of the whole image. Therefore, whencomputing the histogram of a block, the energy (and thusthe contribution) of that histogram will be 1/4 of the energyof the histogram of the whole image. To give the sameimportance to each of them, we normalize all histogramsso that they contribute the same.The results obtained using square and horizontal subregions and without and with normalization are presented inTab. IV. As observed, for this specific dataset, if the image isdivided in horizontal sub-regions, the results are better. Thereason is that most of the images are landscape images witheasily differentiated horizons, as aforementioned. Comparingthe results for the different pyramid levels, it is observedthat levels 1 and 2 outperform level 0 in all cases, whichproves that features spatial information is relevant to imageclassification performance. On the other hand, level 2 andlevel 1 only differ by a small margin in most of the cases,but level 2 comes with a big extra computational cost (e.g.21 vs 5 histograms for the square sub-regions), so level 1 ischosen as the most suitable pyramidal level to use. For higherpyramidal levels, curse of dimensionality also comes intopicture as our feature space becomes sparse. Regarding normalization, we observe that the results are slightly improvedin some cases, specially for the L2 norm and standardScaler.However, the improvement is not significant, which validatesthe consistency and integrity of our data. Even so, it is agood practice to normalize the data, and it will be useful toimprove our results later, so StandardScaler normalizationwill be used.E. ClusteringTo create the visual words, the multidimensional featurespace is clustered using k-means, being k the codebook size(number of visual words). To further improve the results, thishyperparameter is fine-tuned, and the results are presented inFig. 7. As observed, with a small codebook size (e.g. 32), thevisual words are too general, so they are not representativeenough to perform classification properly. With larger codebook sizes, the results improve up to a certain point (0.85 for a codebook size of 512), as with more visual words eachclass is well represented, so it is easier for the classifier todistinct between them. The best results are obtained with acodebook size of 512, so that will be the one used. Withcodebook sizes of 1024 and 2048 the results are also good,but the computational cost is much higher.F. Reducing dimensionalityThe best results are obtained with large codebook sizes,such as 512, so we use Principal Component Analysis (PCA)to decrease the computational time. Concretely, PCA is usedto reduce the feature space dimensionality projecting it to alower dimensional space. This reduction of dimensionalityis very useful in our case, as spatial pyramids increasethe dimension of the vector and thus the computationaltime. Gridsearch is performed to fine-tune the parameternum_components, which is used to select the number ofdimensions to be kept after the dimensionality reduction.As shown in Fig. 8, the best results are obtained with thelarger num_components (up to 0.87). This parameter definesthe dimensionality of the resulting vector, and the higherits dimension, the more representative of the data and thusthe better the performance of the classifier. Applying PCAslightly improves the performance, but more importantly,it speeds up the computation. For this reason, PCA withnum_components=64 will be used.G. Fisher vectorsEven if the BoVW approach performs well on our dataset,it finds the closest word in the vocabulary relying only on thenumber of local descriptors assigned to each Voronoi cell.With fisher vectors, we are not only using the mean of thelocal descriptors, but also including higher order statistics:the covariance. This way, the information of how far is eachfeature from its closest vocabulary word (and also to theother vocabulary words) is obtained.To study its performance in our dataset, we fit an SVMclassifier with the train fisher vectors (obtained from the training dataset) and we use the test fisher vectors to predictthe labels of the test dataset. Fisher vectors allows training more discriminative classifiers with a lower vocabularysize. The obtained results show that encoding our featurevectors using second order information (covariances alongwith means) indeed benefits classification performance, asit provides similar results (0.84) reducing the computationalcost.V. MLPThe results obtained with a classic approach such asthe Bag Of Visual Words system are acceptable, but notgood enough to consider the implemented image classifierrobust nor reliable. For this reason, we need to use advancedtechniques to improve the performance and obtain the desiredresults: the well-known Deep Learning (DL). As a firststep towards DL, we explore the most simple architecture:Multilayer Perceptrons (MLP), in which each neuron of eachlayer is connected (forwards).Using a simple MLP and a softmax layer (last layer usedto predict the class of each input image), the results in termsof accuracy and loss are really bad, as shown in Fig. 9. Thedifference between the train and validation accuracy curves isan indicator that the model is overfitting to the training data,and thus is not able to generalize well to unseen samples(those of the test data). Moreover, the validation loss curveis unstable and not properly minimized.To try to obtain better results, we fine-tune different parameters: learning rate, image size, number of layers (depth),layers sizes, adding normalization or regularization, and soon. Even if the performance is slightly improved in somecases, the potential of the system is limited to the fact thatwe are using a simple MLP for a hard image classificationtask, so the results will not get better. It is worth mentioningthat the impact of each of these parameters (e.g. learningrate) is deeply studied in Section VII, where we designa CNN from scratch, which is more realistic in the imageclassification task.A. Deep Features, SVM and BoVWFinally, before moving on to CNNs, we explore differentvariations of the MLP system in order to see if the resultscan be improved:1) Deep Features (DF) + SVM: Extract DF from thedeeper hidden layer (previous to softmax, where thefeatures are more abstract/general) and use them totrain an SVM classifier. 2) Aggregating predictions: Divide the input image intosmall patches, extract the prediction for each patch andaggregate the final prediction.3) DF as a dense descriptor + BoVW Divide the inputimage into small patches, extract the DF from the lasthidden layer for each patch, and concatenate them tocreate a feature vector for each image. Then, use kmeans to create a codebook and train an SVM classifierwith the histograms of visual words.The obtained results are presented in Tab. V. As observed,extracting DF and using them to train an SVM classifier isnot a good alternative. Another approach is to divide eachimage in patches and extract deep features from each ofthem. In this other two cases, even if the results are slightlyimproved, they are not acceptable, and much worse than theones obtained with the classic BoVW approach. For thisreason, we conclude that MLP is too simple for this imageclassification problem.\nVI. INCEPTIONV3\nThe results obtained with MLP are not acceptable, so wetake a step forward into deep learning to use the state of theart architecture in image-related tasks: Convolutional NeuralNetworks (CNNs). Since the outbreak of CNN in 2012with AlexNet [5], multiple architectures have been presentedto tackle the classification problem, obtaining increasinglybetter results in terms of minimizing the miss-classificationerror. In this paper, we fine-tune InceptionV3 [1] to adapt itto our specific dataset. This network, created by Google, isbased on the idea of using Inception modules to use differentsizes of channels in parallel, as there are four parallelchannels in each module, which are further concatenatedat the end. Specifically, each module includes factorizingconvolutions with large filter size into smaller filter, factorization into asymmetric convolutions and auxiliary classifiersintroduced to tackle the problem of vanishing gradient.This model has been trained and tested with the ImageNetdataset [6], which contains around 1M images divided in1000 classes. Therefore, it has not much to do with thedataset used in this study, and we need to adapt InceptionV3to our specific problem. A. Changing the network architectureThe first approach we take is to use the existing model andweights by modifying only the last layer of the architecture:the softmax layer. This is a required step to adapt the outputto the number of classes that our dataset has: eight. First ofall, we freeze all the layers except the last one, so that thetraining stage does not affect the pre-trained weights of themodel.As observed, in Fig. 10, the results are greatly improvedusing InceptionV3 compared to the ones obtained with asimple MLP, which proves the potential and usefulness ofCNNs in image classification problems. Specifically, thedifference between the training and validation loss is muchlower, so there is no overfitting. Furthermore, both trainingand validation losses are correctly minimized.In the confusion matrix (Fig. 11b), we observe thatInceptionV3 performs really well in most of the cases,but it misclassifies a lot of forest and mountain samplesas opencountry. This is also observed in the ROC curve(Fig. 11a), as the opencountry’s Area Under the Curve (AUC)is lower.B. Unfreezing some layersThe next step is to unfreeze and retrain some layersof InceptionV3, to see if the learned weights improve theresults. InceptionV3 is divided in 11 Inception blocks and atotal of 311 layers, so it is not easy to select which layersto unfreeze. For this reason, we unfreeze by blocks (alwaysstarting from the end).The results are presented in Tab. VI. As expected, the testaccuracy increases as we unfreeze and retrain more blocksof the model, being the best result the one obtained withthe full retrained model. However, the number of trainableparameters also increases, so the computational cost is muchhigher.\n CONCLUSIONS\nIn this paper, we have explored some traditional and DeepLearning techniques that can be used in a classificationsystem, and we have seen the advantages and disadvantages of using some models compared to others. From the resultsobtained, we can draw the following conclusions.First of all, classic approaches (e.g. BoVW) can providegood results, but they are limited and would not makea reliable nor robust system for a real world application.However, it still performs better than a simple Deep Learningtechnique like the Multi-Layer Perceptron. MLPs are toosimple for our image classification problem, and even whenoptimizing their hyper-parameters the performance is poor.Using the MLP system to extract deep features to useafterwards on SVM, or as descriptors for the BoVW systemdoes not provide us with better results neither.Fine-tuning on a pre-trained network (e.g. InceptionV3)gives the best results in terms of accuracy when unfreezingand retraining the weights, 96 percent. But if we take intoaccount the number of parameters, it is not an efficientsystem. Indeed, we can remove layers and reduce by fivetimes the amount of parameters without losing accuracy, andstill be an overkill for our specific dataset.When training a model from scratch, a good weightinitialization is important for making sure our models trainproperly. We need a good amount of data for trainingdeep neural networks, and with the dataset that we had(1,800 samples) we could not improve our accuracy abovea threshold (90 percent), because the data wasn’t enough tolearn features that were representative enough of the dataset.Still our model is better fitted to the specific problem anddataset: while the pruned InceptionV3 had 5M parameters,in our model we have around 4K parameters.",
		"summary": "To classify images based on their content is one ofthe most studied topics in the field of computer vision. Nowadays, this problem can be addressed using modern techniquessuch as Convolutional Neural Networks (CNN), but over theyears different classical methods have been developed. In thisreport, we implement an image classifier using both classiccomputer vision and deep learning techniques. Specifically, westudy the performance of a Bag of Visual Words classifier usingSupport Vector Machines, a Multilayer Perceptron, an existingarchitecture named InceptionV3 and our own CNN, TinyNet,designed from scratch. We evaluate each of the cases in termsof accuracy and loss, and we obtain results that vary between0.6 and 0.96 depending on the model and configuration used.Keywords— Computer vision, Image classification, Bag ofVisual Words, Support Vector Machines, Deep Learning, Multilayer Perceptron, Convolutional Neural Networks.",
		"id": "UUID4"
	},
	{
		"document": "1 Introduction \n What should a data integration framework for knowledge engineers look like? Approaches can transform the non-RDF data sources on the basis of specific ontologies, designed to represent content from popular formats (e.g. Any23). Alternatively, solutions could offer a mapping language (e.g. RML) which reuses components of format-specific query languages (e.g. JSONPath). In addition, systems could extend SPARQL, and incorporate features of pre-existing query languages for each one of the original formats (e.g. Xpath for XML), allowing users to perform mappings within SPARQL queries (SPARQL Generate). In this resource paper, we present a system whose design principle comes from the notion of façade, borrowed from object-oriented software engineering. A façade is a generic interface that aims at hiding the internal complexity of a class, exposing behaviours that better fit the task at hand. This idea, originally introduced by (Daga et al. 2021), is applied to SPARQL Anything, a system that allows querying heterogeneous resources as-if they were in RDF. SPARQL anything does not change the SPARQL 1.1 specification but injects new behaviour by overloading the SERVICE operator with a custom IRI-schema. In (Daga et al. 2021), authors introduced the idea of applying façades for semantic lifting, presented a formalisation of the approach in predicate logic, and showed how it is applicable to a variety of formats, while bringing important benefits in terms of usability and extendibility. In (Asprino et al. 2023), it is demonstrated how Façade-X components are enough to be applicable to whatever is generated by a formal grammar (which is the most common tool for describing data formats), and it can in theory be applied to relational databases as well. In this resource paper, we describe the current version of SPARQL Anything and how it was applied to real-world knowledge graph construction pipelines in the context of various projects, including two EU funded, H2020 projects – SPICE34 (Daga et al. 2022) and Polifonia35, and several projects of a US-based IT company active in the healthcare sector. We discuss the value-to-users of our proposition in two ways. First, we report on a survey which evaluates the benefits and opportunities of this approach, compared to alternative solutions, from the user perspective. Second, we provide a field report, presenting first-hand feedback from the industry sector by one of the authors. The following section illustrates the design principles of façade-based data access (Daga et al. 2021), describing Façade-X, the generic meta-model implemented by our system (Section 2). Section 3 describes the main features of the SPARQL Anything system, the currently supported formats, and additional features that allow the development of RDF construction pipelines from heterogeneous data files. An extensive set of case studies are reported in Section 4. Finally, Section 5 reports on feedback from our community of users. Related work is considered in Section 8. Finally, we conclude the paper in Section 9. \n 2 Façade-based data access \n In this Section, we illustrate the idea behind façade-based data access. We rely on the notion of facade as 'an object that serves as a front-facing interface masking more complex underlying or structural code.' Applied to our problem, a façade acts as a generic meta-model allowing (a) to inform the development of transformers from an open-ended set of formats, and (b) to generate RDF content in a consistent and predictable way. The Façade-X meta-model introduced in (Daga et al. 2021) was designed by selecting a small set of primitive data structures: typing, key-value maps, and sequences. Façade-X defines two types of objects: containers and values. Containers can be typed, and one container in the dataset is always of type root (the only primitive specified by Façade-X). Values can have any datatype. Containers include a set of unique slots, either labelled as strings or as integer numbers. A slot is filled by another container or by a value. An RDF specification of Façade-X uses the following namespaces and associated preferred prefixes. The first is used to express the primitive fx:root. String slots are RDF properties generated with the xyz: namespace, where the local name is supposed to be the string labelling the slot in the data source (for example, a JSON property)37. Instead, integer slots (sequences) are represented with instances of rdf:ContainerMembershipProperty: rdf:_1, rdf:_2, ... rdf:_n. Façade-X uses containers rather than rdf:List since we know that this representation is more efficient to deal with in SPARQL (Daga, Meroño-Peñuela, and Motta 2021). Finally, values are rdf:Literal, while containers can be either IRIs or blank nodes (the specification does not enforce the use of of either, leaving both options to the Façade-X engineer, including the possibility of switch between the two with a tool option). With these set of components, a Façade-X software engineer is supposed to design connectors to an open-ended set of resource types, leaving to the knowledge engineer (Newell et al. 1982) the freedom of accessing those data sources as-if they were RDF. In what follows, we show how to apply a single RDF abstraction to an extensive set of file formats (including some that are served to the SPARQL users for the very first time). CSV. A CSV file is a resource, identifiable by a URI, which contains text organised as an ordered sequence of rows (newline separated), which in turn contains an ordered sequence of data fields (separated by a delimiter). Rows are ordered; therefore, this case of containment can be represented as an ordered sequence (with container membership properties). What about data values within a row? We observe how CSV data may have an optional “header”, where the first line is the list of field names. When this happens, we can use the property component and generate an RDF property reusing the field name, and minting an IRI with the xyz: namespace. Otherwise, we can consider the values on each row as another sequence, and fallback to the ordered sequence component. JSON. The JavaScript Object Notation is specified by ECMA39. The syntax defines three types of elements: objects, a set of key-value pairs, where keys are supposed to be unique; values, which are either strings, numbers, boolean, or the primitive ’null’, and arrays, which specify sequences (containing other arrays, objects, or values). We interpret objects and arrays as containers. We can reuse rdf:Property to link objects to values. Arrays can be represented by the ordered sequence component. Values can be expressed as rdf:Literal, selecting relevant XSD datatypes from the RDFS specification: xsd:string, xsd:boolean, xsd:int, xsd:float40. The following example shows a JSON document with metadata of an artist in the Tate Gallery Collection. The JSON file will be represented as follows in RDF (in Turtle syntax): HTML and XML. Both formats can be captured by the Document Object Model (DOM) specification, which we will refer to in the following description41. HTML/XML elements (also known as tags) can be definitely considered containers, so we can reuse both the rdf:Property Façade-X component for specifying tag attributes, and container membership properties for specifying relations to child elements in the DOM tree. These may include text, which can be expressed as RDF literals of type xsd:string. What about element types (tag names)? Façade-X does already provide a solution for unary attributes: rdf:type. Finally, we can use namespaces declared within the original document to name properties and types, if available, instead of the default xyz:. Examples with HTML content will be referred to later in Section 4. Textual data is an interesting case where we can use containment to refer to different elements of the text. The whole content can be included in one single literal, as follows. Alternatively, the text can be tokenized (with a user-defined delimiter) and the resulting sequence represented with our façade. Binary content such as images can be also supported, by embedding the content in a single literal of datatype xsd:binary64encoding. This solution does not allow to query the actual content, clearly, but still allows to bring in the content and serve it, for example, as linked data. However, binary files such as JPEG images often include annotations, for example, using the common EXIF metadata schema. These annotations can be considered an additional data source, and represented in a separate metadata graph with Façade-X. YAML is a lightweight, human-readable data-serialization language. YAML is a “superset” of JSON (any JSON file can be specified in YAML) and, similarly to JSON, data can be organised in lists or associative arrays42. BibTeX is a text format for computational bibliographies typically used together with the LaTeX system. Each entry consists of the type (e.g. article, inproceedings etc.), a citation key, and key-value pairs for the other characteristics of an entry. Each BibTeX entry can be represented as a typed container that holds a set of key-value pairs. A word processing document is any text-based document compiled using a word processor software. Markdown is a lightweight markup language for writing formatted documents inspired to conventions of web posting. We can interpret a document (compiled with a Word processor or specified in Markdown syntax) as a sequence of blocks (e.g. paragraphs, lists, headings, code blocks). Some blocks (e.g. list items) contain other blocks, whereas others contain inline contents (e.g. links, images etc.). A document can be represented as a list of typed containers. Where the type denotes the kind of block (e.g. heading, paragraph, emphasised text, link, image etc.); lists are needed for specifying the sequence of the blocks. Additional attributes such as the depth of the header or the type of list (bullets, numbers, etc...) can be also supported, relying on the key-value structure. The following shows an example of Markdown: Directory structures can be interpreted as collections of folders (containers) or file names (values). This allows to develop façade-based data access to explore the content of a local directory. The same concept applies to archives (e.g. zip files). As discussed, Facade-X is the same for all the surveyed formats. façade-based data access acts as a virtual endpoint that can be queried exactly as a remote SPARQL endpoint, through a SERVICE call to the special IRI-schema x-sparql-anything:. The related URIschema supports an open-ended set of parameters specified by the facade implementations available. A minimal example only includes the resource locator, and guesses the data source type from the file extension. Options are embedded as key-value pairs, separated by comma. These can incorporate a set of parameters, to allow the user to configure the system (for example, to indicate that the system should consider the first line of a CSV as headers). \n 3 SPARQL Anything: system overview \n In what follows, we describe the process of executing façade-based data access with SPARQL Anything. After that, we summarise the main features of the system. SPARQL Anything extends the Apache Jena framework44 with a special query executor capable of handling façade-based data access. The system behaves essentially as a standard SPARQL 1.1 query engine, receiving as input a query and returning either a SPARQL Result Set (for SELECT/ASK queries) or an RDF stream (for CONSTRUCT/DESCRIBE types of queries). Figure 2 describes the general architecture of the system. The process starts with an input query, which is handled by the ARQ engine of Apache Jena. The query is parsed into an abstract algebra, and operations are executed according to their internal dependencies, where the output of one operation is served as input to the dependent one. Our system intercepts attempts to execute any SERVICE pointing to a x-sparqlanything: IRI. However, configuration can be expressed to SPARQL Anything either via the IRI schema (as in the previous example query) or by using triples having as subject the special entity fx:properties, as in the following examples: In the last line, the input data is supposed to come from a previous operation. Therefore, if there are configuration variables that are not been evaluated, the execution is postponed. When all the input parameters are ready, our process starts. The system first gathers all configuration options. We refer the reader to the official documentation on the web for a complete list of configuration options, including format-specific ones (“SPARQL Anything software documentation” 2022). Next, the system identifies the input source. Currently, SPARQL Anything supports input from within the SPARQL query – content, or by defining a command to be executed on the host machine (process STDOUT is streamed to the triplifier), or by specifying a resource URL – option location. In the case the location is an HTTP URL, the resource is resolved via a full fledged HTTP client. HTTP client options include setting the HTTP method (GET< POST, PUT,...), passing authentication options and query parameters, and setting HTTP request headers such as Accept: application/json; charset=utf-8. Through this component, SPARQL Anything is capable of supporting querying complex Web APIs. The output of the request can be interpreted as any of the content types supported. Finally, any other URLs are resolved via the underlying Java IO URL connection library. Independently from the method to obtain a resource, the input is passed to a triplifier. In this phase, the system uses the option mediatype or tries to guess the format from the file extension, if available. The re-engineering of the input is performed by the triplifier according to the available configuration options. By default, the process triplifies all the content, materialising a view in-memory, then executes the query (the inner part of the SERVICE clause) and returns a query solution, like any other SPARQL operations. When the input is too large to be loaded all in-memory, the user has two possibilities. The on-disk option instructs the system to save the façade-based RDF into a temporary local triple store database (Jena TDB2), and then execute the query on the database. In the alternative, the slice option can be invoked, and the content is partitioned and the triplification and query execution performed on each one of the parts separately (currently supported only for CSV, JSON, and XML). The output is streamed together so that external operations can continue in the same way as with an execution with a complete view. In all cases, the triplification process only preserves the part of the Façade-X RDF view that is mentioned in the input query – triple-filtering. Specifically, if no quad pattern in the SERVICE clause matches a given generated RDF statement, this is not included in the materialised view (this feature, enabled by default, can be disabled via the option strategy, see the documentation (“SPARQL Anything software documentation” 2022). We illustrate the main features of SPARQL Anything, referring to the official documentation for further details (“SPARQL Anything software documentation” 2022). Users can customise the behaviour of façade-based data access according to a set of options. In addition to the ones already mentioned, options include preparing string values (trim-strings), generate IRIs instead of BNodes (blank-nodes=false), specifying the input charset, or a custom namespace instead of the default xyz:. With the option null-string, users can request to ignore triples having as value the given string (for example, an empty string or the string N/A). Additional metadata can be extracted (e.g. from EXIF annotations in image files) via the metadata=true option. The system allows to query the following file formats in plain SPARQL 1.1: XML, JSON, CSV, HTML, Excel, Text, Binary, EXIF, File System, Zip/Tar, Markdown, YAML, Bibtex, DOCx. Users can customise the behaviour of the triplifiers with formatspecific options. For example, JSON can be filtered passing a JsonPath expression (json.path), while XML with XPath (xml.path), HTML content with a CSS selector (html.selector), and plain text via a regular expression (txt.regex) or a string delimiter (txt.split). The CSV triplifier can be applied to any char-separated format (e.g. TSV) with the option csv.delimiter. HTML pages can be loaded with a virtual browser before querying, allowing to parse content produced via JavaScript (html.browser). SPARQL Anything includes a full fledged HTTP client to query Web APIs (options include http.header.*, http.auth.*, http.query.*, and others). SPARQL Anything exposes an extensive set of functions in addition to the ones already provided by Apache Jena, for example, the magic property fx:anySlot to match the value of any container membership property. These include shorthand functions for building RDF nodes – fx:entity, fx:literal – and querying and manipulating container membership properties (fx:before, fx:after, fx:prev, fx:next), string manipulations and hashes, and others (Wood et al. 1998). SPARQL Anything can be used as a Command Line Interface or via as a SPARQL Endpoint, featuring Apache Jena Fuseki. The CLI supports additional features that allow to combine the output of a SPARQL Anything query as the input of another one, designing rich data flows. Output formats (-f) can be JSON, XML, CSV, TEXT, TTL, NT, NQ, and result can be saved to a file (-o). For example, query in Figure 1 can be executed as follows: The system supports parametrized queries using the BASIL syntax convention (Daga, Panziera, and Pedrinaci 2015). Users can specify a SPARQL Result Set file to provide variable parameter values (option -i) or specify values inline (-v). When present, the query is pre-processed by replacing parameters with values from the input file (or values), and repeated for each of the provided bindings. Parameter values can be used in the output file name (-p). In addition, it is possible to reuse content from a previously performed transformation and execute the query against an existing (set of) RDF files (option -l or --load). The option requires the path to one RDF file or a folder including a set of files to be loaded. When present, the data is loaded in memory and the query executed against it. \n 4 Showcase \n In this Section we provide references to a set of real-world use cases implemented with SPARQL Anything. The Tate Gallery Collection open data This showcase (Daga, 2022) provides examples of using SPARQL Anything to query open data from the Tate Gallery collection (Daga, 2022). The repository shows basic features such as the use of the option csv.headers=true. In addition, it showcases more advanced features demonstrating how to (a) query local CSV and JSON files together to build a knowledge graph of artists and artworks metadata using Schema.org and (b) build a SKOS taxonomy from artworks’ topics distributed in thousands of JSON files. Showcased features include incorporating binary data in the RDF graph and dynamically generating the location option from previous façade-based data access operations, among others. The Irish Museum of Modern Art Website Data can be anywhere! In this showcase, implemented during the SPICE project (Daga et al. 2022), we developed a knowledge graph of artists and artworks scraping the website at http://imma.ie. The code shows how to query an HTML web page using a custom CSS selector (option html.selector), and the use of BASIL (Daga, Panziera, and Pedrinaci 2015) variables in parametrised queries. In addition, the use case developed demonstrates how CLI commands can be used in sequence to build complex knowledge graph construction pipelines. Through these features, we extract artists names and pages, from which we scrape additional metadata, including the list of artworks’ pages’. These are visited next to complete the KG with artworks’ information Generating a Knowledge Graph from The Proposition Bank (PropBank) This showcase features the popular PropBank corpus of linguistic frames45. The input for the process is a release of the PropBank dataset, typically released as a single zip file containing a folder which stores all the XML files, one for each frame. The query shows how to chain multiple façade-based data access operations, exploiting key features of SPARQL Anything such as archive and directories querying (to get all XML files from archive), iterate on the solution of one operation to feed it into another (query each one of the XML) and project all transformations into a CONSTRUCT graph template. Scraping Webpages with SPARQL This online guide46, explores advanced usage of the HTML triplifier, showing features such as the headless browser. In addition, it demonstrates the use of list functions such as fx:before, fx:next, and others. Querying YAML metadata embedded in GitHub Markdown files Projects on GitHub typically include a README.md file. The Polifonia project publishes an ecosystem of tools and data for the computational treatment of musical cultural heritage. Such collection of material is spread over a number of GitHub repositories exposing a collection of components, each one described in a documentation Markdown file, annotated according to an annotation schema. This showcase demonstrates how to query Markdown and YAML annotations contained within by chaining multiple façade-based data access operations with a single SPARQL Anything query, exploiting the content option. The query47 traverses a local file system (where all the relevant repositories are included) in search of .md files, extracts the YAML front matter, and transforms the annotations according to Façade-X . Relevant RDF is projected into a KG of component types. Musical scores feature extraction (MusicXML) Musical scores are an excellent example of a complex data object. In (Ratta and Daga, 2022), the authors explore the application of SPARQL Anything to extrapolate musical features such as (a) extracting melodic information, (b) extracting N-grams of musical information, (c) supporting the analysis of those N-grams and (d) populate a musical note ontology (code available on GitHub48). \n 5 Engagement with target users \n A survey was conducted in order to engage with Semantic Web practitioners and SPARQL developers and users and also gain some initial insights into their data transformation requirements and perception and use of existing tools. There were 27 completed responses to the survey. A fuller account of the survey can be found in (Daga et al. 2021). Participants covered a diverse range of expertise. 37% rated their expertise in transforming data to RDF as high or very high, and 36% as low or none. 51.8% were frequent or very frequent users of SPARQL 1.1. 33.3% rated their expertise in SPARQL 1.1. as high or very high, and 37% as low. The software most commonly used for data transformation was custom code written for the specific task, followed by RML and SPARQL Generate. Participants transformed datasets of different sizes, from less than 10MB (18.5% of participants) through to over 1GB (also 18.5% of participants). 33% transformed the data into fewer than 1 million triples. 22.2% generated over 100 million triples. A set of questions in the survey asked participants to rate the importance of various usability characteristics of systems for transforming data into RDF. 51.8% considered it very important or essential that the system should minimise the languages and syntaxes needed. 70.3% considered it very important or essential that the system be easy to learn. 55.5% considered it essential or very important that the system can support new types of data sources without changes to the mapping language. A final set of questions asked participants to rate the usability of three notations for transforming non-RDF data into RDF: RML, SPARQL Generate and SPARQL Anything. 29.6% rated data transformations specified in SPARQL Anything code as very easy to understand, 63% as easy to understand. 4.8% rated the SPARQL Generate code as very easy, 40.7% as easy. 7.4% rated the RML code as very easy, 22.2% as easy. In summary, this small survey of a sample of target users covering a range of expertise and experience suggests a potentially receptive audience for an easy-to-use method for transforming a range of data sources without the need to learn additional languages and syntaxes. 6 The Open-Source project SPARQL Anything is produced by a community of contributors as open source software distributed under the commercial-friendly Apache Licence 2.0. The project is managed on GitHub at this address: https://github.com/SPARQL-Anything, and can be cited via its related entry in Zenodo (“SPARQL Anything software” 2022), following good practices of Open Science and FAIR data management policies. The official documentation is published via Readthedocs.io49. The development activity started in November 2020 and has continued since then with a steady increment of contributions from people external to the original team, including practitioners from the industry. The GitHub project has currently 12 watchers, 5 forks, and 132 stars. In one month (March 2023, time of this submission), the project had 72 unique visitors and was cloned by 25 unique users. \n 7 The industry perspective: a field report \n In this Section we report on the experience of a software engineer who joined the SPARQL Anything open source project recently, working within the context of an US-based IT company active in the healthcare sector. The US company team included, apart from software engineers, two ontologists and a data scientist, all collaborating for constructing RDF KGs for about a year and a half. It took the team several months to find a method for constructing RDF that worked well for them. Initial experiments involved R2RML/RML based tools, including Mapeathor50 (Iglesias-Molina et al. 2020), RML Mapper51, and Ontop52 (Calvanese et al. 2017). At first, the ontologists did not implement their own mappings. Instead, they would annotate sample data with the triples they would like to produce and then one of the software engineers would use one of the RML based tools to implement the mappings. However, the ontologists were unable to revise the mappings and any subsequent evolution required intervention by the software engineers. Indeed, SPARQL and Bash were the only common languages everyone on the team used. The team was changing tools on each project, in search for a better solution. The discovery of SPARQL Anything had a huge impact on the workflow, allowing the team to transform more sources and doing it more quickly. Now the ontologists and data scientist implement their own mappings and everyone participates in the maintenance of the mappings. One episode illustrates well the impact that the tool had on the team. A few months ago, the company needed graph of data from a paying service to which the company had access. Unfortunately, the data was only available via a web browser as HTML. This brought the opportunity of contributing to the SPARQL Anything open-source project by expanding the capabilities of the HTML triplifier with a headless browser option, illustrated in a blog post (see Section 4), written with the ontologists in mind, demonstrating how to scrape a webpage (with content produced by javascript) using SPARQL Anything53. As a result, the ontologists read the blog post and created a SPARQL construct query, without any assistance from the software engineers, to construct a KG with the content of the webpage of interest. Recently the team were involved in a short workshop where one of the goals was to produce a graph of a product catalog54. Because the team was producing triples so quickly the workshop was mainly spent working with a subject matter expert on the data in order to carve nature at its joints. The team was happy to adapt their workflow so that ontologists could use SPARQL Anything in complex data integration pipelines, sometimes involving millions of data points. In one case, healthcare data is integrated from a relational database that gets exported (as CSV) weekly. The ontologists wrote one or more construct queries for each table. To overcome the in-memory limits of the tool, a script split each CSV file into files of a few thousand rows. SPARQL Anything was run over each file (with a configurable number of parallel processes) to produce quads. As an example, with 5 parallel processes one table with about 10 million rows took 4-5 hours to complete and produced 82 million quads. The team used this technique before SPARQL Anything had the slice option. As that option gets more optimised, the team looks forward to having SPARQL Anything doing the slicing rather than doing it as a pre-processing stage. \n 9 Conclusions \n We introduced SPARQL Anything, a reusable research software supporting façade-based data access over heterogeneous data sources, for the benefits of knowledge engineers. The Façade-X approach, introduced in (Daga et al. 2021) has received the attention of the community, confirmed by the user survey (Section 5), and it is unique in the landscape of solutions for RDF re-engineering. SPARQL Anything has a low complexity, compared to existing solutions: the framework allows others to save significant coding effort. In addition, the design methodology allows it to be extended to support an ever-ending set of formats. The performance of the first version of the system was evaluated and discussed in (Daga et al. 2021). Future work is going to focus on optimisation strategies and improving the internal machinery. However, the current version of the system is ready to tackle real-world problems, such as the ones encountered in our field report. In addition, engaging with semantic web practitioners highlighted the valueto-users of essential and unique aspects of our tool. With SPARQL Anything, Semantic Web practitioners are relieved of the problem of content re-engineering and can finally focus on generating high-quality semantic data in plain SPARQL.",
		"summary": "What should a data integration framework for knowledge engineers look like? Recent research on Knowledge Graph construction proposes the design of a façade, a notion borrowed from object-oriented software engineering. This idea is applied to SPARQL Anything, a system that allows querying heterogeneous resources as-if they were in RDF, in plain SPARQL 1.1, by overloading the SERVICE clause. SPARQL Anything supports a wide variety of file formats, from popular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by alternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying Web APIs with high flexibility, parametrised queries, and chaining multiple transformations into complex pipelines. In this paper, we describe the design rationale and software architecture of the SPARQL Anything system. We provide references to an extensive set of reusable, real-world scenarios from various application domains. We report on the value-to-users of the founding assumptions of its design, compared to alternative solutions through a community survey and a field report from the industry. ",
		"id": "UUID5"
	},
	{
		"document": "1. Introduction In recent years there has been rapid progress in tackling the problem of protein folding: predicting the three-dimensional structure of a protein based only on its sequence data. Machine learning methods have been shown to achieve new standards of accuracy, often predicting complicated structures with experimental level accuracy e.g. (Jumper et al., 2021; Baek et al., 2021; Lin et al., 2022). De novo protein design can be described as an inverse folding problem: given a backbone structure with atomic coordinates, what amino acid sequences will fold to this shape? Solving this problem has implications in a range of important applications in protein engineering, from the design of novel enzymes, receptors and other biomolecules with tailored functions, to drug discovery for efficiently exploring binder designs that can target a specific protein’s active site. It is also of high relevance in applications of certain machine learning models, such as generative diffusion models. Residues are then often represented as non-specific backbone nodes in the form of Cα atom coordinates and *Equal contribution 1Exscientia, Oxford Science Park, Oxford, OX4 4GE, UK. Correspondence to: Fred´ eric A. Dreyer ´ <fdreyer@exscientia.co.uk>. The 2023 ICML Workshop on Computational Biology. Honolulu, Hawai’i, USA, 2023. Copyright 2023 by the author(s). frame orientations such that a sequence needs to be designed from the predicted structure (Watson et al., 2023; Lin & AlQuraishi, 2023; Yim et al., 2023). Inverse folding has historically been approached as an energy optimisation problem, using tools such as Rosetta to search for combinations of amino acid identities and conformations that result in the lowest energy for a given structure (Alford et al., 2017). Recent advances in deep learning have offered an alternative data-driven approach which results in significantly faster and often more accurate models (Ingraham et al., 2019; Strokach et al., 2020; AnandAchim et al., 2021; Jing et al., 2021; Hsu et al., 2022). In this study, we consider the structured graph neural network method used in the recent ProteinMPNN model (Dauparas et al., 2022) to build an antibody-specific inverse folding model. Antibodies, illustrated in Figure 1, are proteins that play a central role in the adaptive immune system due to their ability to bind to a wide range of pathogens. They consist of two heavy and two light chains divided into domains of approximately 110 amino acid residues, with the N-terminal domains, called variable regions, each containing three hypervariable loops known as the complementarity determining regions (CDRs) that make up the majority of the antigen binding (Sela-Culang et al., 2013). We create our model by fine-tuning on data from the Structural Antibody Database (SAbDab) (Dunbar et al., 2013; Schneider et al., 2022), as well as on paired sequences from the Observed Antibody Space (OAS) (Kovaltsuk et al., 2018; Olsen et al., 2022) using structures predicted by the ABodyBuilder2 model (Abanades et al., 2022). We show that our approach provides state-of-the-art performance in predicting the amino acid residues in the CDR loops, with notable improvement in sequence recovery and designability for the third CDR loop of the heavy chain (CDR-H3), the most sequentially and structurally diverse loop and typically the most important region for antigen recognition (Narciso et al., 2011; Tsuchiya & Mizuguchi, 2016). We publish our model weights to allow further downstream applications (Dreyer et al., 2023). 2. Inverse folding with ProteinMPNN We follow the recent approach introduced in the ProteinMPNN paper (Dauparas et al., 2022), illustrated in Figure 2. arXiv:2310.19513v1 [q-bio.BM] 30 Oct 2023 AbMPNN ICML-WCB 2023 Figure 1. Overview of an antibody structure and its domains. This model is based on structured transformers using a message passing neural network (MPNN) as the aggregation function (Ingraham et al., 2019) with the addition of an order agnostic decoding and edge updates in the encoder. It aims to provide an autoregressive decomposition of the distribution of a protein sequence s given a backbone 3D structure x p(s|x) = Y i p(si |x, s<i) (1) where p(si |x, s<i) is the conditional probability of the amino acid si at decoding step i, and s<i = {s1, . . . , si−1} refers to previously decoded residues.1 These probabilities are parametrized using two components, an encoder that computes node and edge embeddings from structural information and a decoder that autoregressively predicts the next decoded residue given the preceding decoded letters and structural embeddings. The backbone encoder takes as input distances between atoms as edge features, along with an all zero node vector. The node features are updated by the three-layer MPNN. In contrast to Ingraham et al. (2019), ProteinMPNN also then updates the edge features, before iterating through three layers of encoders. The input edge features consist of the distances between N, Cα, C, O, and virtual Cβ atoms of the 48 nearest residues in Euclidean space, decomposed in Gaussian radial basis functions. These inter-atom distance features are accompanied by a relative positional encoding in terms of the one-hot encoded distance in primary sequence space of two residues, with an additional token signaling whether they are in different chains. A key change of ProteinMPNN to the original structured transformer implementation is the use of an order agnostic decoding, i.e. at each step the next residue to be predicted is chosen randomly among the remaining ones, with the full context of previous predictions given on either side. Of particular relevance for antibodies with defined framework 1Note that this is not the same as their index in the chain, as residues are decoded in random order. Figure 2. Schematic representation of the data processing steps and model architecture. regions, this allows effective inference on structures with fixed regions where part of the sequence is known, which can then be provided as context. The model is then trained to minimize the categorical cross entropy loss per residue. 3. Training on antibody data Here we train an antibody specific variant of ProteinMPNN, which we will refer to as AbMPNN, that can predict valid antibody sequences and achieve improved accuracy in the variable region, notably for the CDR loops that determine antigen specificity. We fine-tune the original ProteinMPNN model weights on antibody data. We consider two different datasets: • Antigen-binding fragments in complex from SAbDab. We filter the full database for antibodies in complex with a protein antigen, and obtain 3500 complexes after removing redundant fragments and filtering out those with an experimental resolution worse than 5 A. ˚ • 147919 paired heavy and light chain variable regions with unique concatenated CDRs from the OAS database. These are not experimentally resolved structures and do not contain epitopes, as the OAS contains only sequence data, but have been predicted using ABodyBuilder2 and structures are available as part of the ImmuneBuilder dataset (Abanades, 2022). We use the North definition of CDRs throughout this study (North et al., 2011). We fine-tune our model in two steps: first through an initial fine-tuning on the OAS structure predictions, for which we have a large number of natively paired heavy and light variable sequences modelled by ABodyBuilder2. We then further fine-tune the model on a small number of experimentally resolved antigen binding fragments in complex. In both cases the model is trained to predict the full variable domain, with the epitope given as context in the SAbDab training. AbMPNN ICML-WCB 2023 CDRL1 CDRL2 CDRL3 CDRH1 CDRH2 Region 0 1 2 3 4 5 6 scRMSD ProteinMPNN AbMPNN native CDRH3 Region 0 2 4 6 8 10 Model 20 0 20 40 60 80 Interface energy relative to native (kcal/mol) ProteinMPNN OAS only SAbDab only AbMPNN Figure 3. Left: Comparison of the backbone self-consistency RMSD between the original backbone and the ABodyBuilder2 structure predictions for the sequence obtained from ProteinMPNN (blue) and AbMPNN (red) as well as for the original sequence (purple). Right: Difference in interface energy as calculated by Rosetta of the heavy and light chains between each model prediction and the native sequence. The dashed line indicates a 5 kcal/mol threshold, and outliers are not displayed. To filter the OAS antibodies, we first remove any duplicate entries with identical concatenated CDR sequences. For filtering the SAbDab antibody complexes, we remove antibodies that have both an identical concatenated CDR sequence and epitope sequences with greater than 90% similarity by CD-HIT (Fu et al., 2012). Here the epitope is defined as the residues in each antigen chain with backbone atoms within 6 A of the antibody backbone. ˚ We next cluster the antibodies in both datasets using CDHIT at 90% similarity for the concatenated CDR sequences. This results in 107961 clusters for the OAS dataset, and 1701 clusters for the SAbDab in complex dataset. Finally, we split these into training, validation and test sets, with a ratio of 8-1-1. To ensure that there is no pollution between the OAS and SAbDab dataset, we ensure that any cluster with a sequence in the SAbDab dataset that would have been clustered into a OAS training or validation cluster is placed into the SAbDab training set2 . We fine-tune our AbMPNN model starting from the original ProteinMPNN model weights, first on the OAS dataset, then on the SAbDab dataset. Both of these fine-tuning steps use an Adam optimiser. We reduce the learning rate by a factor of 10 if the validation loss does not improve for 10 epochs (5 for the OAS data), starting from an initial learning rate of 5 × 10−4 for the OAS fine-tuning and of 10−4 for the SAbDab training step. Each epoch consists of 1000 randomly selected antigen binding fragments. 2This corresponds to 50 SAbDab clusters before filtering for resolution. 4. Designability and accuracy study We now report on the results obtained from our fine-tuned AbMPNN model, and analyze its accuracy and robustness. For each model, we run inference to predict 20 variations of the CDR sequences for 20 complexes from different clusters in the SAbDab testset, using a sampling temperature of 0.2 at inference. We start by studying the designability of sequences predicted by AbMPNN. To this end, we assess the selfconsistency of the model similarly to Trippe et al. (2022), by giving the sequences as input to a structure prediction model, in this case ABodyBuilder2, and compute the RMSD between the original backbone and the predicted structure. This is shown in Figure 3 (left), where the RMSD is shown separately for each CDR loop. Here the self-consistency RMSD for the original sequence is displayed in purple, and provides a benchmark of how well a model can perform on this designability test. The boxes show the first and third quartile, with the median shown as a horizontal line, and outliers3 indicated as dots. Both the original ProteinMPNN model and the fine-tuned AbMPNN are shown, and we can observe a clear improvement of about 20% on the median RMSD of CDR-H3 for the fine-tuned model, with smaller improvements visible for all other CDRs. To further probe designability, we compute the interface energy of the heavy and light chains of these structures using Rosetta (Alford et al., 2017). The difference of the 3Outliers are defined as values more than 1.5 times the interquartile range away from the first or third quartile. AbMPNN ICML-WCB 2023 CDRL1 CDRL2 CDRL3 CDRH1 CDRH2 CDRH3 Region 0% 20% 40% 60% 80% 100% Amino acid recovery rate ProteinMPNN OAS only SAbDab only AbMPNN Figure 4. Left: Sequence recovery of the original ProteinMPNN model and of AbMPNN, shown separately for each CDR loop with the mean value indicated by a white circle. Right: Fraction of mismatching conformations as predicted by SCALOP for CDR L1-3 and CDR H1-2. interface energy for each model with the structure predicted from the native sequence is shown in Figure 3 (right), where we also display intermediate models fine-tuned on only one of the antibody datasets.4 Here we note that the antibody interface energy improves after each fine-tuning step, with our final AbMPNN model producing structures predicted to be more stable than the original ProteinMPNN model. Interestingly, 40% of AbMPNN sequences are within 5 kcal/mol or less of the original sequence interface energy, compared with only 20.5% for ProteinMPNN, indicating that the AbMPNN model is better at recovering light and heavy chain residue contacts and generating stable heavy chain - light chain dimers. In Figure 4 (left), we show the proportion of correctly recovered residues across each CDR loop. A notable improvement can again be seen for the fine-tuned model, with sequence recovery rates around 60% for the AbMPNN model while the original ProteinMPNN model only has a recovery rate of about 40% across CDR loops. Despite having a relatively low RMSD with the input structure when predicted using ABodyBuilder2, the sequences predicted by the original ProteinMPNN model have significant differences to the original antibody sequences. We examine the conformations of the CDR loops. We cluster the non-H3 CDRs into canonical forms using SCALOP (Wong et al., 2018). Canonical clusters are built from sequence space using position-specific scoring matrices. The fraction of predicted sequences that do not match the canonical form of their original sequence is shown in Figure 4 (right). Here we observe a large improvement in recovery of the correct canonical cluster with the fine-tuned 4 For readability reasons, we do not display the ∼ 5% percent of outliers with sometimes very large interface energy values. AbMPNN model. We consider the validity of the predicted antibody sequences, by redesigning the full variable region of our previous test set, this time including the framework region. We then annotate these sequences using ANARCI (Dunbar & Deane, 2015), a tool for numbering variable domains, and find that while every sequence predicted by AbMPNN is recognised as an antibody sequence, 16.8% of those predicted by ProteinMPNN can not be annotated due to errors in the framework region of either the light or heavy chain. 5. Conclusions In this article, we have introduced an inverse folding model specifically adapted to antibodies to predict their sequences based on structural backbone information. This model follows the architecture of the recent generic protein model ProteinMPNN, and is fine-tuned on experimental structures of antigen binding fragments as well as numerical structure predictions of variable antibody fragments derived from the Observed Antibody Space. We showed that with a few changes and appropriate retraining, our AbMPNN model can set new state-of-the-art benchmarks for designability and amino acid sequence recovery, particularly for the hypervariable CDRH3 loop. We discussed the canonical forms of CDRs and showed that a sequence-based conformational clustering achieves excellent recovery of the original sequence cluster for all available CDR loops. Antibody-specific inverse folding tools can provide a powerful approach to AI-driven drug discovery, notably by improving designability and affinity of existing binders, and as a final sequence recovery step for de novo structural models (Watson et al., 2023). We release the weights of our model to allow for the use of this work in other downstream applications (Dreyer et al., 2023).",
		"summary": "We consider the problem of antibody sequence design given 3D structural information. Building on previous work, we propose a fine-tuned inverse folding model that is specifically optimised for antibody structures and outperforms generic protein models on sequence recovery and structure robustness when applied on antibodies, with notable improvement on the hypervariable CDRH3 loop. We study the canonical conformations of complementarity-determining regions and find improved encoding of these loops into known clusters. Finally, we consider the applications of our model to drug discovery and binder design and evaluate the quality of proposed sequences using physics-based methods.",
		"id": "UUID6"
	},
	{
		"document": "This research paper introduces an innovative payload deployment mechanism tailored for sounding rockets, addressing a crucial challenge in the field. The problem statement revolves around the need to efficiently and compactly deploy multiple payloads during a single rocket launch. This mechanism, designed to be exceptionally suitable for sounding rockets, features a cylindrical carrier structure equipped with multiple independently operable deployment ports. Powered by a motor, the carrier structure rotates to enable radial ejection of payloads. In this paper, we present the mechanism’s design and conduct a comprehensive performance analysis. This analysis encompasses an examination of structural stability, system dynamics, motor torque, and power requirements. Additionally, we develop a simulation model to assess payload deployment behavior under various conditions. Our findings demonstrate the viability and efficiency of this proposed mechanism for deploying multiple payloads within a single sounding rocket launch. Its adaptability to accommodate diverse payload types and sizes enhances its versatility. Moreover, the mechanism’s radial deployment capability allows payloads to be released at different altitudes, thereby offering greater flexibility for scientific experiments. In summary, this innovative payload radial deployment mechanism represents a significant advancement in sounding rocket technology and holds promise for a wide array of applications in both scientific and commercial missions. ",
		"summary": "1 Introduction Sounding rockets are an essential tool for scientific research and exploration, providing a means of collecting data and conducting experiments in the upper atmosphere and beyond[1]. These rockets are designed to reach high altitudes and provide a brief period of microgravity, allowing researchers to study a wide range of phenomena, including atmospheric composition, ionospheric properties, and the behavior of biological and physical systems in a low-gravity environment. Sounding rockets typically consist of a small rocket motor, a payload compartment, and a deployment mechanism.[2] The rocket motor provides the thrust necessary to lift the rocket to the desired altitude, while the payload compartment houses the instruments and experiments to be carried out during the flight. The deployment mechanism is used to release the payloads at the appropriate time and altitude, allowing researchers to collect data and conduct experiments in real time. One of the key advantages of sounding rockets are their cost-effectiveness and rapid deployment time. Compared to other space launch vehicles, sounding rockets are relatively inexpensive and can be launched quickly and efficiently, allowing researchers to conduct experiments on a relatively tight schedule. Additionally, sounding rockets can be launched from a variety of locations around the world, making them an attractive option for researchers in remote or hard-to-reach areas. thrustMIT’s sounding rocket Altair is shown in Fig.1.[3] Payload deployment mechanisms are a crucial part of sounding 3Corresponding Author. Version 1.18, October 31, 2023 Fig. 1 Sounding Rocket: Altair Journal of Applied Mechanics Copyright © 2023 by ASME PREPRINT FOR REVIEW / 1 arXiv:2310.19673v1 [eess.SY] 30 Oct 2023 1 A 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 B C D E F A B CDE F Dept. Technical reference Created by Approved by Document type Document status Title DWG No. Rev. Date of issue Sheet Payload 1 18-08-2023 Part Drawing Released 17-08-2023 17-08-20231/1Radial Deployment MechanismALTAIR-PAYLOADPayload Pranav Tanvi Pranav All Linear Dimensions are in mm. Linear Tolerances are ±0.1 mm unless specified. Angular Tolerances are ±1° unless specified Parts List Item Qty Part Name Description Material Mass 1 1 Servo plate 03 v4 Laminate, Blue, Matte 73.395 g 2 1 Servo v5 PA 11 - Nylon- HP 11-30 (with EOS P 396 3D Printer) 34.136 g 3 1 Rack and pinion v5 7.324 g 4 1 Push joint v2 PAEK Plastic 41.76 g 4 1 2 3 3D PRINTED MGR 996R, MAX TORQUE 11 Kg/cm 3D PRINTED 3D PRINTED SCALE- 1:2180mm Orthographic view Fig. 2 Draft of the Radial Deployment Mechanism rocket technology, allowing researchers to carry out experiments and collect data in the upper atmosphere and beyond. There are two main types of payload deployment mechanisms: axial and radial.[4] Axial deployment mechanisms are the most common type and are typically used for payloads that require a specific orientation or trajectory. In an axial deployment mechanism, the payload is mounted to a support structure at the top of the rocket and is ejected through a port in the rocket body. The ejection process is typically initiated by a pyrotechnic device, which releases a locking mechanism and allows the payload to be ejected from the rocket. This type of mechanism is ideal for payloads that require a specific orientation, such as telescopes or cameras, and can provide high accuracy and stability during deployment. Radial payload deployment mechanisms represent a significant advancement in the field of sounding rocket technology, enabling the deployment of multiple payloads during a single launch. Unlike traditional deployment mechanisms, which typically require the rocket to be pointed in a specific direction for ejection, radial deployment mechanisms enable payloads to be released in any direction, providing greater flexibility and versatility for scientific experiments. A typical radial payload deployment mechanism consists of a cylindrical carrier structure with several deployment ports, each of which can be opened independently. The carrier structure is rotated by a motor, which enables the payloads to be ejected radially.[5] The mechanism can be adapted to accommodate a variety of payload types and sizes, making it highly versatile and suitable for a range of scientific experiments. One of the key advantages of a radial payload deployment mechanism is its ability to release payloads at different altitudes during a single launch. This provides greater flexibility for scientific experiments, allowing researchers to study phenomena at different altitudes and compare results. The mechanism is also useful for releasing payloads in different directions, which can be particularly important for experiments that require a specific orientation or require the payload to be deployed away from the rocket. In addition to its flexibility and versatility, a radial payload deployment mechanism also offers several advantages in terms of reliability and efficiency. The mechanism is simple in design and can be easily integrated into existing rocket systems, making it an attractive option for researchers and scientists. [6] The radial ejection of payloads also reduces the risk of damage or interference to other payloads during deployment, which can be a concern in traditional deployment mechanisms. Despite its many advantages, radial payload deployment mechanisms do require careful design and analysis to ensure their effectiveness and safety. This typically involves a thorough analysis of the structural stability and dynamics of the system, as well as an evaluation of the motor torque and power requirements. Simulation models can also be used to assess the deployment behavior of the payloads under various conditions, ensuring that the mechanism is optimized for the specific requirements of each experiment. Overall, radial payload deployment mechanisms represent a significant advancement in the field of sounding rocket technology, offering greater flexibility, versatility, and reliability for scientific experiments. As research continues to push the boundaries of what is possible, it is likely that radial payload deployment mechanisms will become increasingly important and widespread in the field of space exploration and scientific research.[7] To establish the contextual framework and underscore the pertinence of this novel mechanism, a comprehensive review of recent literature concerning radial deployment mechanisms will be undertaken. Subsequent sections will delve into the intricacies of the novel mechanism, elucidating its design, operation, and potential for disruptive impact across diverse academic and practical spheres. 2 Background Theory In the realm of engineering and technology, the evolution of deployment mechanisms stands as a testament to innovation in response to the ever-advancing demands of various applications. One such mechanism that has garnered substantial interest is the radial deployment mechanism. Distinguished by its capacity to unfold in a circular pattern, this mechanism offers precision, spatial efficiency, and reliability, characteristics that elevate its significance 2 / PREPRINT FOR REVIEW Transactions of the ASME in a variety of contexts. The core function of deployment mechanisms is to facilitate the controlled and precise expansion of components, bridging the conceptual and functional aspects of systems, devices, and structures. This pivotal role endows them with a centrality that profoundly impacts overall system performance. Traditional deployment mechanisms, whether linear, rotational, or hybrid, have demonstrated effectiveness in numerous applications. However, they often encounter limitations in scenarios that demand radial expansion. The extant catalog of radial deployment mechanisms, while serving admirably, faces challenges posed by the evolving complexity of modern technology. The contemporary landscape is characterized by a surge in research and development endeavors aimed at transcending the constraints of traditional radial deployment mechanisms. Driven by the imperative for heightened efficiency, reliability, and versatility, engineers and scientists are venturing into uncharted territories. They seek to engender innovations that not only surmount extant limitations but also redefine the very essence of radial deployment mechanisms. In the pursuit of this objective, this research paper embarks on a comprehensive exploration of a novel radial deployment mechanism. The research aims to facilitate the progression of radial deployment mechanisms and expand their applicability. The primary objective of a radial deployment mechanism is to ensure a compact and secure stowage of payloads during the launch and transit phases. Once the spacecraft reaches its designated orbit or location, the mechanism activates, initiating the controlled release of the payload. This deployment process is crucial for achieving proper satellite positioning, optimizing sensor exposure angles, and ensuring successful mission execution. The theory behind a radial deployment mechanism involves several key considerations: • Compact Stowage: The mechanism must allow the payload to be compactly stored within the spacecraft’s confines to optimize space utilization during launch and minimize launch vehicle payload fairing constraints. The design should account for potential volume limitations and weight restrictions[8]. • Controlled Expansion: The deployment process requires precision and control to avoid undue stress on the payload and spacecraft. The mechanism must ensure gradual, predictable, and symmetric expansion to prevent vibrations or disturbances that may affect the satellite’s stability. • Structural Integrity: The materials and components used in the radial deployment mechanism should exhibit high strength-to-weight ratios and excellent fatigue resistance. The mechanism must withstand the harsh conditions of launch, space environment, and potential mechanical loads during deployment. Fig. 3 CAD of Radial Deployment Mechanism Fig. 4 Physical Prototype of the Mechanism • Reliability and Redundancy: To ensure mission success and mitigate potential failures, the mechanism often incorporates redundant components and mechanisms. Fail-safe designs and robust deployment sequencing are crucial for the overall reliability of the system. • Mechanism Actuation: Radial deployment mechanisms can be actuated through various means, such as pyrotechnics, shape memory alloys, mechanical springs, or motors. The chosen actuation method should be well-suited for the specific mission requirements and should demonstrate repeatability and reliability. • Thermal Considerations: The mechanism must account for thermal variations in the space environment to ensure consistent and predictable deployment performance. Thermal expansion and contraction of materials should be factored into the design to avoid undesirable effects on deployment accuracy. The successful implementation of a radial deployment mechanism plays a pivotal role in a wide range of space missions, including communication satellites, Earth observation satellites, scientific instruments, and interplanetary probes. The efficiency, reliability, and precision of the mechanism contribute significantly to the overall success of the mission and the scientific or operational objectives of the deployed payloads. 3 Discussions & Ongoing Research The deployment of payloads in sounding rockets has been a critical aspect of scientific research and exploration for decades. Traditional payload deployment mechanisms have primarily relied on axial deployment methods, where the payload is ejected from the rocket’s nose cone. While these methods have been effective for many missions, they are not without limitations. Early sounding rockets employed simple pyrotechnic mechanisms for payload separation. These mechanisms were relatively Journal of Applied Mechanics PREPRINT FOR REVIEW / 3 reliable but lacked precision in controlling the exact moment of deployment. This imprecision could lead to unintended variations in the data collected during experiments. As the demand for more precise scientific measurements increased, so did the need for improved deployment systems. In response to these challenges, researchers began to explore alternative deployment mechanisms, including radial deployment systems. The concept of radial deployment involves releasing the payload from the rocket’s sides rather than its nose. This approach offers several advantages, including increased payload capacity and improved control over the deployment process. Recent developments in radial deployment mechanisms have shown promise. For instance, the use of spring-loaded mechanisms and miniaturized thrusters has allowed for controlled and precise deployment. Researchers have also explored the use of smart materials that can change shape or expand upon activation, providing a novel approach to payload deployment. Despite these advancements, challenges remain in optimizing radial deployment systems for various payload sizes and mission requirements. The historical evolution of payload deployment mechanisms highlights the need for ongoing research in this field to address these challenges and advance the capabilities of sounding rockets for scientific exploration. This section provides an overview of the historical context of payload deployment mechanisms and sets the stage for the examination of recent advances and ongoing research efforts in the field. 4 Methodology As mentioned in the previous section, the deployment of payloads can be achieved through two possible means: axially or radially. Prior to discussing radial deployment, it is important to explore the reasons for abandoning axial deployment. Axial deployment[9], whereby the top structure or nosecone is fairing[10], is the most commonly used method by prestigious space agencies such as ISRO, NASA, and ESA. However, this method presents several technical challenges, particularly with regard to safe pyrotechnic ejection. For orbital or larger sub-orbital rockets, the space constraints are not as significant, thus allowing for numerous pyro ejecting bolts and systematic wiring. When the nosecones or top halves split into two sections vertically, they are held together by chord wires for sub-orbital rockets, but are left as debris in the case of orbital rockets. This method is relatively expensive due to its constituents of ejection powder, precise manufacturing, and small-scale components. Moreover, axial deployment requires redundancy for the ejection mechanism, which is essential but also adds to the budget. For a researcher, the method or mechanism need not be cost-effective, but for a manufacturer, this is an equally important property. Appendix A contains a flow chart representing the radial deployment mechanism in detail. Fig. 5 Orthogonal View of CAD of Radial Deployment Mechanism 4.1 Design Justifications. The main goal of designing the electrical system is to create a reliable and effective control mechanism for the deployment process. With the specific target of releasing the payload at a particular altitude, the system is crafted to coordinate the actions of two critical elements: the servo motors that manage the door and the push arm in charge of payload ejection. This customized approach guarantees that the payload is released precisely and on time once the designated altitude is reached. For our dynamic control strategy, we’ve opted for the Teensy 4.1 microcontroller as our central control unit due to its versatility and processing power. This microcontroller seamlessly handles the deployment sequence by continuously receiving real-time altitude data from the BMP388 altimeter sensor, a crucial element in determining the rocket’s altitude. The stream of altitude data from the BMP388 altimeter sensor serves as the bedrock of the deployment system’s decision-making process. By processing this data in real-time, the microcontroller gains a clear understanding of the rocket’s ascent. This dynamic information empowers the system to execute commands precisely, optimizing the deployment sequence according to preset altitude thresholds. This design approach reaches a crucial juncture—initiating payload deployment at the designated altitude. The Teensy 4.1 microcontroller takes charge, sending precise commands to the MG996R servo motors. Strategically placed to control the door and push arm, these servos respond promptly to the microcontroller’s cues. This coordinated response results in the graceful opening of the doors and the smooth release of the payload. In summary, our electrical system’s design is geared towards achieving controlled, precise, and timely payload deployment. By selecting the Teensy 4.1 microcontroller as the central control unit and integrating altitude data from the BMP388 altimeter sensor, we’ve harmonized technology with engineering prowess. This design ethos enhances deployment accuracy and underscores the innovative spirit propelling our research. 4.2 Physical Testing of Mechanism. To evaluate the performance of the mechanism, a test was conducted under a no-load condition to check whether the force produced by the servo motor was sufficient to push the CanSat out without any hindrance.[11] The desired outcome of the test was to ensure that the mechanism worked smoothly and met the deployment requirements.[12] The primary benefit of this approach is its ability to dispense with redundancies, as the rack can repeatedly translate and push the cansat out through the use of delays to the servo motor. However, a significant drawback is that the experiment within the cansat was also tilted, rendering it useless without proper orientation.[13] Additionally, the tilted payload may slide during the rocket’s initial impulse and come to rest on the door, thereby adding an extra force on the door hinge in addition to the aerodynamic forces acting upon it. Ultimately, this method was deemed inefficient for our purposes.[14] During the initial test, a few other minor issues were also identified and analyzed. Firstly, the gear had interference while rotating with the plate, as the plate compressed upon tightening with the screws by at least 3mm when attached to the top bulkhead. Additionally, the movement between the rack and the rack slot was not as free as it should be, as the 3D printed materials at extrusions needed to be sanded for improved performance.[15] Test 02 involved updates to the previous version, which included a re-design of the plate that held the servo and push mechanism to allow sufficient space for gear rotation and rack sliding. The mating of rack and pinion was improved to achieve almost 100% mating, and lubricants were used to ensure proper sliding of the rack. The force was sufficient for the no-load condition of the Cansat. Multiple views for CAD model of deployment mechanism discussed in this paper can be seen in Fig. 3 and Fig.4. The gear dimensions used in the test were a module of 2, pitch diameter of 4 / PREPRINT FOR REVIEW Transactions of the ASME Fig. 6 Schematic of Electrical System 20 mm (approximately 0.79 in), root fillet radius of 1.1 mm (about 0.04 in), gear thickness of 5 mm (about 0.2 in), hole diameter of 6 mm (about 0.24 in), and 10 teeth. The rack dimensions were a length of 66.48 mm (about 2.62 in), a height of mating sections of 4.274 mm (about 0.17 in), addendum of 2.137 mm (about 0.08 in), and dedendum of 2.137 mm (about 0.08 in).[16] The system was evaluated using an 11.7 V Lipo battery, and calculations were performed to determine the power and torque required. Using the formula 𝑃 = 2𝜋𝑁 · 𝑇 60000 (1) where RPM (N) was 4000, power (P) was calculated to be 13.7 kW. Torque (T) was determined using the formula 𝑇 = 𝑃60000 2𝜋𝑁 (2) which resulted in 32.7 Nm. The module (m) was calculated using the formula Diametrical pitch (Dp) / number of teeth (z), where Dp was 20 mm and z was 10 teeth. Therefore, m was equal to 2 mm. To calculate the force tangential (Ft), the formula 𝑇 = (𝐹𝑡) ∗ 𝑟 𝑝 (3) was used, where rp was the pitch radius of 10 mm. Thus, Ft was calculated to be 3270.63 N. The gear angle, alpha, was 20 degrees. To determine the resultant force (F), the formula 𝐹 = 𝐹𝑡 𝑐𝑜𝑠(20) (4) was used, resulting in F being equal to 3480.5 N. The Teensy 4.1 is a microcontroller board that utilizes the ATmega328P microcontroller and features a set of hardware specifications. The board includes 14 digital input/output pins, six of which can be used as PWM outputs, six analog inputs, a USB connection, a power jack, an ICSP header, a reset button, and a 16 MHz quartz crystal. It is designed to be easily connected to a computer with a USB cable or powered with an AC-to-DC adapter or battery. The Schematic for the entire electrical system can be seen in Fig. 6.[17] The microcontroller utilized in the Teensy 4.1 is the ATmega328P. The board operates at 5V and can be powered through a USB connection or an external power supply with a recommended input voltage of 7-12V and a limit of 6-20V. The digital I/O pins provided are 14, six of which support PWM output, while there are six analog input pins. The DC current for each I/O pin is 20 mA, while the DC current for the 3.3V pin is 50 mA. The flash memory is 32 KB (ATmega328P), with 0.5 KB being utilized by the bootloader. The SRAM is 2 KB (ATmega328P), and the EEPROM is 1 KB (ATmega328P). The clock speed is 16 MHz, and there is a built-in LED at pin 13. The dimensions of the board are 68.6 mm (length) x 58.4 mm (width), and it weighs 25 g.[18] The Teensy 4.1 board can be powered via a USB connection or an external power supply with automatic power source selection. The recommended input voltage range is 7-12V, and the board can operate on an external supply from 6 to 20 volts. However, if supplied with less than 7V, the 5V pin may supply less than five volts, and the board may become unstable. If using more than 12V, the voltage regulator may overheat and damage the board. [19] The MG996R Servo Motor is a high-torque digital servo with dimensions of 40.7 x 19.7 x 42.9 mm.[20] It features metal gearing, resulting in a high 10kg stalling torque, with upgraded shockproofing and a redesigned PCB and IC control system that makes it more accurate than its predecessor, MG995[21]. The gearing and motor have also been upgraded to improve dead bandwidth and centering. This high-torque standard servo can rotate approximately 120º (60º in each direction) and operates at 4.8V to 7.2V. It features a double ball bearing design for increased durability.[22] 4.3 Influence of New Mexico Conditions on Aerodynamic Forces and Structural Response. The physical testing conditions under investigation are crucial to understanding the impact of New Mexico’s unique atmospheric properties Journal of Applied Mechanics PREPRINT FOR REVIEW / 5 Table 1 Comparison of Sounding Rocket Payload Deployment Methods Deployment Method Advantages Disadvantages Axial Deployment • Simple mechanism • Minimal interference with payload • Highly accurate and stable deployment of payload. • Requires rocket to be in specific orientation • Pyrotechnic charge required depending on payload size. • Multiple payloads cannot be deployed in a single launch. Fairing Deployment • Enhanced payload protection during ascent • Suitable for larger payloads • No specific orientation of rocket required. • Increased complexity • Additional weight • Potential aerodynamic constraints • Explosive bolts used in fairing. Radial Deployment (Favorable) • Relatively simpler mechanism • Reduced aerodynamic constraints • No pyrotechnics involved • Multiple payloads can be deployed in a single launch • No specific orientation of rocket required • Payload protection during ascent. • Potential risk of tangling. Fig. 7 Concept of Operations on aerodynamic forces and structural behavior. These conditions directly affect the calculated values and mechanical responses, providing insights into the performance of objects subjected to such conditions. The following subsection highlights the key parameters and their effects as observed in the study. 4.3.1 Aerodynamic Force Calculation. The aerodynamic force (𝐹𝑑) acting on an object can be determined using the equation: 𝐹𝑑 = 1 2 × 𝑝 × 𝑣 2 × 𝐶𝑑 × 𝐴 (5) Where: • p: represents density (𝑘𝑔/𝑚 3 ), normally 1.293 𝑘𝑔/𝑚 3 but in the specific case of New Mexico, it is 0.96 𝑘𝑔/𝑚 3 . • Deformation, indicating structural flexibility, is observed to be 0.0166 m. • Equivalent stress, representing the uniform stress distribution causing the same deformation, is calculated as 8.2045 MPa. • The equivalent strain, which quantifies the deformation in terms of strain, is computed as 4.3113 × 10−5 mm. Consequently, the calculated aerodynamic force 𝐹𝑑 amounts to 370 N. 4.3.2 Structural Response and Mechanical Parameters. The conditions during testing also influence the structural behavior of the object. These effects are captured through various mechanical parameters, providing a comprehensive understanding of the object’s performance. • The moment applied is measured at 1480 N mm. • Deformation, indicating structural flexibility, is observed to be 0.0166 m. • Equivalent stress, representing the uniform stress distribution causing the same deformation, is calculated as 8.2045 MPa. • The equivalent strain, which quantifies the deformation in terms of strain, is computed as 4.3113 × 10−5 mm. In conclusion, this subsection has delineated the physical testing conditions pertinent to the research, elucidating the alterations in aerodynamic forces and structural responses resulting from the unique atmospheric conditions in New Mexico. These conditions collectively contribute to a comprehensive understanding of the studied object’s behavior in this specific environment. 4.4 Calculations. In this section, we present the calculations and mechanism employed for the successful deployment of the CanSat payload. The primary objective was to ensure that the CanSat could be released and translated radially outwards with sufficient torque and force. The following calculations were conducted to determine the feasibility of the deployment mechanism.[23] 4.4.1 Force Calculation. : The force required to lift and deploy the CanSat can be calculated using the formula F = m * g, where 'm' is the mass and 'g' is the acceleration due to gravity. For our setup, the total mass (carrier + CanSat) was determined to be 960g. Thus, the force required was calculated as follows: 𝐹 = 0.96𝑘𝑔 ∗ 9.8𝑚/𝑠 2 (6) 𝐹 = 9.408𝑁 (7) 6 / PREPRINT FOR REVIEW Transactions of the ASME 4.4.2 Torque Calculation. : To assess the torque necessary for the linear actuator to unlock the CanSat door, we used the formula T = F * r, where 'r' is the pitch diameter. The pitch diameter was measured to be 0.01 m. The torque required for door unlocking was calculated as follows: 𝑇 = 0.09408𝑁𝑚 (8) 𝑇 = 9.408𝑘𝑔 𝑓 𝑐𝑚 (9) Comparing the calculated torque with the specifications of the MG 995 servo, which has a stall torque of 11 kgfcm at 6v, it was evident that our setup fell well within the acceptable range. The power source used for the mechanism was a LiPo battery with an output of 7.4V, regulated to the appropriate voltage level using a step-down voltage regulator. 5 Results After the successful lift-off at the apogee of the mission trajectory, a pivotal phase of the deployment process was initiated. This phase adhered to a meticulously designed protocol. Upon the command signal, promptly dispatched to the dedicated linear actuator, the door mechanism engaged. This command signified the unlocking of the door, a pivotal event that marked the commencement of payload deployment.[24] Following the door’s systematic opening, a calculated temporal delay of 2 seconds was introduced, accounting for an optimal synchronization with the subsequent steps. Subsequently, a precisely timed pulse triggered the carrier mechanism, orchestrated through a robust rack and pinion system. This intricate assembly displayed its functionality as it embarked on a continuous translation process, successfully performing its duty threefold. The dynamic interplay of this mechanism effectually facilitated the sequential and controlled ejection of the CanSat payloads, ensuring a synchronized radial trajectory. The mechanical engineering underpinning this deployment process demonstrated exceptional efficacy. The torque exerted by the carrier mechanism proved sufficient to enact the desired radial translation of the CanSat payloads. Their subsequent independent descent was unobstructed, allowing them to descend unimpeded until reaching the terrestrial surface. In the context of simulation, the prevailing forces and conditions exhibited by the mechanism obviated the necessity for comprehensive simulations. The absence of necessary complex forces mitigated the requirement for extensive simulations. Notably, the hinge component, exposed to atmospheric conditions, exhibited a minimal extrusion of no more than 3mm. This discreet displacement yielded negligible aerodynamic implications. However, with meticulous engineering insight, a static structural simulation was deemed essential for further analysis. The objective Fig. 8 Static Structural Simulation representing Equivalent Von-Mises Strain Fig. 9 Static Structural Simulation Representing Total Deformation was to ascertain the dynamic response of the mechanism under uncontrolled door-opening conditions and to identify potential weak points in the hinge’s structural integrity. The application of drag forces to both the hinge and the exposed door facilitated a comprehensive assessment, yielding enlightening results (see Fig. 8, Fig. 9 and Fig. 10). 6 Conclusions Following the lift-off at the apogee, a carefully controlled sequence of actions was initiated to facilitate the successful ejection of the CanSat payload. The process involved a series of welldefined steps, which are discussed in detail below: 1. Signal Transmission and Door Unlocking: Upon reaching the apogee, the designated signal was transmitted, prompting the activation of the linear actuator responsible for unlocking the CanSat door. This critical step allowed for the subsequent release of the payload. 2. Carrier Mechanism Activation: After the door’s opening, a brief delay of 2 seconds was introduced to ensure the stability of the system. Subsequently, a pulse was sent to the carrier mechanism, employing a rack and pinion mechanism. This pulse initiated the continuous translation of the carrier mechanism three times, effectively pushing the CanSat payload outwards. 3. Radial Translation of CanSat Payload: The carrier mechanism’s torque output was meticulously calibrated to generate sufficient force, resulting in the smooth radial translation of the CanSat payload away from the launch vehicle. This controlled radial movement was vital to ensure the safe and precise ejection of the payload. 4. Independent Descent of Payload: Following its radial translation, the CanSat payload entered into a state of independent freefall until it made contact with the ground. This unassisted descent phase allowed the payload to undergo its intended mission objectives without any further interference. It is important to note that the successful ejection process and the subsequent payload descent are integral components of the overall mission’s success. The precise execution of these actions guarantees the accurate deployment of the payload, ensuring the gathering of reliable data during its descent phase. Future Directions In the pursuit of refining and enhancing this innovative deployment mechanism, a series of avenues beckon for exploration. One such avenue entails the expansion of the mechanism’s capabilities through thoughtful extensions. By extending the pushing rod, the mechanism could potentially accommodate stacked payloads, creating an avenue for multiple payloads to be released in tandem. This extended translatory motion holds promise for more sophisticated mission profiles and expanded scientific data gathering. Journal of Applied Mechanics PREPRINT FOR REVIEW / 7 Fig. 10 Static Structural Simulation representing Equivalent Elastic Strain Furthermore, the door mechanism itself presents an opportunity for improvement. By integrating hydraulic components into the system, a controlled and deliberate lowering motion can be achieved, harmonizing with the unlocking of the latch. This heightened level of control, realized as the linear actuator is gracefully retracted, could potentially enhance overall mission dynamics and the precision of deployment. In conclusion, the intricacies of the deployment mechanism have been successfully navigated, yielding a controlled and synchronized radial ejection of the CanSat payloads. The culmination of engineering ingenuity, empirical validation, and prospective enhancements underscores the significance of this mechanism in furthering the realm of payload deployment technologies. Acknowledgment We sincerely thank thrustMIT, Manipal Institute of Technology, and the Manipal Academy of Higher Education for their invaluable support and resources that greatly facilitated the successful completion of this research. Additionally, we would be remiss to not highlight the contributions of Diya Parekh, and Hrishikesh Singh Yadav. Their unwavering assistance and commitment to fostering a conducive research environment have been instrumental in shaping the outcomes of this study.",
		"id": "UUID7"
	},
	{
		"document": "I. INTRODUCTION Disturbance widely exists in mechanical systems and aeronautic systems, such as industrial robotic manipulators [1], motion servo systems [2], disk drive systems [3], missiles [4], and spacecrafts [5]. It deteriorates the control performance significantly and even induces system instability. Hence, disturbance rejection has been a key component of the controller design. One approach for mitigating disturbance is to use feedforward control, which can be effective when the disturbance is measurable. However, in some cases, the cost of sensors may be prohibitive or direct measurement of the disturbance may not be possible. An alternative approach is to design robust controllers. However, there is an intrinsic trade-off between the controller’s robustness and its nominal performance, which is referred to as the single degree of freedom control structure [6]. The disturbance observer (DOB) is a promising technique to address the aforementioned issues. It acts as an Manuscript received September 10, 2022; revised June 20, 2023; accepted September 23, 2023. The work of D. Shi was supported by National Natural Science Foundation of China under grants 62261160575 and 61973030. This work of Y. Lou was supported by the National Key Research and Development Program of China under Grant 2020YFB1313900. Shilei Li, and Ling Shi are with the Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China (e-mail: slidk@connect.ust.hk, eesling@ust.hk). Dawei Shi is with the School of Automation, Beijing Institute of Technology, China (e-mail: daweishi@bit.edu.cn). Yunjiang Lou is with the State Key Laboratory of Robotics and System, School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen 518055, China (e-mail: louyj@hit.edu.cn). Wulin Zou is with Xeno Dynamics, Control Department, Xeno Dynamics Co., Ltd, Shenzhen 518055, China (e-mail: zouwulin@xeno.com). add-on component for the baseline controller and can increase its robustness against disturbance and recover the controller’s nominal performance when disturbance disappears. Therefore, it is favored by many researchers. Various linear disturbance observers have been designed by different researchers for different applications, which include the frequency domain-based DOB [7], the extended state observer (ESO) in active disturbance rejection control (ADRC) [8], [9], the unknown input observer (UIO) in disturbance accommodation control (DAC) [10], the Kalman filterbased disturbance observer (KF-DOB) [11]; the uncertainty disturbance estimator (UDE) [12], and the equivalent input disturbance estimator (EID) [13]. The frequency domain-based DOB was proposed by Ohishi et al. [7] and the inverse of the plant model accompanied by a filter was used to estimate the lumped disturbance; the ESO was designed by Han [14] for the purpose of estimating the lumped disturbance; the UIO was developed by Johnson [10] which estimated the state and the disturbance by assuming that the disturbance dynamics was the a priori knowledge; the KF-DOB [11] also estimated the state and the disturbance simultaneously by involving the disturbance as a new state and constructing an augmented state Kalman filter; the mechanism of the UDE [12] was quite close to the frequency domain-based DOB where a filter was utilized to make the disturbance estimation implementable; the EID [13] can be regarded as an alternative to the ESO by deliberately selecting the parameters. One can refer to [6] for a more comprehensive review about the DOB. Although many linear disturbance observers are available with different characteristics, they usually use a constant gain to update the estimate of the state or disturbance [7]–[13], which intrinsically induces a trade-off among disturbance estimation, state estimation, and noise suppression. The constant gain cannot handle the time-varying noise characteristics effectively. For example, the KF-DOB is derived under the well-known minimum mean square error (MMSE) criterion and is the minimum variance estimator under Gaussian assumption (note that the KF gain is constant under the steady state). However, its performance degenerates significantly with heavy-tailed noise induced by disturbance. A prescription for this issue is to re-tune the noise covariance matrices at the price of sacrificing the nominal performance. However, this method usually is unsatisfactory especially when outliers are involved. Many robust techniques have been applied to KF to increase its robustness, such as the modified influence function-based KF by Masreliez et al. [15], Huber-based KF [16], [17], robust Student’s t-based KF [18], [19]. Those methods improve the robustness of the KF by a bounded arXiv:2310.19586v1 [eess.SY] 30 Oct 2023 2 influence function [15]–[17], or by employing the heavy-tailed Student’s t-distribution [18], [19]. However, they mainly focus on non-Gaussian noises existing in all measurements or all process channels, rather than only existing in some specific ones. The correntropy provides a potential tool for improving the robustness of the KF. It originates from information-theoretic learning (ITL) and has been widely used as a robust cost for machine learning [20], adaptive filtering [21], regression [22], and state estimation [23], [24]. Correntropy is a local similarity measure of two random variables, which captures higher-order statistics [24] compared with the conventional second-order error moment and hence is more suitable for applications with heavy-tailed noise. A good property of the correntropy is that the correntropy induced metric (CIM) varies from an ℓ2 norm to an ℓ0 norm with the growth of the error [25]. Using this property, the maximum correntropy KF (MCKF) was derived in [24], [26], [27]. Its sequential form, Chandrasekhartype recursion, and square-root form were derived in [28]– [30]. It was also extended to the nonlinear system with MCEKF [31], MC-UKF [32], MC-GHKF [33], and was applied to systems with state constraints [34], [35], distributed state estimation [36], and interacting multiple model [37]. The above correntropy-based algorithms are mainly derived under the Gaussian kernel. Actually, they can also be derived based on others kernels, e.g., the generalized Gaussian kernel and Cauchy kernel. The generalized Gaussian kernel first used by Chen et al. [38] for adaptive filtering. After that, it had been utilized in active noise control [39] and multiple-hypothesis detection [40]. The Cauchy kernel was initially employed by Wang et al. [41] for target tracking, and then it was utilized in the distributed filtering subject to cyber-attacks [42]. Unfortunately, although these correntropy-based algorithms are robust to outliers or heavy-tailed distributions in general, they use a unified kernel bandwidth for all channels, which are very conservative when only some channels contain nonGaussian noises and the others are Gaussian. To handle this issue, in our previous works [43]–[45], we extended the definition of correntropy from random variables to random vectors and presented the multi-kernel maximum correntropy Kalman filter (MKMCKF) where the bandwidth of each channel can be tuned flexibly. With this modification, the behavior of the CIM in different channels can be designed independently. More specifically, the infinite bandwidth is applied to the Gaussian channel so that the CIM in this type of channel is an ℓ2 norm. As for the non-Gaussian channel, a suitable bandwidth is selected so that the CIM changes from an ℓ2 norm to an ℓ0 norm with the growth of the error. The MKMCKF is not conservative compared with the traditional correntropy-based algorithms. However, it still has some defects: it is derived based on the Gaussian kernel, which is less powerful than the generalized Gaussian kernel; the connection between the objective function (or the kernel parameter selection) and noise distribution for a general estimation problem is vague; the detailed convergence analysis of the fixed-point algorithm in the MKMCKF is missing. This paper aims to cope with the aforementioned problems. We first extend our previous multi-kernel correntropy under the Gaussian kernel to a generalized multi-kernel correntropy (GMKC) under the generalized Gaussian kernel and provide the corresponding generalized loss (GL) function. Then, we provide some important properties of the GMKC and build a connection between the GL and the noise distribution based on the maximum a posteriori probability (MAP). Finally, we derive a generalized multi-kernel maximum correntropy Kalman filter (GMKMCKF) for disturbance estimation and give a sufficient condition for the convergence of the fixedpoint algorithm in GMKMCKF. We also analyze the complexity and kernel parameter sensitiveness of the GMKMCKF, and compare it with the ESO [8], KF-DOB [11], MCKF [24], and particle filter (PF) [46]. The major contributions of this paper lie in three aspects: Firstly, the proposed “multi-kernel correntropy” methodology can significantly mitigate the conservatism of traditional correntropy. Secondly, we associate the GL with the noise distribution based on MAP, which illustrates the conservatism of the traditional correntropy and provides general guidance for kernel parameter selection. Thirdly, the convergence analysis of the fixed-point iteration in the GMKMCKF is given and its performance is compared with some benchmark methods. The comprehensive contributions of this paper are summarized as follows: 1) We find that the noise distribution in the disturbance channel is heavy-tailed and the KF cannot serve this type of noise effectively from the modeling perspective. To cope with this issue, we propose the GMKC and GL, demonstrate their properties (Theorem 1–5), and compare the GL with the least mean p power (LMP) criterion. 2) We reveal that the traditional KF can be derived by an MSE criterion and is sensitive to heavy-tailed noises. To increase its robustness, we derive a novel estimator GMKMCKF by employing the GL as the cost function, which is an extension of the MCKF and MKMCKF (Theorem 7) but less conservative. 3) The convergence of the fixed-point algorithm in GMKMCKF is provided (Theorem 9). Moreover, the algorithm complexity is given and the parameter sensitiveness is numerically analyzed. Simulations on a robotic manipulator verify the effectiveness of the proposed method. The remainder of this paper is organized as follows. In Section II, the GMKC and GL are introduced and their properties are given. In Section III, the GMKMCKF is derived and its convergence and complexity are discussed. In Section IV, simulations are conducted to verify the effectiveness of the proposed method. In Section V, a conclusion is drawn. Notations: The transpose of a matrix A is denoted by A′ . The a priori and the a posteriori estimate of state x is denoted by x − and x +, respectively. The vector with l dimensions is denoted by R l and the matrix with m rows and n columns is denoted by R m×n. X ≻ 0 (X ≽ 0) denotes X is positive definite (semi-positive definite) matrix. The Gaussian distribution with mean µ and covariance Σ is denoted by N (µ, Σ). The Laplace distribution with location parameter µ and scale parameter s is denoted by L(µ, s). The uniform distribution with bounds a and b is denoted by U(a, b). The p norm of a vector x or matrix A is denoted by ∥x∥p or ∥A∥p. The p power of p vector norm x is denoted by ∥x∥ p p . The 3 expectation of a random variable X is denoted by E(X). II. GENERALIZED MULTI-KERNEL CORRENTROPY In this section, we first provide the traditional Kalman filter. Then, we formulate an estimation problem with unknown process disturbance. Finally, we introduce the GMKC and GL and provide their properties. A. Kalman Filter We consider a linear time-invariant (LTI) system: xk+1 = Axk + wk yk = Cxk + vk (1) where xk ∈ R n is the state, yk ∈ R m is the measurement, and wk and vk are Gaussian noises with wk ∼ N (0, Qk) and vk ∼ N (0, Rk) where Qk ≽ 0 and Rk ≻ 0. The pair (A, √ Qk ) is assumed to be controllable and (A, C) is observable. The initial state x0 ∼ N (0, Q 0 ) is assumed to be uncorrelated with wk and vk for k > 0. Denote the measurement set until time step k as {yk} := {y1, y2, . . . , yk}. In KF, we have xˆ − k = Axˆ + k−1 (2a) P − k = AP + k−1A ′ + Qk (2b) Kk = P − k C ′ (CP − k C ′ + Rk) −1 (2c) xˆ + k = ˆx − k + Kk(yk − Cxˆ − k ) (2d) P + k = (I − KkC)P − k (2e) where xˆ − k and xˆ + k is the a priori and a posteriori estimate of xk, and P − k and P + k is the a priori and a posteriori estimate of error covariance at time step k, respectively. B. Problem Formulation In many practical applications, systems contain unknown process disturbance, i.e., xk+1 = Axk + Γdk + wx,k yk = Cxk + vk (3) where dk ∈ R q is the unknown disturbance, Γ ∈ R n×q map the disturbance to the state, and wx,k and vk are nominal noises. To estimate the disturbance, we treat the disturbance as a new state and construct the augmented state as x¯k = [d ′ k , x′ k ] ′ (the aim of putting dk ahead of xk can be found in Theorem 2 in [43]). We assume that disturbance dynamics follows dk+1 = dk + wd,k (4) since we do not have the a priori knowledge about the disturbance dynamics (the assumption dk+1 = dk is equivalent to ˙d = 0 in the continuous case which is employed in many existing works [1], [47]). Then, we obtain x¯k+1 = A¯x¯k + ¯wk yk = C¯x¯k + ¯vk (5) with A¯ =  I 0 Γ A  , C¯ =  0 C  where w¯k = [w ′ d,k, wx,k] ′ and v¯k = vk. In the conventional Kalman filter, the initial state x0 is assumed to be Gaussian with N (0, Σ0) and the noises follow wd,k ∼ N (0, Qd), wx,k ∼ N (0, Qx), vk ∼ N (0, R). Moreover, process noise [w ′ d,k, w′ x,k] ′ , measurement noise vk and initial state x0 are mutually uncorrelated for k ≥ 0. However, the Gaussian assumption of wd,k usually is unrealistic. In a practical application, the disturbance dynamics generally is time-varying with dk+1 = f(dk) + w ∗ k where f(dk) is a timevarying nonlinear function and w ∗ is the nominal disturbance noise (conventionally it is assumed to be Gaussian). We use the nominal model (4) for implementation since we are not accessible to the practical disturbance dynamics f(dk). In this case, wd,k = f(dk) − dk + w ∗ k which should be heavy-tailed since it contains both the modelling mismatch f(dk) − dk and the noise w ∗ k . A possible representation for this kind of distribution may be the ϵ-contaminated mixture model. For example, we can use the uniform distribution U(a, b) to capture the noise induced by the modelling mismatch f(dk) − dk and employ the Gaussian distribution N (0, Qw) for the nominal noise w ∗ k , which follows wd,k ∼ ϵU(a, b) + (1 − ϵ)N (0, Qw), 0 < ϵ < 1 where ϵ is a weight that determines the probability of a distribution occurs. Unfortunately, this mixture model cannot be approximated by a single Gaussian distribution effectively (see Fig. 1). This reveals that KF is not an efficient estimator for this type of noise from the perspective of noise distribution. Moreover, in some cases, the nominal noises may follow other types of distributions (e.g., the heavy-tailed distribution in [48], Laplace distribution in [49]). All these factors deteriorate the estimation accuracy of the KF-DOB. -5 0 5 0 0.1 0.2 0.3 0.4 Fig. 1. Approximating a ϵ-contaminated mixture model using a Gaussian distribution. The Gaussian distribution is obtained by minimizing the mean squared error 1 N PN k=1  p(wd,k) − pˆ(wd,k) 2 where p(wd,k) = 0.37U(−5, 5) + 0.63N (0, 0.5) is the target distribution and pˆ(wd,k) is a Gaussian distribution to be determined. The estimated Gaussian distribution pˆ(wd,k) follows N (0, 1.03). One can see that the Gaussian distribution cannot approach a general mixture distribution effectively. Remark 1. Although this paper focuses on process disturbance estimation, measurement disturbance actually can also be handled in a similar way by augmenting the disturbance as a new state (see Section III of [43] for details). A conventional way for the heavy-tailed distribution in the disturbed channel is to enlarge the covariance matrix Qd. However, this would 4 deteriorate its estimation performance with the disappearance of disturbance [50]. C. Generalized Multi-kernel Correntropy The correntropy is originally defined as a local similarity measure for two random variables X, Y ∈ R with joint distribution FXY (x, y) C(X, Y ) = E[κ(X, Y )] = Z κ(x, y)dFXY (x, y) where κ(x, y) is a shift-invariant Mercer kernel, and x and y are the realizations of X and Y . A common used kernel is the Gaussian density function with κ(x, y) = Gσ(x, y) = exp(− e 2 2σ 2 ) where e = x−y and σ is the kernel bandwidth. In the case that only N samples of x(k) and y(k) are available and FXY (x, y) is unknown, the correntropy can be obtained by the simple mean estimator C(X, Y ) = 1 N X N k=1 κ  x(k), y(k)  = 1 N X N k=1 Gσ  x(k), y(k)  . In this paper, we adopt the generalized Gaussian density (GGD) function as the kernel κ(x, y) = Gα,β(x, y) = exp(−|e/β| α ) (6) where e = x − y is the error, α > 0 is the shape parameter, and β > 0 is the kernel bandwidth. Under the GGD, we define the GMKC for random vectors X , Y ∈ R l as follows (the i-th element of X and Y is Xi and Yi , respectively): C¯(X , Y) = X l i=1 E[˜κi(Xi , Yi)] = X l i=1 Z κ˜i(xi , yi)dFXiYi (xi , yi) with κ˜α,βi (xi , yi) = β α i Gα,βi (xi , yi) = β α i exp(−|ei/βi | α ) where xi and yi are realizations of Xi and Yi , ei = xi − yi is the realization error, and βi is the i-th bandwidth for Xi and Yi . In a practical application, the joint distribution FXiYi (xi , yi) is not available and only N samples can be obtained. In this case, we can estimate the GMKC as C¯(X , Y) = X l i=1 β α i Cα,βi (Xi , Yi) (7) with Cα,βi (Xi , Yi) = 1 N X N k=1 Gα,βi (xi(k), yi(k)) (8) where Cα,βi (Xi , Yi) is the correntropy for Xi , Yi under α and βi , and xi(k) and yi(k) is the k-th sample of random variables Xi and Yi , respectively. Correspondingly, the generalized loss (GL) can be defined as JGL(X , Y) = X l i=1 β α i (1 − Cα,βi (Xi , Yi)). (9) Remark 2. It is worth mentioning that the proposed GMKC is different from the concept in [51], [52]. The mechanism of our proposed method is to use different kernel bandwidths at different channels, while [51], [52] employ a combination of different kernels to generate a new kernel. The purpose of our method is to reject the heavy-tailed noises in the disturbing channel without sacrificing the performance of the other channels while the aim of [51], [52] is to accommodate more complex error distributions (i.e., skewed distributions, see Fig. 1 in [51]). D. Properties of the Generalized Multi-kernel Correntropy In this section, we provide some properties of the GMKC and GL. Theorem 1. In the case of 0 < α ≤ 2, the GMKC in (7) can be regarded as a weighted summation of the second-order statistic in the mapped feature space. The proof of this theorem is shown in Appendix VI-A. Theorem 2. When setting β α i → ∞, the GL in (9) becomes the expectation of α-order absolute moments with lim β α i →∞ JGL(X , Y) = E∥X − Y∥α α. The proof of this theorem is shown in VI-B. Remark 3. Theorem 2 reveals that when setting all kernel parameters as β α i → ∞, the GL becomes the traditional least mean p-power (LMP) criterion with α = p. One can refer to [53], [54] for more information about the LMP in the design of a filter. Theorem 3. Denote the correntropy induced metric as GCIM(X , Y) = (JGL(X , Y)) 1 2 . Then, it defines a metric in the N-dimensional sample vector space when 0 < α ≤ 2. The proof is shown in Appendix VI-C. The contour plots of JGL(X , 0) 1 α in 2D space with different shape parameters α and different bandwidths βi are shown in Fig. 2. One can see that JGL(X , 0) 1 α behaves like an ℓα norm in the vertical direction when setting β2 to be a big value (i.e., 100). Moreover, it changes from an ℓα to ℓ0 in the horizontal direction when setting β1 to be a relatively small value (i.e., 1). For the traditional correntropy, the contour plot is isotropic since it shares a unified bandwidth [25], which restrains its capability on the system that only some channels contain heavy-tailed noises. On the contrary, the contour plot of the proposed method can be anisotropic by using different bandwidths at different channels, which is very efficient when different channels contain different types of noise distributions. Another advantage of the proposed method is that the GGD is more powerful than the Gaussian density function (since it has an additional shape parameter α) and hence can accommodate more types of noise distributions. E. Influence Function of the LMP and GL In many applications, we have only one measurement at each time instance. In this section, we discuss the property of the LMP and GL in this scenario, i.e., N = 5 0.4 0.4 0.8 0.8 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.6 1.6 1.6 1.6 1.6 1.6 2 2 2 2 2 2 2 2 2.4 2.4 2.4 2.4 -2 -1 0 1 2 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 (a) α = 1, β1 = 1, β2 = 100 0.5 0.5 0.9 0.9 0.9 0.9 1.3 1.3 1.3 1.3 1.3 1.3 1.7 1.7 1.7 1.7 2.1 2.1 2.1 2.1 -2 -1 0 1 2 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 (b) α = 2, β1 = 1, β2 = 100 0.2 0.4 0.4 0.6 0.6 0.8 0.8 0.8 1 1 1 1 1 1 1.2 1.2 1.2 1.2 1.4 1.4 1.4 1.4 1.6 1.6 1.6 1.6 1.8 1.8 1.8 1.8 2 2 2 2 -2 -1 0 1 2 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 (c) α = 4, β1 = 1, β2 = 100 Fig. 2. Contours of JGL(X, 0) 1 α in 2D space with different shape parameters and different bandwidths. The influence function measures the derivative of the loss function with respect to the error [55], [56], and gives a straightforward view of how errors influence the objective function. Therefore, it provides guidance for the objective function design. For the LMP criterion [53], [54], we have JLMP (e) = ∥e∥ p p = X l i=1 |ei | p (10) where e ∈ R l and ei is the i-th element of e. Substituting (8) into (9) with N = 1, we have JGL(e) = X l i=1 β α i (1 − Gα,βi (ei)). (11) The influence functions can be obtained by calculating the gradients ∇JLMP (e) = ∂JLMP ∂e = [ρ1, ρ2, · · · , ρl ] T ∇JGL(e) = ∂JGL ∂e = [γ1, γ2, . . . , γl ] T (12) with ρi = p |ei | p ei , i = 1, 2, . . . , l γi = α exp − |ei |α βα i |ei | α ei , i = 1, 2, . . . , l. Then, we have the following two theorems. Theorem 4. The JGL(e) in (11) is identical to the JLMP (e) in (10) when α = p and β α i → ∞. Moreover, in the case of 0 < α ≤ 1, JGL(e) is concave with e ̸= 0; in the case of α > 1, JGL(e) is convex within the region |ei | ≤ ( α−1 α ) 1 α βi . The proof of this theorem can be found in Appendix VI-D. Remark 4. In many situations, a non-convex loss function is beneficial to strengthen some particular features. For example, the conventional MSE loss gives a linear influence function (i.e., p = 2 in (12)), which provides each residual constant influence and hence cannot eliminate the effect of outliers (if exists). On the contrary, a redescending influence function that is induced by a non-convex loss (e.g., the GL in (11)) is preferable [55]. Existing solutions for non-convex optimization include the fixed-point iteration [24], the gradient descent [51], and the evolutionary algorithms [57]. Theorem 5. The GL in (11) is a differential invex function of e with α > 1 and ei ≤ φ (i = 1, 2, · · · , l) where φ ∈ R + is an arbitrary positive number. The proof of this theorem is shown in Appendix VI-E. We consider the loss function of (11) in one-dimensional case for simplicity. In this case, JGL(e) = β α(1 − Gα,β(e)) and JLMP (e) = |e| p . The graphs of JGL(e), ∇JGL(e), JLMP (e), and ∇JLMP (e) are shown in Figs. 3(a), 3(b), 3(c), and 3(d). One can see that JGL approaches JLMP when setting β = 100 (see Theorem 2), and it changes from ∥e∥ α α to β α with the growth of the error when setting β = 1. The influence function ∇JGL goes towards zero when the error is bigger than α−1 α β and α > 1 (see Theorem 4), and is close to ∇JLMP (e) when the error is very small, which makes the performance of GL is similar to LMP when the error is small, but is highly resistant to outliers when the error is large. Remark 5. The well-known MSE and LMP actually is a subset of the GL. The MSE-based algorithm is sensitive to outliers since its influence function grows linearly with respect to e (note that ∂|e| 2 ∂e = 2e). On the contrary, this effect can be mitigated by the GL by using a relatively small kernel bandwidth with α > 1 since its corresponding influence function goes towards zero with the increment of the error. Due to this property, the GL is a more attractive loss function compared with the LMP and MSE criterion. F. Relationship with the Kalman Filter For the linear system with Gaussian assumption described in (1), based on the Bayes’ theorem, the a posteriori probability of xk with measurement set {yk} has p(xk|{yk}) = p(xk|{yk−1}, yk) = p(xk, {yk−1}, yk) p({yk−1}, yk) = p(yk|xk, {yk−1})p(xk|{yk−1})p({yk−1}) p(yk|{yk−1})p({yk−1}) ∝ p(yk|xk)p(xk|{yk−1}) (13) 6 -2 -1 0 1 2 0 1 2 3 4 5 6 (a) -2 -1 0 1 2 -4 -2 0 2 4 (b) -2 -1 0 1 2 0 1 2 3 4 5 6 (c) -2 -1 0 1 2 -4 -2 0 2 4 (d) Fig. 3. Objective functions and influence functions of JGL and JLMP with different α, β and p. where p(yk|xk) is the probability of yk conditional on the a priori estimate of xk, and p(xk|{yk−1} is the a priori estimate of xk with measurement set {yk−1}. From the perspective of MAP, we have arg max xk p(xk|{yk}) = arg max xk p(yk|xk)p(xk|{yk−1}). Since wk and vk are Gaussian (see the assumptions in (1)), p(yk|xk) and p(xk|{yk−1} should also follow the Gaussian distribution after a linear transformation (see [58]) with p(yk|xk) = exp − (yk − Cxk) ′R −1 k (yk − Cxk)  p (2π)m|Rk| p(xk|yk−1) = exp − (xk − Axˆk−1) ′ (P − k ) −1 (xk − Axˆk−1)  q (2π) n|P − k | (14) where xˆk−1 is the a posteriori estimate of the state at time step k − 1, |Rk| is the determinant of Rk, P − k is the a priori estimate of error covariance, and |P − k | is the determinant of P − k . Due to the fact that the normalization constants in the denominator of (14) are independent with the argument xk, they can be ignored which follows that arg max xk p(xk|{yk}) = arg max xk exp − (yk − Cxk) ′R −1 k × (yk − Cxk)  exp − (xk − Axˆk−1) ′ (P − k ) −1 (xk − Axˆk−1)  . (15) It is equivalent to minimizing the negative log: arg min xk JKF =∥R −1/2 k (yk − Cxk)∥ 2 2+ ∥(P − k ) −1/2 (xk − Axˆk−1)∥ 2 2 . (16) By defining the measurement error er,k and process error ep,k as er,k ≜ R −1/2 k (yk − Cxk) ep,k ≜ (P − k ) −1/2 (xk − Akxˆk−1), (17) we obtain arg min xk JKF =∥er,k∥ 2 2 + ∥ep,k∥ 2 2 . (18) Theorem 6. The KF in equations (2a)-(2e) can be derived by the MSE criterion using (18). The proof of this theorem can be found in some existing works [30], [58], [59]. Equations (13)-(18) reveal that KF is optimal for a linear system with Gaussian noises from the perspective of MAP. However, when the noises wk, vk are non-Gaussian, the probability density function (PDF) in (14) does not hold. In this case, the ℓ2 norm-based loss function is not the best. By analogy the negative logarithm relationship between the noise distribution and the loss function in (14) and (16), we find that the GL induces the following distribution: p(yk|xk) =  cr exp(−JGL,α,βr (er,k)), er,k ∈ Y 0, otherwise p(xk|yk−1) =  cp exp(−JGL,α,βp (ep,k)), ep,k ∈ X 0, otherwise (19) where JGL,α,βr (·) and JGL,α,βp (·) are the generalized loss functions with the N = 1 (details shown in (11)), α ∈ R, βr = [βn+1, βn+2, · · · , βn+m] ′ ∈ R m, and βp = [β1, β2, · · · , βn] ′ ∈ R n. The symbol Y is the domain of er,k, X is the domain of ep,k, and cr and cp are two constants so that p(yk|xk) and p(xk|yk−1) are two proper distributions. The error in (19) is assumed to be bounded and this assumption is reasonable in practical applications. Compared with the Gaussian distribution in (14), equation (19) can represent a wide range of noise distributions. By this assumption with MAP, we have arg max xk p(xk|{yk}) = arg min xk JGL,KF with JGL,KF = JGL,α,βr (er,k) + JGL,α,βp (ep,k). (20) One can see that the ℓ2-norm based objective function in (16) is replaced by the GL function (20). To simplify the visualization of the noise distributions in (19), in one dimensional case, we have p(e) =  c exp(−JGL,α,β(e)), e ∈ E 0, otherwise . (21) A comparison of the p(e), the Laplace distribution, the Gaussian distribution, and the ε-contaminated mixture model is shown in Fig. 4. One can see that p(e) approaches L(0, 1) with α = 1 and β = 100 in Fig. 4(a), and is close to N (0, 0.5) with α = 2 and β = 100 in Fig. 4(b). Moreover, when selecting a proper bandwidth, it can approach a εcontaminated mixture model effectively (see the magenta and the dot blue lines in Fig. 4(a) and Fig. 4(b)). Actually, p(e) approaches a α-order exponential distribution when set 7 β → ∞ since lim β→∞ c exp(−JGL,α,β(e)) = c exp(−e α). When setting a relative small bandwidth, it represents a heavy-tailed distribution with order α. This implies that the shape of p(e) can be controlled by the bandwidth flexibly and it is a suitable representation for a large number of distributions. -5 0 5 0 0.1 0.2 0.3 0.4 0.5 0.6 (a) -5 0 5 0 0.1 0.2 0.3 0.4 0.5 0.6 (b) Fig. 4. Laplace distribution, Gaussian distribution, p(e) in (21) with different α and β, and p(wd,k) using a ε-contaminated mixture model. The error domain is set to be E = [−5, 5] in p(e). The noise distribution pL(wd,k) follows wd,k ∼ 0.2L(0, 1)+ 0.8U(−5, 5) while pG(wd,k) follows wd,k ∼ 0.2N (0, 0.5) + 0.8U(−5, 5). Remark 6. When the kernel bandwidth β is very small, the distribution (21) is very similar to a uniform distribution. However, a very small kernel bandwidth may bring difficulty in the convergence when solving (20) (see Theorems 8 and 9 in the following section). Remark 7. The GL has two advantages compared with the traditional correntropy-based loss function. Firstly, it has a shape parameter to tune so that it can accommodate many types of distributions. Secondly, it employs different kernel bandwidths at distinct channels. Moreover, the GL is associated with the noise distribution through (19), which provides the general guidance for kernel parameter selection, i.e., a smaller kernel bandwidth corresponds to a heavier tail distribution. A more detailed kernel parameter tuning strategy is available in Section III-C. III. ALGORITHM DERIVATION In this section, we derive the GMKMCKF, analyze its convergence and complexity. Then, we apply this algorithm to disturbance estimation. A. Algorithm Derivation The system dynamics in (1) can be rewritten as  x − k yk  =  I C  xk + νk (22) where x − k is the a priori estimate of state which can be obtained by (2a). The noise νk has νk =  x − k − xk vk  with E(νkν ′ k) =  P − k 0 0 Rk  =  BpB ′ p 0 0 BrB ′ r  = BkB ′ k where P − k is the a priori error covariance, and Bp and Br can be obtained by Cholesky decomposition. Left multiplying B −1 k in both sides of (22), we obtain Tk = Wkxk + ζk (23) with Tk = B −1 k  x − k yk  , Wk = B −1 k  I C  (24) and ζk = B −1 k νk. Using the GL in (20) as the loss function, we have arg min xk JGL,KF = nX +m i=1 β α i (1 − Gα,βi (ei,k)) where ei,k = ti,k − wi,kxk is the error at time step k, ti,k is the i-th element of Tk, wi,k is the i-th row of Wk, and α and βi are kernel parameters. It follows that arg min xk JGL,KF = arg max xk JGC,KF (25) with JGC,KF = nX +m i=1 β α i Gα,βi (ei,k). Equation (25) can be solved by ∂JGC,KF ∂xk = 0 and it follows that nX +m i=1 w ′ i,k(ti,k − wi,kxk)α|ei,k| α−2 exp(−β −α i |ei,k| α ) = 0. We denote |ei,k| α−2 exp(−β −α i |ei,k| α) as gC (ei,k). Then, we have nX +m i=1 w ′ i,kgC (ei,k)ti,k = nX +m i=1 w ′ i,kgC (ei,k)wi,kxk. It is easy to obtain that xk =  nX +m i=1 w ′ i,kgC (ei,k)wi,k!−1 nX +m i=1 w ′ i,kgC (ei,k)ti,k! . (26) One can see that the above question is a fixed-point equation since both sides of (26) contain xk (note that ei,k = ti,k − wi,kxk is a function of xk). It can be expressed as xk = (W′ kMkWk) −1 (W′ kMkTk) (27) with Mk =  Mp 0 0 Mr  where Mp = diag(gC (e1,k), . . . , gC (en,k)) and Mr = diag(gC (en+1,k), . . . , gC (en+m,k)). Substituting the expres- 8 Algorithm 1 GMKMCKF 1: Step 1: Initialization 2: Choose α, β1, β2, . . . , βn+m, maximum iteration number miter, and a threshold ε. 3: Step 2: State Prediction 4: xˆ − k = Axˆ + k−1 5: P − k = AP + k−1A′ + Qk 6: Obtain Bp with P − k = BpB′ p 7: Obtain Br with Rk = BrB′ r 8: Step 3: State Update 9: xˆ + k,0 = ˆx − k 10: while ∥xˆ + k,t−xˆ + k,t−1∥ ∥xˆ + k,t∥ > ε or t ≤ miter do 11: xˆ + k,t = ˆx − k + K˜ k,t(yk − Hxˆ − k ) ▷ t starts from 1 12: K˜ k,t = P˜− k H′ (HP˜− k H′ + R˜ k) −1 13: P˜− k = BpM˜ −1 p B′ p 14: R˜ k = BrM˜ −1 r B′ r 15: Mp = diag(gC (e1,k), . . . , gC (en,k)) 16: Mr = diag(gC (en+1,k), . . . , gC (en+m,k)) 17: ei,k = ti,k − wi,kx + k,t−1 18: t = t + 1 19: end while 20: P + k = (I − K˜ kH)P − k (I − K˜ kH) ′ + K˜ kRkK˜ ′ k sion of Wk from (24) into (27), we have (W′ kMkWk) −1 =[(B −1 p ) ′MpB −1 p + C ′ (B −1 r ) ′MrB −1 r C] −1 . Using the matrix inversion lemma, we arrive at (W′ kMkWk) −1 = BpM−1 p B ′ p − BpM−1 p B ′ pC ′ (BrM−1 r B ′ r + CBpM−1 p B ′ pC ′ ) −1CBpM−1 p B ′ p . (28) Further, we have W′ kMkTk = (B −1 p ) ′MpB −1 p x − k + C ′ (B −1 r ) ′MrB −1 r yk. (29) Substituting the (28) and (29) into (27), we have xk = x − k + K˜ (yk − Cx− k ) (30) with K˜ = P˜− k C ′ (CP˜− k C ′ + R˜ k) −1 P˜− k = BpM−1 p B ′ p , R˜ k = BrM−1 r B ′ r . (31) The a posteriori error covariance is given as P + k = (I − K˜ kC)P − k (I − K˜ kC) ′ + K˜ kRkK˜ ′ k . (32) The detailed algorithm of the GMKMCKF is summarized in Algorithm 1. Theorem 7. The GMKMCKF is identical to the KF when α = 2 and βi → ∞. It is identical to the traditional MCKF [24] when α = 2 and β1 = β2 = · · · = βn+m = √ 2σ. Moreover, it becomes the MKMCKF [43] when α = 2 and βi = √ 2σi . The proof of this theorem is shown in Appendix VI-F B. Convergence Issue The religious convergence of the GMKMCKF remains open. In this section, we provide a sufficient condition under which the fixed-point iteration (26) surely converges to a unique solution when setting α = 2 . We drop the subscript k and use l = n + m for ease of notation. Then, (26) can be rewritten as x = f(x) = R −1 wwPwt =  X l i=1 w ′ i gC (ei)wi !−1 X l i=1 w ′ i gC (ei)ti ! α=2 =  X l i=1 w ′ iGβi (ei)wi !−1 X l i=1 w ′ iGβi (ei)ti ! (33) where gC (ei) = Gβi (ei) = exp−e 2 i /β2 i when setting α = 2, ei = ti − wix, wi ∈ R 1×n, and βi is the kernel bandwidth for i-th channel. We assume that Rww is invertible with λmin[Rww] > 0 for any value of βi where λmin[·] denotes the minimum eigenvalue of a matrix for tractability. Then, we present the following lemma. Lemma 1. Based on contraction mapping theorem (also known as Banach fixed-point theorem) [60], the convergence of the fixed-point algorithm (33) is guaranteed if ∃ γ > 0 and 0 < η < 1 such that the initial vector ∥x0∥p < γ, and ∀x ∈ {x ∈ R n : ∥x∥p ≤ γ}, it holds that  ∥f(x)∥p ≤ γ ∥∇xf(x)∥p ≤ η (34) where ∥ · ∥ denotes an ℓp norm of a vector or an induced norm of a matrix defined by ∥A∥p = max ∥x∥p ∥Ax∥p ∥x∥p with p ≥ 1, A ∈ R n×n, x ∈ R n×1 , and ∇xf(x) is the Jacobian matrix of f(x) given by ∇xf(x) = ∂ ∂x1 f(x), ∂ ∂x2 f(x), · · · , ∂ ∂xn f(x)  with x = [x1, x2, · · · , xn] ′ . Denote bandwidth vector as β¯ = [β1, β2, . . . , βl ] ′ ∈ R l and the unified bandwidth as β1 = β2 = · · · = βl = β ∈ R. Then, we have the following two theorems. Theorem 8. If γ > ξ, where ξ = √ n Pl i=1 ∥w ′ i∥1|ti| λmin[ Pl i=1 w′ iwi] , and βi ≥ β ∗ for i = 1, 2, · · · , l, where β ∗ is the solution of equation ϕ(β) = γ with ϕ(β) = √ n Pl i=1 ∥w ′ i ∥1|ti | λminhPl i=1 w′ iGβ  γ∥w′ i ∥1 + ti  wi i , (35) then ∥f(x)∥1 ≤ γ for all x ∈ {x ∈ R n : ∥x∥1 ≤ γ}. Proof. The proof of this theorem is shown in VI-G. ■ Theorem 9. If γ > ξ = √ n Pl i=1 ∥w ′ i∥1|ti| λminhPl i=1 w′ iwi i , and ∀i, βi ≥ max{β ∗ , β+}, where β ∗ is the solution of ϕ(β) = γ [see ϕ(β) in (35)], and β + is the solution of ψ(β) = η (0 < η < 1) with ψ(β) = 2 √ n Pl i=1(|ti | + γ∥w ′ i ∥1)∥w ′ i ∥1  γ∥w ′ iwi∥1 + ∥w ′ i ti∥1  β 2λminhPl i=1 w′ iGβ  γ∥w′ i ∥1 + |ti |  wi i 9 then it holds that ∥f(x)∥ ≤ γ, and ∥∇xf(x)∥ ≤ η for all x ∈ {x ∈ R n : ∥x∥1 ≤ γ}. Proof. The proof of this theorem is in VI-H. ■ Remark 8. Theorems 8 and 9 are extensions of Theorem 1 and Theorem 2 in [61], which give a sufficient condition for the fixed-point iteration of the GMKMCKF with α = 2. By Theorem 9 and the contraction mapping theorem [60], given the initial condition ∥x0∥1 < γ, the fixed-point algorithm (33) will surely converge to a unique solution provided that α = 2 and βi is larger than a certain value and the value of η guarantees the convergence speed. Theorem 9 also indicates that the algorithm may diverge if the kernel bandwidth βi is too small, although conceptually a small kernel bandwidth may be more effective in rejecting outliers or disturbance since it corresponds to a much heavier PDF (see Fig. 4 for details). The rigorous convergence discussion of α ̸= 2 is ignored in this paper. However, in the simulation (as shown in Fig. 7 of the following section), we observe that the convergence of (33) holds with a large range of α. C. Algorithm Complexity and Kernel Parameters Selection The main computational complexity of the GMKMCKF is summarized in Table I. Note that Mp and Mr are diagonal matrices and their inverse matrices are easy to compute. Assume that the average iteration number for the while loop in Algorithm 1 is t¯. Then, the computational complexity of the GMKMCKF is Sour = [8n 3 + 4nm2 + 2mn2 − n 2 − n + O(n 3 ) + O(m3 )] + t¯[2m3 + 2n 3 + 4n 2m + 6m2n + 4n 2 + 2m2 + 2mn + 6n + 6m + O(m3 )]. (36) Similarly, we can obtain the complexity of the KF in equations (2a)-(2e), which is Skf = 6n 3 + 6n 2m + 4m2n + mn − n + O(m3 ). (37) One can see that the complexity of the GMKMCKF is moderately heavier than that of the KF. In general, the fixed-point algorithm can converge very quickly [24] which indicates that the computation complexity of the GMKMCKF is mild. In Algorithm 1, we have to tune a total of n+m bandwidths and a shape parameter α. In general, these parameters can be tuned based on the noise PDF as indicated by (19). In the application of disturbance estimation, we can select βi → ∞ for channels without disturbance, and use βj = cj for channels contaminated by disturbance. The shape parameter α can be tuned based on the shape of the nominal noises (i.e., without considering the disturbance). We can select 1 ≤ α ≤ 2 if the nominal noises are heavy-tailed, use α = 2 if they are Gaussian, and employ α > 2 if they are lighttailed. An alternative way to tune the kernel parameters is the optimization algorithm, e.g., Bayesian optimization in [44]. IV. SIMULATIONS In this section, we employ the GMKMCKF as a disturbance observer for a robotic manipulator tracking problem. TABLE I THE COMPUTATION COMPLEXITY OF ALGORITHM 1. Lines or equations Absolute value, addition/subtraction, and multiplication exponentiation, exponent, division, and Cholesky decomposition Line 4 2n 2 − n 0 Line 5 4n 3 − n 2 0 Line 6 0 O(n 3 ) Line 7 0 O(m3 ) Line 11 4nm 0 Line 12 4n 2m + 4m2n − 3nm O(m3 ) Line 13 2n 3 n Line 14 2m3 m Line 15 3n 3n Line 16 3m 3m Line 17 2n 0 Line 20 4n 3 + 4n 2m −2n 2 + 2nm2 0 (24) 2m2n + 2n 2 + 2m2 −mn − m − n 0 Moreover, we compare it with the ESO [8], KF-DOB [11], MCKF [24], and PF [46]. A. System Modeling We consider a one-degree of freedom robotic manipulator tracking problem. The target of the robot is to track a predefined angle θd with or without disturbance d. The system dynamics of the robotic manipulator can be written as Im ¨θ + bm ˙θ + kmθ + mglsin(θ) = τ + d (38) where Im is the inertia, m is the mass, l is the length of the link, bm is the damping coefficient, km is the stiffness coefficient, θ is the angle, g is the gravity constant, τ is the motor output, and d is the disturbance caused by unknown friction or the environment. To eliminate the nonlinear term in (38), we use the feedback linearization technique [62] by applying the control input ug = mglsin(θ). In this case, the new model becomes Im ¨θ + bm ˙θ + kmθ = ¯τ + d. (39) where τ¯ = τ − ug. Then, equation (39) can be rewritten as a discrete state-space form by Euler discretization xk+1 = Axk + F uk + wk yk = Cxk + vk (40) with A =   1 0 0 T Im 1 − bmT Im − kmT Im 0 T 1   F =   0 T Im 0   , C =  0, 0, 1  10 where uk = ¯τk = τk − ug,k, xk = [dk, ˙θk, θk] ′ including the disturbance, the angular velocity, and the angle, T is the sampling time, wk = [wd,k, wθ,k˙ , wθ,k] ′ is the process noise, and vk is the measurement noise. In simulation, the desired angle follows θd,k = 15 sin(0.4πkT). As for the controller, we use a feedforward term uf f,k to compensate for the system dynamics, a feedback controller uf b,k to stabilize the plant, and a disturbance compensator ud,k to counteract the disturbance. The feedback controller is the PD controller [63]. The overall controller has uk = uf f,k + ud,k + uf b,k with    uf f,k = Im ¨θd,k + b ˙θd,k + kθd,k ud,k = − ˆdk uf b,k = kp(θd,k − ˆθk) + kd( ˙θd,k − ˆ˙θk) where ¨θd,k is the desired angular acceleration, ˙θd,k is the desired angular velocity, θd,k is the desired angle, ˆ˙θk, ˆθk, and ˆdk are the estimated angular velocity, angle, and disturbance, kp and kd are controller gains. The overall motor output is τk = uk + ug,k = uk + mglsin ˆθk. Without considering the external disturbance, it actually is a proportional-–derivative (PD) controller with a feedforward term and its stability is proved in [63]. In simulation, the disturbance is assumed to be step-like and follows dk =  50 + wd,k, 400 ≤ k ≤ 600 wd,k, otherwise . In simulation, the manipulator inertia is Im = 0.1 Nm·s 2/ deg, the damping coefficient is bm = 1 Nm·s/ deg, the stiffness coefficient is km = 0.1 Nm/ deg, and the sampling time is T = 0.01 s. We compare the performance of controller (41) using the GMKMCKF, KF-DOB, MCKF, ESO, and PF as an observer in two situations: 1) the nominal noises are Laplacian; 2) the nominal noises are Gaussian. In those two cases, we use the same measurement covariance, process covariance, and initial error covariance for the KF-DOB, MCKF, and GMKMCKF. The particle number for the PF is N = 1000 while the resampling method is the systematic resample. To investigate the error performance of different observers, we conduct 100 independent Monte Carlo runs for each observer. B. Laplace Distribution with Unknown Disturbance For system dynamics in (40) with nominal noise as Laplace distribution, we assume that w1,k ∼ L(0, 0.1 √ 2 2 ), w2,k ∼ L(0, 0.01√ 2 2 ) w3,k ∼ L(0, 0.01√ 2 2 ), vk ∼ L(0, 0.01√ 2 2 ). (41) It is worth mentioning that the disturbance process noise w1,k in (41) is the nominal noise rather than the practical noise since the modeling of the disturbance in (40) is not accurate. To this end, we select β1 = 1 to suppress the heavy-tailed noises for the disturbance channel. As for other channels, we use β2 = β3 = β4 = 108 . We employ the shape parameter α = 1.6 for the GMKMCKF1 and α = 2 for the GMKMCKF2. The maximum iteration number in each sample interval is set to be miter = 5. The disturbance error using different observers in one Monte Carlo run is shown in Fig. 5. The corresponding tracking angle error is shown in Fig. 6. The root mean squared errors (RMSE) of the x1 (disturbance), x2 (angular velocity), x3 (angle), θd − θa (tracking error), and the average time consumption of different algorithms are summarized in Table II. These algorithms are executed on MATLAB 2019b on a laptop (Intel i7-8750H, 2.20GHz). One can see that the GMKMCKF1 outperforms the others and has a moderate complexity compared with the other algorithms, which reveals that α < 2 is suitable for Laplace nominal noises. 3 3.5 4 4.5 5 5.5 6 6.5 7 -60 -40 -20 0 20 40 60 Fig. 5. Disturbance estimation error of different observers. The step-like disturbance is added at t = 4 seconds and disappears at t = 6 seconds. 3 3.5 4 4.5 5 5.5 6 6.5 7 -2 -1 0 1 2 Fig. 6. Tracking error using different observers. TABLE II PERFORMANCE OF DIFFERENT OBSERVERS WITH LAPLACE NOISES. Observer RMSE of x1 (Nm) RMSE of x2 (deg/s) RMSE of x3 (deg) RMSE of (θd − θa) (deg) time cost (s) KF-DOB 8.0460 4.1637 0.0241 0.4951 0.0934 ESO 6.8374 3.3209 0.0198 0.3606 0.0927 MCKF 6.5352 3.4596 0.0249 0.3648 0.1239 GMKMCKF1 4.9361 0.7900 0.0083 0.1085 0.1237 GMKMCKF2 5.0331 0.9480 0.0088 0.1086 0.1283 PF 5.3955 1.5472 0.0100 0.1473 3.4388 11 C. Gaussian Distribution with Unknown Disturbance We consider the nominal noise as Gaussian distribution for (40) with w1,k ∼ N (0, 0.01), w2,k ∼ N (0, 0.0001) w3,k ∼ N (0, 0.0001), vk ∼ N (0, 0.0001). (42) Similarly, we apply β1 = 1 for the disturbance channel, and β2 = β3 = β4 = 108 for other channels in the GMKMCKF. Moreover, We employ the shape parameter α = 1.6 for the GMKMCKF1 and α = 2 for the GMKMCKF2. The maximum iteration number is set to be miter = 3. The RMSE and the average time consumption of different observers are summarized in Table III. One can see the GMKMCKF2 outperforms the others which indicates that α = 2 is an better option for Gaussian nominal noises. TABLE III PERFORMANCE OF DIFFERENT OBSERVERS WITH GAUSSIAN NOISES. Observer RMSE of x1 (Nm) RMSE of x2 (deg/s) RMSE of x3 (deg) RMSE of (θd − θa) (deg) time cost (s) KF-DOB 8.0397 4.1571 0.0241 0.4350 0.007 ESO 6.8507 3.3302 0.0198 0.3182 0.006 MCKF 6.5286 3.4494 0.0250 0.3626 0.0287 GMKMCKF1 4.9871 0.8199 0.0083 0.0837 0.0288 GMKMCKF2 4.9193 0.7494 0.0082 0.0777 0.0305 PF 5.2685 1.3566 0.0097 0.1005 3.4380 To investigate the parameter sensitiveness of the GMKMCKF, we conduct simulations using different α and β1 when the nominal noise is Gaussian. The result is shown in Fig. 7. One can see that the performance of the GMKMCKF is significantly better than the KF-DOB [i.e., RMSE of x1 is 8.0397 as shown in Table III] under a range of kernel parameters. Moreover, we observe that a smaller bandwidth β1 is more effective in terms of disturbance mitigation. However, a very small β1 may induce the divergence of the fixed-point algorithm (see Theorem 9). We also find that the estimation result is less sensitive to the shape parameter α compared with β1, especially when the bandwidth β1 is relatively small. The numerical results indicate that the estimation accuracy is preferable with α ≈ 2 when the nominal noise is Gaussian. Fig. 7. RMSE of the disturbance (i.e., x1) with different α and β1. V. CONCLUSION In this paper, we derive a novel algorithm called the generalized multi-kernel maximum correntropy Kalman filter (GMKMCKF). Our algorithm is derived based on the generalized loss (GL) rather than the conventional mean square error criterion and is capable of situations with some channels contaminated by heavy-tail noise. The proposed algorithm is an extension of the MCKF and MKMCKF but is much more versatile. The convergence of the proposed algorithm can be guaranteed when the kernel bandwidth is bigger than a certain level and its complexity is moderate. Simulations on a robotic manipulator verify the effectiveness of the proposed method. One limitation of this work is that the selection of the kernel parameters is demanding. In the future, we would design adaptive kernel parameter strategies. ",
		"summary": "Disturbance observers have been attracting continuing research efforts and are widely used in many applications. Among them, the Kalman filter-based disturbance observer is an attractive one since it estimates both the state and the disturbance simultaneously, and is optimal for a linear system with Gaussian noises. Unfortunately, The noise in the disturbance channel typically exhibits a heavy-tailed distribution because the nominal disturbance dynamics usually do not align with the practical ones. To handle this issue, we propose a generalized multi-kernel maximum correntropy Kalman filter for disturbance estimation, which is less conservative by adopting different kernel bandwidths for different channels and exhibits excellent performance both with and without external disturbance. The convergence of the fixed point iteration and the complexity of the proposed algorithm are given. Simulations on a robotic manipulator reveal that the proposed algorithm is very efficient in disturbance estimation with moderate algorithm complexity. ",
		"id": "UUID8"
	},
	{
		"document": "I. INTRODUCTION As autonomous systems are more frequently used in safety-critical settings (e.g., autonomous vehicles [1], medical diagnosis [2], and defense systems [3]), there is a growing need to provide statements about the safety of these systems. This is especially important as autonomy pipelines become more complicated, possibly including learned components that are sensitive to distribution shifts [4] or perturbations from nominal conditions [5]. Generating such safety assurances is challenging because applying them to real-world settings often requires dealing with uncertain, and potentially high-dimensional, systems. Moreover, in many settings, it is impossible to predict all environmental conditions or system disturbances a priori. In such cases, it is necessary to develop methods capable of certifying safety of a given system at runtime. ∗ Indicates equal contribution. 1Aerospace Controls Laboratory, Massachusetts Institute of Technology, Cambridge, USA. e-mail: {nrober,jhow}@mit.edu. 2Aurora Flight Sciences, a Boeing Company, Cambridge, USA. e-mail: {mahesh.karan, greene.max, lee.steven, monteiro.sildomar}@aurora.aero. 3Marine Autonomy Laboratory, Massachusetts Institute of Technology, Cambridge, USA. e-mail: {tpaine,mikerb}@mit.edu. 4Woods Hole Oceanographic Institution, Woods Hole, MA 02543, USA This work was supported in part by the US Navy NIWC Atlantic Award N6523623C8011. This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. Distribution Statement A. Approved for public release: distribution unlimited. Fig. 1: Reachability analysis detects a possible collision for a system experiencing a hardware malfunction Safety assurances can generally be obtained using either testing/data collection [6], [7], or formal analysis [8]–[12]. Testing-based approaches rely on collecting large amounts of data through simulation or deployment of a given system and evaluating if/how the system may fail. Testing-based approaches are incapable of providing guarantees about safety because it is generally impossible to cover all possible failure cases. Alternatively, formal methods can provide guarantees about a given system, but their practical application can be limited due to computational constraints or assumptions about the system that may not reflect its actual behavior. In this paper, we incorporate online data collection into a formal reachability analysis framework to verify the safety of a system, thus striking a balance between data-driven and formal methods. Reachability analysis, including Hamilton-Jacobi methods [13], [14], Lagrangian methods [15], and more recent approaches developed for systems with neural networks (NNs) [16]–[24], determines a set of possible future states, i.e., reachable sets (shown in Fig. 1), of a system given a set of possible initial states. While all reachability analysis techniques handle uncertainty in the state (reflected in the initial state set), and many consider process/measurement noise [13], [24], they typically assume knowledge of the system’s behavior under bounded uncertainty. Moreover, with only a few exceptions [25]–[27], reachability analysis is often too computationally expensive for online implementation, especially when the system is high-dimensional. In this work, we perform online reachability analysis of a system subject to disturbances that are unknown a priori. Reachable sets are calculated in real time by leveraging jax verify [28], a computational graph analysis tool capable of efficiently providing output bounds of nonlinear arXiv:2310.19256v1 [cs.SY] 30 Oct 2023 functions. Disturbances acting on the system are estimated online via a moving horizon estimator (MHE) [29], and are incorporated into the reachability analysis to predict the behavior of the actual system. Our approach is validated using numerical results and deployed on a Clearpath Robotics® Heron unmanned surface vehicle (USV), demonstrating the efficacy of the developed method on a physical system with unknown disturbances. The main contributions of this paper are as follows: • Developed a data-driven reachability analysis technique for online safety verification of closed-loop systems subject to disturbances and modeling errors • A proof of the validity of our reachable set overapproximations assuming limits on the rate of change of possible disturbances • Hardware experiments demonstrating real-time reachability calculations at 10 Hz for a 6DOF USV subject to actuator failures and environmental disturbances, e.g., wind and currents II. PRELIMINARIES A. System Dynamics Consider the nonlinear system xt+1 = f(xt, ut) + wt yt = h(xt) + νt (1) where xt ∈ R nx , ut ∈ R nu , yt ∈ R ny , wt ∼ N (µt,Wt), νt ∼ N (0, R) and t ∈ N. The dynamics function f : R nx × R nu → R nx and measurement function h : R nx → R nx are assumed to be known, but there is uncertainty in the system due to wt because the distribution N (µt,Wt) is time-varying and unknown, conditioned on Wt being diagonal and |(µt+1)i − (µt)i | ≤ ∆µi |(σt+1)i − (σt)i | ≤ ∆σi , (2) where (σt)i ∈ R for i = 1, . . . , nx is the standard deviation associated with the covariance matrix Wt, i.e., (Wt)ii = (σt) 2 i , and ∆µ, ∆σ ∈ R nx represent the possible variation in µt and σt, respectively, per time step. Note that the unknown nature of µt and Wt (along with the potentially nonlinear dynamics) is a departure from [24], where the support of wt is assumed known at each time step. Additionally, while we consider diagonal Wt for brevity of our theoretical arguments here, our approach can be extended to the nondiagonal case in future work. Finally, by introducing a general state-feedback control policy ut = π(xt), the closedloop dynamics are xt+1 = fcl(xt; π) + wt. (3) Distribution Statement A. Approved for public release: distribution unlimited. B. Reachability Analysis The forward reachable set at time t + 1 of a system with closed-loop dynamics (3) is defined recursively as Rt+1(X0) = fcl(Rt(X0); π), (4) where R0(X0) = X0 is the set of possible initial states, and fcl(Rt(X0); π) is shorthand for {fcl(x; π) | x ∈ Rt(X0)}. Going forward, we will omit the X0 argument unless it is needed for clarity. The exact reachable set Rt is typically expensive to compute, so we instead compute reachable set overapproximations (RSOAs), i.e., Rt ⊇ Rt, as is specified in §III. The RSOAs {Rt, Rt+1, . . . , Rt+τr }, denoted as Rt:t+τr , can be used to verify safety of the system over a horizon τr by checking if the future states can reach an unsafe region of the state space C ⊂ R nx . If there is an i ∈ T = {t, . . . , t + τr} such that Ri T C ̸= ∅, the system may enter the unsafe region and safety is not guaranteed. Additionally, Rt:t+τr can be used to check if the system enters a goal region G. If ∃i ∈ T such that Ri T G = Ri , the system is guaranteed to reach G ⊂ R nx . Note that while RSOAs are capable of verifying safety and liveness as described, more conservative RSOAs make the verification conditions harder to satisfy, so tight RSOAs are preferred. C. Computational Graphs A computational graph (CG) G is defined as a directed acyclic graph (DAG) with nodes V = {V1, V2, . . . , VnG } and edges E where each edge is a pair of two nodes (Vi , Vj ), indicating that the output of node Vi is an input of node Vj . Each node has an associated computation function Gi(·) consisting of a basic computation such as ReLU or matmul, such that g G i = Gi(u(Vi)) where g G i is the output of Vi and u(Vi) is the set of inputs to Vi , i.e., the outputs from nodes with edges directed toward Vi . The inputs to the graph are denoted as z ∈ R ni . For brevity going forward, we express g G i in terms of the graph’s input, i.e., g G i = g G i (z), thus avoiding the explicit use of u(Vi). Without loss of generality, we assume G has a single output node Vo with dim(Vo) = R no , allowing us to express the output of the graph as g G o (z) ∈ R no . D. Computational Graph Relaxation Computational graph relaxation is used to determine relationships between sets of inputs and outputs of a CG. More specifically, given a set of possible inputs I to a given CG G, the goal is to determine a set of possible outputs O = {g G o (z0) | z0 ∈ I}. We construct I as an ℓ∞-ball, defined as B∞(z0, ϵ) ≜ {z | ∥(z − z0) ⊘ ϵ∥∞ ≤ 1}, (5) where z0 ∈ R ni is the center of the ball, ϵ ∈ R ni ≥0 is a vector whose elements are the radii for the corresponding elements of z, and ⊘ denotes element-wise division. Theorem 2.1 (Linear Relaxation of CGs [10]): Given a CG G and a hyper-rectangular set of possible inputs I, there exist two explicit functions g G L,o(z) = Ψz + α, gG U,o(z) = Φz + β such that the inequality g G L,o(z) ≤ g G o (z) ≤ g G U,o(z) holds element-wise for all z ∈ I, with Ψ, Φ ∈ R no×ni and α, β ∈ R no . Using Theorem 2.1, a hyper-rectangular over-approximation of the output set O is constructed as O ⊆ O = {o | g G L,o(z) ≤ o ≤ g G U,o(z), ∃z ∈ I}. E. Moving-Horizon Estimation MHEs are a class of optimization-based estimators [29]. Assuming a system of the form (1) with a prior on the initial state and its covariance, an MHE uses measurement data from a moving window to estimate the current state of the system and its covariance. The MHE optimization formulation is constructed as min xˆt:t+τe ,wˆ t:t+τe J, (6) where the cost function J is J = ∥xˆt − xt∥ 2 Q−1 t|t−1 + tX +τe k=t ∥yk − h(xˆk)∥ 2 R−1 + ∥wˆ k∥ 2 W−1 k , where xt is the state estimate prior, Qt|t−1 ∈ R nx×nx is the state uncertainty prior, yt:t+τe represents data measurements from time t to t + τe, and ∥xˆt − xt∥ 2 Q−1 t|t−1 ≜ (xˆt − xt) ⊤Q−1 t|t−1 (xˆt − xt). The result of (6) are values of xˆt:t+τe and wˆ t:t+τe that optimally estimate the state and disturbance terms over the given window. Much like model predictive control [30], the typical idea behind MHE is to execute (6) and collect xˆt at each discrete time step. Each iteration of the process calculates a new estimate of xˆt is generated with (6), and the priors of the state estimate xt+1 and covariance Qt+1|t are generated for the next time step using the update law xt+1 = fcl(xˆt; π) + wˆ t Qt|t =  Q−1 t|t−1 + H⊤R−1H −1 Qt+1|t = AQt|tA⊤ + Wt, (7) where A = ∂fcl ∂x   x=xˆt and H = ∂h ∂x   x=xˆt . III. REACHABILITY FOR UNCERTAIN SYSTEMS In this section we outline our approach to generating RSOAs for a system of the form (3), subject to a priori unknown disturbances. Our approach is designed to be executed online, regularly generating RSOAs over a finite time horizon at a fixed interval. The proposed data-driven reachability approach is summarized as follows: Distribution Statement A. Approved for public release: distribution unlimited. 1) First, before the deployment of the system, we construct fcl as a CG. 2) At each time step during runtime, we execute an MHE iteration to obtain xˆt and estimates of the most recent mean and covariance values of wt. 3) Finally, we feed the mean and covariance estimates from the MHE into a CG relaxation to conduct reachability analysis, thus predicting the behavior of the actual system. We first address the MHE component. Notice that the MHE formulation described in §II-E uses data in the forward direction, but in practice we only have access to data up until time t. Thus, we configure the MHE to use data over the window from time t−τe to time t, maintaining a prior on xt−τe and a queue of the most recent output measurements yt−τe:t. With these values, we obtain xˆt−τe:t and wˆ t−τe:t from (6). As described in §II-E, we collect the state estimate xˆt and generate the prior xt+1, but we also use the rest of the information obtained from wˆ t−τe:t to calculate estimates of µt and Wt. Specifically, we make the approximations µˆt = mean(wˆ t−τe:t) and Wˆ t = cov(wˆ t−τe:t). To conduct reachability analysis, we generate RSOAs using the CG analysis tool jax verify [28]. As shown in Fig. 2, by specifying the control policy π and the open-loop dynamics (1) as functions in the jax verify framework, we can generate a CG representation of (3) and use Theorem 2.1 to obtain bounds on xˆt+1 from a set of possible xˆt. Moreover, to account for the range of possible current and future disturbances, we must also include terms for the disturbance and its rate of change as inputs to the CG. Thus, we introduce the CG Gcl with input zt = [x˜ ⊤ t , µ˜ ⊤ t ,˚µ ⊤ t , ˚σ ⊤ t ] ⊤ and output denoted g Gcl o (zt) = [x˜ ⊤ t+1, µ˜ ⊤ t+1,˚µ ⊤ t+1, ˚σ ⊤ t+1] ⊤ where x˜ ⊤ t , µ˜ ⊤ t ,˚µt, ˚σt ∈ R nx . Note that while the inputs x˜t and µ˜t correspond to xˆt and wˆ t, the inputs ˚µt and ˚σt are internal to the reachability analysis and are used to account for the time variation of µt and Wt as described by (2). Gcl is then specified by the equations x˜t+1 = fcl(x˜t; π) + µ˜t (8) µ˜t+1 = µ˜t + ˚µt + γ˚σt (9) ˚µt+1 = ˚µt (10) ˚σt+1 = ˚σt, (11) where γ > 0 is a parameter used to concretize an uncertainty bound for a selected confidence interval, e.g., γ = 3 means we assume all samples fall within three standard deviations of the mean. Concretization is done because jax verify (and many other analysis tools, such as [10]) assumes concrete bounds on the possible input states. Having constructed Gcl and established how we use the MHE, we can now explicitly specify our approach, which is summarized pictorially in Fig. 2 and via pseudo-code in Algorithm 1. At each time step, yt−τe:t is collected from the boat and passed to the MHE, which determines optimal values for xˆt−τe:t and wˆ t−τe:t. Using the output from the MHE, we determine estimates for the state xˆt and its covariance Qt|t−1, as well as disturbance parameter estimates µˆt and Fig. 2: Block diagram depicting our approach. Data is collected from the system and fed into the MHE, which estimates the state and disturbance terms. The outputs of the MHE are used with a CG representation of the closed-loop dynamics to conduct reachability analysis and certify safety. Wˆ t. On Lines 5 to 7, we then obtain concrete uncertainty bounds ϵ by truncating the normal distribution according to concretization parameter γ as previously described. Next we construct the set of possible inputs for Gcl. On Lines 7 to 9, the possible states B(xˆt, ϵx), noise values B(µˆt, ϵµ), and reachability variables B(06, [∆µ ⊤, ∆σ ⊤] ⊤), are concatenated to get the initial set R ′ t necessary for reachability analysis. Next, on Lines 10 and 11 we loop over the horizon τr, calculating RSOAs for each time step. Notice that the RSOA is the set of possible states and disturbance terms, R ′ t ⊂ R 4nx . Thus, on Line 12, we project R ′ t onto R nx , thereby enabling us to check the safety condition described in §II-B on Line 13, which is the desired result. Theorem 3.1 formally states the result of our approach. Theorem 3.1 (Safety Verification for an Uncertain System): Consider a system (3) subject to a disturbance wt ∼ N (µt,Wt) truncated at γ standard deviations, where µt and Wt satisfy the assumptions specified by (2) and where µˆt and Wˆ t are accurate estimates for their respective parameters at the current time step. The iterative application of Theorem 2.1 with Gcl defined by Eqs. (8) to (11) and where I = B∞(zt, ϵ), ϵ = [ϵ ⊤ x , ϵ ⊤ µ , ∆µ ⊤, γ∆σ ⊤] ⊤ and zt = [xˆ ⊤ t , µˆ ⊤ t , 0 ⊤ 3 , 0 ⊤ 3 ] ⊤ provides bounds on all possible xˆt:t+τr , i.e., Rt:t+τr . Proof: First, consider the RSOA calculation for time t + 1. Since the value of wt is bound by a distribution with mean µˆt and truncated at γ standard deviations with covariance Wˆ t, (8) describes all possible values for xˆt+1 when x˜t ∈ B(xˆt, ϵx) and µ˜t ∈ B(µˆt, ϵµ). Moreover, since µˆt+1 ∈ B(µˆt, ∆µ + γ∆σ) (via (2)), (9) captures all values of µˆt+1 when ˚µt ∈ B(03, ∆µ) and ˚σt ∈ B(03, ∆σ). Distribution Statement A. Approved for public release: distribution unlimited. Algorithm 1 Online Safety Certification Input: computational graph Gcl, reachability horizon τr, vehicle data yt−τe:t, concretization parameter γ, unsafe region C Output: safety certificate c over horizon τr 1: c ← true 2: xˆt−τe:t, wˆ t−τe:t ← MHE(yt−τe:t) 3: µt ← mean(xˆt−τe:t) 4: Wt ← cov(wˆ t−τe:t) 5: ϵx ← concretize(MHE.Qt, γ) 6: ϵµ ← concretize(Wt, γ) 7: ϵ ← [ϵ ⊤ x , ϵ ⊤ µ , ∆µ ⊤, γ∆σ ⊤] ⊤ 8: zt ← [xˆ ⊤ t , µˆ ⊤ t , 0 ⊤ 3 , 0 ⊤ 3 ] ⊤ 9: R ′ t ← B(zt, ϵ) 10: for i in {t + 1, . . . t + τr} do 11: R ′ i ← jax verify(Gcl, R ′ i−1 ) 12: Ri ← projection(R ′ i ) 13: if Ri T C ̸= ∅ then 14: c ← false 15: end if 16: end for 17: return c Thus, the application of Theorem 2.1 with Gcl and I provides hyper-rectangular bounds on xˆt+1 and µˆt+1, i.e., R ′ t+1 = {o | g Gcl L,o (zt) ≤ o ≤ g Gcl U,o (zt), ∃zt ∈ I}. Let Rt+1 = {x | x = projRnx o, ∀o ∈ R ′ t+1} (note that because Rt+1 is hyper-rectangular, to project onto R nx , we simply select the the first nx components of Rt+1). Since Rt+1 contains the projection of elements of R ′ t+1 onto the statespace R nx , and R ′ t+1 provides bounds on xˆt+1 and µˆt+1, Rt+1 is a RSOA for all possible xˆt+1. To extend this argument to time steps beyond t + 1, recognize that R ′ t+1 gives an over-approximation of both xˆt+1 and µˆt+1, allowing us to apply the same argument by considering a new initial state set I1 = R ′ t+1. Thus, by taking over-approximations of over-approximations, we can construct RSOAs over an arbitrary horizon τr. IV. RESULTS In this section we demonstrate the performance of our approach with results from both numerical and hardware experiments. For each set of experiments, we consider a differential-thrust USV whose body-frame velocity vector ηv = [u, v, r] ⊤ ∈ R 3 characterizes the vehicle’s surge u, sway v, and yaw rate r. We use the widely accepted model of marine vehicle motion [31]: Mη˙ v + C(M, ηv)ηv + D(ηv)ηv = τ (12) where M ∈ R 3×3 is the inertia matrix which includes the rigid-body and added mass terms, C(M, ηv) ∈ R 3×3 is the Coriolis matrix, D(ηv) ∈ R 3×3 is the drag matrix, and τ ∈ R 3 are the forces and moments acting on the vehicle due to the control inputs. The position and orientation of the vehicle, ηx = [x, y, ψ] ⊤, where x ∈ R is the x-position, y ∈ R is the y-position, and ψ ∈ S 1 is the heading angle, are subject to the dynamics η˙ x =    u cos(ψ)−v sin(ψ) u sin(ψ)+v cos(ψ) r    . (13) For the purpose of control design and application of our approach, we make the following discrete time approximation of the update law for the full USV state x = [η ⊤ x , η ⊤ v ] ⊤: xt+1 = ' fx,d(xt) Aclηv,t+Bclηdes,t# , (14) where fx,d : R 6 → R 3 is a discrete approximation of (13), Acl ∈ R 3×3 and Bcl ∈ R 3×2 characterize the closed-loop dynamics obtained via controller design (discussed further in §IV-A and §IV-B) and system ID techniques [32], and ηdes,t = [udes,t, rdes,t] ⊤ ∈ R 2 contains the desired surge udes,t and yaw rate rdes,t that the closed-loop system should track. Note that ηdes,t is the output of a waypoint-following algorithm, which did not fit into the CG framework required by jax verify. Therefore, to predict future values of ηdes,t, we simulate the system forward from the nominal state and collect the desired surges and yaw rates, which are passed to the reachability calculation. A. Numerical Experiments First, we use a simulated environment to evaluate the efficacy of our approach while maintaining control over the disturbances that influence the system. We employ a model reference adaptive controller (MRAC) [33], which provides a control signal to (12) such that the closed-loop system behaves according to a user-selected Acl and Bcl by design. The details on the implementation of the closed-loop system and MRAC controller can found in the linked repository. 1) Symmetric Thruster Failure: Fig. 3 shows an experiment where the USV is commanded to follow the trackline between two waypoints, shown as a dotted black line. At t = 0, the USV is behaving as expected, travelling at 0.5 m/s to the right along the trackline. Simulated trajectories (orange) are propagated forward from the initial state set (black), and are contained within the RSOAs (blue) over a time horizon of 2.5 s. The RSOAs do not intersect with the unsafe regions (red) 0.5 m away from the trackline, so the USV is guaranteed to be safe over the time horizon. At t = 4, a disturbance is introduced, causing both the USV’s left and right thrusters to operate at 50% effectiveness. Fig. 4 shows that until t = 4, the estimated disturbance in the u dynamics was near 0, but the MHE registers the loss of control effectiveness as a disturbance, which is captured by the estimates (µˆt)1 (line) and (σˆt)1 (shaded region). Notice that as the system experiences a new disturbance, the RSOAs inflate in response to the increased uncertainty captured by σˆt. This phenomenon is reflected in the RSOAs calculated Distribution Statement A. Approved for public release: distribution unlimited. Thruster Health t = 0 left right 100% 100% t = 5 50% 50% t = 12 50% 50% Fig. 3: Snapshots of reachable sets for a USV model over the course of a waypoint-tracking mission. Possible trajectories (orange) are sampled from the initial state set (black) and bounded by the RSOAs (blue). The RSOAs never intersect with the unsafe region (red), so the USV is guaranteed to be safe, but a 50% decrease in thruster effectiveness causes the USV to slow down. The RSOAs accurately capture the behavior of the system despite the thruster malfunction. Fig. 4: Estimates (µˆt)1 (line) and (σˆt)1 (shaded) for the experiment shown in Fig. 3. The MHE estimates the bias and covariance to characterize the disturbance (thruster malfunction) affecting the USV. at t = 5, which are stretched horizontally. At t = 12, the disturbance estimate has stabilized and the reachable sets accurately capture the sampled trajectories under the new operating conditions. 2) Asymmetric Thruster Failure: Fig. 5 shows a scenario similar to the one discussed in §IV-A.1: the USV is commanded to follow the dotted trackline at 0.5 m/s. In this experiment however, only the USV’s right thruster experiences a malfunction. The asymmetry in the system’s control effectiveness is too much for the controller to handle, causing the vehicle to steer to the right at t = 5. The RSOAs accurately reflect the bias in the yaw rate, correctly predicting that the USV is going to veer off course. Because the RSOAs calculated at t = 5 intersect with the unsafe region, safety cannot be guaranteed over the 2.5 s time horizon. B. Hardware Experiments To test the ability of our approach to handle real-world disturbances, we deployed Algorithm 1 using a Clearpath t = 0 left right 100% 100% t = 5 100% 50% Fig. 5: Snapshots of reachable sets over the course of a waypoint-tracking mission where the USV’s right thruster malfunctions. The disturbance from the thruster malfunction is incorporated into the RSOAs, allowing the analysis to predict a possible collision with the unsafe region. Fig. 6: Clearpath Robotics Heron® USV at the MIT Sailing Pavilion. Robotics® Heron USV shown in Fig. 6. The Heron USV is a 1.35m × 0.98m × 0.32m catamaran-style vehicle with parallel differential thrusters [34]. The Heron USV’s sensor package includes a compass magnetometer, IMU, GPS module and antenna, WiFi antenna, and RF antenna. The Heron USV has two onboard computers for managing high-frequency operations (e.g., motor control) and lower frequency operations (e.g., waypoint following). These computers communicate with a command station equipped with an Intel® Core™ i7 CPU running at 2.7 GHz. While the command station is used to run the online computation of our approach in the following experiment, future work will move the calculation onboard the vessel. For the hardware experiment, we used system identification to estimate a linear set of matrices that represent (12) and designed a robust servomechanism LQR (RSLQR) as specified in [35]. Fig. 7 shows the results of the hardware experiment. SimiDistribution Statement A. Approved for public release: distribution unlimited. t = 0 left right 100% 100% t = 40 30% 30% 10 m Fig. 7: Hardware experiment with symmetric thruster malfunctions. RSOAs (cyan) are smaller after the disturbance is introduced, correctly predicting the lower speed of the USV. Because the RSOAs do not intersect with the unsafe region (purple), safety is guaranteed. larly to the experiment described in §IV-A.1, the Heron USV is commanded to follow a trackline between two waypoints. Initially, the USV behaves as expected: about 40% of its maximum power is applied to each thruster to travel at 1 m/s. As the USV performs the mission, we induce a malfunction in both thrusters, reducing their effectiveness to 30% of the commanded thruster output. The controller attempts to overcome the reduced thruster effectiveness by increasing the commanded power to 100% ; however, given the degraded thruster health, the Heron USV can only maintain a forward velocity of 0.7 m/s. The effect of this disturbance appears in the RSOAs shortly after it is applied: the RSOAs are compressed toward the vehicle, thus reflecting the slower forward velocity. At each instance in Fig. 7, we calculate 20 RSOAs (cyan) over a time horizon of τr = 8 s with an average total computation time of 1.03 ± 0.67 ms. V. CONCLUSION This paper introduces an online safety certification method that leverages moving horizon estimation (MHE) and forward reachability analysis to predict the future states of a system despite unknown disturbances. Reachability analysis is often computationally expensive and assumes knowledge of the system’s true dynamics; these trade-offs prohibit such methods from being used on real robotics systems. This method employs jax verify to construct linear bounds on a computational graph representation of the system’s estimated dynamics, which includes the nominal system dynamics and a bias estimate from MHE. Future work includes extending this method with online system learning techniques to improve the bias estimation throughout the reachability horizon, allowing us to handle more complex, time-varying disturbances in a less conservative way. Additionally, exploring more rigorous methods of including multi-loop control architectures, e.g., waypoint following, would enable more complex reachable set behaviors and improve this method’s safety verification accuracy.",
		"summary": "— Deploying autonomous systems in safety critical settings necessitates methods to verify their safety properties. This is challenging because real-world systems may be subject to disturbances that affect their performance, but are unknown a priori. This work develops a safety-verification strategy wherein data is collected online and incorporated into a reachability analysis approach to check in real-time that the system avoids dangerous regions of the state space. Specifically, we employ an optimization-based moving horizon estimator (MHE) to characterize the disturbance affecting the system, which is incorporated into an online reachability calculation. Reachable sets are calculated using a computational graph analysis tool to predict the possible future states of the system and verify that they satisfy safety constraints. We include theoretical arguments proving our approach generates reachable sets that bound the future states of the system, as well as numerical results demonstrating how it can be used for safety verification. Finally, we present results from hardware experiments demonstrating our approach’s ability to perform online reachability calculations for an unmanned surface vehicle subject to currents and actuator failures.",
		"id": "UUID9"
	},
	{
		"document": "I. INTRODUCTION I N the design of control systems for unknown industrial plants, there are two common approaches for tuning controller parameters: trial and error, and the use of a mathematical model. Both methods are time–consuming: the former requires many preliminary experiments, whereas the latter requires complex model structure selection, identification, and evaluation [1]. Thus, data–driven control methods have bee n proposed [2], [3]. These strategies use only input–output (I/O) data and do not require explicit and/or precise mathematical models. Examples include simultaneous perturbation stochastic approximation–based model–free control [4], virtual reference feedback tuning (VRFT) [5], fictitious reference iterative tuning (FRIT) [6], and model–free adaptive control [7], among others [8]–[11]. In particular, FRIT and VRFT are direct controller tuning methods. These design controller parameters offline using only a single I/O data under a practi - cal controller structure. This significantly reduced the design effort. Additionally, in FRIT, tuning is performed according to the output results, which depend on the reference [12]. Hence, an inverse reference model is not required. Therefore, to select a proper transfer function that generates pseudo–signals for parameter tuning, a pre–filter is unnecessary, which is more practical when the reference is almost fixed. However, these methods have drawbacks in that they require a reference model in advance to generate the desired output for a closed–loop system. The control performance deteriorates or even becomes unstable when the reference model is inappropriately selected. This is because these methods force matching behaviors among a closed–loop system with an unknown plant, designed controller, and reference model [13]. To solve this problem, tuning the controller and reference model simultaneously through optimization has been proposed [13]. However, in this case, the direct controller tuning method generates controller parameters that achieve tracking to the optimized desired output and not to the reference signal. Consequently, control performance cannot be guaranteed. I n several previous studies, the reference model was decouple d from the performance [14], [15]. In these studies, model predictive control (MPC) was applied using a direct controller tuning method instead of a mathematical model. As a result, these methods achieved tracking of not the desired output bu t the reference signal. To solve this problem, we propose a new controller design [16]. In the inner loop, we introduce a pseudo–linearizatio n (PL) model for model matching based on FRIT, while the outer loop adopts a model–based controller. Moreover, we integrated MPC to directly consider the input difference an d its constraints. The contribution of this study lies in the integration of the model–based and data–driven control strategies. Thus, we achieved high control performance without a complex mathematical model. In a similar study, data–drive n control was applied to design an MPC system considering input constraints, and a Kalman filter was used to estimate internal inputs [14]. Our method estimates the internal inputs using a PL model and an optimized controller [16]. In addition, the proposed method optimizes the reference model because the control performance depends on the accuracy of matching the reference model with a closed-loop system. In a previous study, the proposed method was applied to an experimental machine. However, it has not been fully analyzed for various classes through numerical simulations or validated with several references. Therefore, this paper presents a more precise analysis of the proposed method. Numerical simulations wer e conducted using two nonlinear classes. These models have a similar structure consisting of linear and nonlinear parts and are suitable for practical applications in the verification of control systems because they can describe hysteresis characteristics and dead zones. Furthermore, the frequency domain was analyzed in the simulations. In addition, control experiments for the angle tracking of artificial muscles with nonlinear characteristics were conducted for two types of references . The contributions of this study are as follows: 1) Proposal of a PL method combining FRIT and MPC; 2 Plant G(z) Controller r(k) - e(k) C(, z) y(k) Reference model Gm(z) ym(k) + Output Control input Closed-loop T (, z) u(k) Tracking Reference error Desired output  ― ― Fig. 1. Block diagram for FRIT 2) Evaluation of the control performance for two hysteresis and dead-zone classes, i.e., the Bouc–Wen model and Hammerstein model, via simulations; 3) Design of a PL model with systematic optimization and its analysis through simulations; 4) Experimental evaluation of the model’s effectiveness for artificial muscles with sinusoidal and square references. The remainder of this paper is organized as follows. First, FRIT and its extended versions are briefly reviewed. Next, the PL method using FRIT is explained, and the PL model is introduced. Subsequently, a design method for model–based control systems using the PL model is proposed. The subsequent sections detail the design of MPC, which explicitly considers the control input constraints using the PL model. Finally, to confirm the effectiveness of the proposed method, reference tracking control was performed and analyzed through simulations for two nonlinear classes and experiments, i.e., artificial muscle actuators, in comparison with conventional methods. II. DATA–DRIVEN CONTROL In this section, we introduce the data–driven control methods, i.e., FRIT and the enhanced version Extended FRIT (E–FRIT), which are used to design control systems with unknown characteristics. First, we provide an overview of the conventional FRIT method and its principles. Next, we explain the E–FRIT method for improving the control performance by suppressing the control input signal. FRIT is a data–driven control method that determines controller parameters using only a single closed–loop I/O data. We introduce the time–shift operator z and consider the block diagram shown in Fig. 1, where G(z) denotes an unknown plant system. C(θ¯, z) denotes the discrete–time PID controller structure, which is expressed as follows: C(θ¯, z) = β T (z)θ¯ = KP + KITs 1 − z−1 + KD(1 − z −1 ) Ts , (1) where β(z) = [1, Ts/(1 − z −1 ),(1 − z −1 )/Ts] T ∈ R 3 denotes the known vector of the discrete-time transfer function, θ¯ = [KP, KI , KD] T ∈ R 3 denotes the vector of the controller parameters, and Ts denotes the sampling time. T (θ¯, z) denotes a closed–loop composed of C(θ¯, z) and G(z). The objective of FRIT involves minimizing the following model reference evaluation function: J(θ¯) =         G(z)C(θ¯, z) 1 + G(z)C(θ¯, z) r(k) − Gm(z)r(k)         2 2 , (2) where k · k2 represents the Euclidean norm, Gm(z) denotes the reference model appropriately selected by the designer, and r(k) ∈ R denotes the reference. From (2), FRIT is essentially a model matching problem, in which θ¯ is numerically optimized to match T (θ¯, z) with Gm(z). To realize the aforementioned objective, the prior single I/O data y0(θ¯ 0, k) ∈ R and u0(θ¯ 0, k) ∈ R in the closed–loop system obtained in a prior experiment with initial parameters θ¯ 0 are used to tune θ¯. However, minimizing the evaluation function in (2) is impossible, because G(z) in (2) is an unknown system. Therefore, we rewrite the evaluation function (2) to (3) as follows: JF(θ¯) =    y0(θ¯ 0, k) − y˜(θ¯, k)     2 2 . (3) Here, y˜(θ¯, k) denotes the desired output when r˜(θ¯, k) is the input: y˜(θ¯, k) = Gm(z)˜r(θ¯, k), (4) where r˜(θ¯, k) denotes the fictitious reference signal. r˜(θ¯, k) = C −1 (θ¯, z)u0(θ¯, k) + y0(θ¯, k). (5) Here, r˜(θ¯, k) can be calculated because the evaluation function (3) does not contain an unknown system G(z). Therefore, FRIT provides desirable controller parameters without requiring a mathematical model of the plant. Thus, (3) can be rewritten as (6). JF(θ¯) =          1 − Gm(z) T (θ¯, z)  y0(θ¯ 0, k)         2 2 , (6) (6) can be transformed into the frequency domain using Parseval’s theorem and is expressed as follows: JF(θ¯) ≈ 1 2π Z π −π          1 − Gm(e jω) T (θ¯, ejω)  Y0(θ¯ 0, ejω)         2 2 dω, (7) where Y0(θ¯ 0, ejω) denotes the power spectral density of y0(θ¯ 0, k). It can be observed that the evaluation function of FRIT numerically determines the value of θ¯ such that T (θ¯, z) matches Gm(z) for the frequency components contained in the initial output y0(θ¯ 0, k). Because Y0(θ¯ 0, ejω) depends on the reference r0(k) in the preliminary experiment and the initial parameters θ¯ 0, the FRIT method performs local matching of Gm(z) and T (θ¯, z) under initial conditions that depend on these two factors. In addition, we explain the evaluation function for E– FRIT that effectively suppresses input variations. The E–FRIT evaluation function is as follows: JEF(θ¯) = JF(θ¯) + λ    ∆˜u(θ¯, k)     2 2 , (8) where λ is the weight and ∆˜u(θ¯, k) denotes the fictitious input variation. ∆˜u(θ¯) , u˜(θ¯, k) − u˜(θ¯, k − 1) (9) u˜(θ¯) = C(θ¯, z)(˜r(θ¯, k) − y˜(θ¯, k)) (10 3 Fig. 2. Block diagram for model–based controller using PL with E–FRIT III. MODEL–BASED CONTROL SYSTEM USING PL In this section, we propose a method for designing model– based control systems using a PL technique based on E– FRIT. A block diagram of the method is shown in Fig. 2, where v(k) ∈ R denotes the input generated by the model– based controller and u(k) denotes the actual plant input. Furthermore, ev(k) ∈ R denotes the internal error between v(k) and y(k). Typically, the purpose of E–FRIT is to perform model matching between the augmented controlled system and a reference model that corresponds to an ideal model for a given reference signal. However, the optimal design of an appropriate reference model for unknown plants remains challenging. In Section III. A, we address this issue by explicitly focusing on the concept of E–FRIT. We propose a method for separating E–FRIT from the reference tracking problem and introduce a PL technique that allows a closed–loop system to match a linear model with adjustable parameters. The linear model used for matching was defined as the PL model. The PL model differs from the reference model in that the former does not denote the ideal characteristics as a reference. Because the PL model was optimized to match the closed–loop system, it was not designed to consider the relationship with the reference. Next, we designed a model–based controller in the outer loop using the obtained PL model, as shown in Fig. 2. However, the PL model depends solely on the I/O data and cannot explicitly handle the control input u(k) in G(z), which matches the state variables in the closed loop. To address this issue, in Section III. B, we propose a method for estimating the control input u(k) via the PL model and PID controller based on the internal error ev(k). Finally, in Section III. C, we design a MPC using the PL model as a predictor and estimate the input u(k) with consideration of the input constraints. A. PL Using E–FRIT We propose a PL technique for an unknown system using the E–FRIT method. As discussed above, E–FRIT designs a controller that matches the closed–loop system T (θ¯, z) with a reference model Gm(z) that ideally exhibits the desired system characteristics. However, in the absence of information on G(z), it is challenging to design a reference model appropriately. Moreover, forcing an unknown T (θ¯, z) to match an inappropriate Gm(z) can degrade the system’s control performance. Therefore, to improve the control performance, it is necessary to optimize the reference model. However, this approach involves arbitrary alteration of the desired output. Consequently, there is no guarantee that the control system designed according to this optimized reference model will result in improved control performance with respect to the reference. In response to this challenge, we shifted our focus from the fundamental concept of E–FRIT to model matching, thereby distinguishing this concept from reference tracking. Accordingly, the closed–loop system matches with a linear model whose parameters can be optimized, which is defined as a PL model. In the proposed method, the E–FRIT method adjusts θ¯ such that the I/O characteristics of T (θ¯, z) match the PL model and does not aim to match the state variables of T (θ¯, z) to those of the PL model. Therefore, in the control system design using the PL model, the state variables fed back from T (θ¯, z) do not have any physical interpretation other than the output; therefore, it is preferable that the PL model has only one state variable. Therefore, we adopt a structure that is the discretization of the first–order system in (11): PL(Tc, z) =  1 − e − Ts Tc  z −1 1 − e − Ts Tc z−1 , (11) where Tc denotes the time constant. Therefore, the adjustable parameter θ¯ of the conventional E–FRIT using a PID controller can be rewritten as follows, including the adjustable parameter of the PL model: θ = [KP, KI , KD, Tc] T = [θ¯T, Tc] T (12) where θ represents the modified E–FRIT parameter used in the proposed method. This allowed us to obtain a optimized PL model for the plant and tune the control parameters. From (8) and (12), it can be observed that the closed-loop system composed of the obtained PL model and controller is separate from the reference tracking problem because the PL model used only for matching can be arbitrarily tuned through optimization. B. Estimation of Internal Variables The PL model was obtained solely from single I/O data without considering the system characteristics. This allows us to easily design various model–based controllers without requiring complex modeling routines. However, the PL model strongly depends on the I/O relationship in a closed–loop system. Therefore, the state variables of the PL model lack inherent meaning, and it is impossible to directly consider the input u(k) in G(z), which corresponds to the state variables in the matched closed–loop system T (θ¯, z). Some model–based control approaches–particularly MPC–generate control inputs by considering the input constraints. However, in the proposed method, the controller–generated input v(k) functions as a reference for the inner–loop system, as shown in Fig. 2. This differs from the actual input u(k) applied to the plant. To address this problem, we estimate the input u(k) applied to the plant using the internal reference v(k) generated by the model–based controller via the PL model and tuned PID controller. The following state–space model corresponding to the discretized PL model of (11) is considered: ( x(k + 1) = aPx(k) + bPv(k) y(k) = cPx(k) , (13) 4 where x(k), v(k) , and y(k) ∈ R denote the state variable; the internal reference, which is the input to the PL model; and the output, respectively. The parameters of the state–space representation are as follows: aP = e − Ts Tc , bP = 1 − e − Ts Tc , cP = 1. (14) The state vector of T (θ¯, z), which is linearized to the PL model using E–FRIT, only corresponds to the plant output, given that E–FRIT tunes θ such that T (θ¯, z) and the I/O characteristics of the PL model match. Therefore, it is preferable to include only the scalar state variable for the PL model handled by MPC. Hence, we assume that the predictor of MPC is a first–order system, which is the PL model structure set by the designer. In this study, we estimate the i–step–ahead input as uˆ(k + i) from the PID controller and the optimized PL model as follows: uˆ(k + i) = ˆuP(k + i) + ˆuI(k + i) + ˆuD(k + i) (15) where uˆP(k+i), uˆI(k+i) , and uˆD(k+i) denote the estimated proportional, integral, and differential inputs, respectively, which are expressed below.    uˆP(k + i) = KPeˆv(k + i) uˆI(k + i) = ˆuI(k + i − 1) + KIeˆv(k + i)Ts uˆD(k + i) = KD eˆv(k + i) − eˆv(k + i − 1) Ts , (16) Here, eˆv(k + i) denotes the estimation of the internal error using the estimated output yˆ(k + i) for the PL model, which is expressed as follows: eˆv(k + i) = v(k + i) − yˆ(k + i), (17) where v(k) denotes the internal reference generated by the model–based controller. Note that uˆ(k) at i = 0 because the real output can be obtained from (15)–(17). Furthermore, the estimation accuracy of uˆ(k + i) for i ≥ 1 depends on the matching accuracy of the PL model and T (θ¯, z). C. MPC Design with PL Model In this subsection, we design an MPC system as an example of a model–based control system design using the PL model and the estimated input to the plant explicitly. Figure 3 shows a block diagram of the proposed method, which uses the PL model as a predictor of MPC. Hence, MPC can generate the optimal input while predicting the output of the closed–loop system. The cost function of the outer–loop MPC shown in Fig. 3 is expressed as follows: J(k, ∆v) =Jy(k, ∆v) + Ju(k, ∆v) + Jv(k, ∆v) subject to umin ≤ uˆ(k + i) ≤ umax, ∀k ≥ 0, i = 0 . . . Hu − 1, (18) Fig. 3. Block diagram for adaptive E–FRIT–based MPC where    Jy(k, ∆v) = X Hp i=1 kyˆ(k + i|k) − r(k + i|k)k 2 Q(i) Ju(k, ∆v) = H Xu−1 i=0 k∆ˆu(k + i|k)k 2 R(i) Jv(k, ∆v) = H Xu−1 i=0 k∆v(k + i|k)k 2 V (i) . (19) Jy(k, ∆v), Ju(k, ∆v), and Jv(k, ∆v) denote the evaluation terms for error, input variation, and internal reference variation, respectively. Ju(k, ∆v) evaluates ∆v(k) as a variable of the cost function because ∆ˆu(k) is generated by ∆v(k) as indicated by (15)–(17). In addition, Hp and Hu denote the prediction and control horizons, respectively. Furthermore, umin and umax denote the input constraints of the plant, and the matrices Q(i) ≥ 0 (i = 1, . . . , Hp), R(j) ≥ 0, V (j) > 0 (j = 0, . . . , Hu−1) denote the weight of each evaluation term. In the proposed method, the optimized variable that the MPC handles is v(k), and the internal reference variation weight V (i) is provided as a positive–definite matrix. Therefore, similar to the general MPC weight design, Q(i) is adjusted to reduce the error, and V (i) is adjusted to attenuate excessive input variation. In addition, the input variation weight R(i) is adjusted to suppress the oscillation of the input u(k) applied to the plant. In cases where oscillations do not occur, R(i) is designed to be 0. Specifically, in the simulations described in Section IV, R(i) ≡ 0, ∀i is designed. For simplicity, we assume Hp = Hu. Therefore, the result of the MPC under constraints provides an optimized internal reference v(k) by minimizing the evaluation function in (18), allowing high control performance to be achieved with consideration of plant input constraints without plant model information. IV. SIMULATION STUDY In this section, we verify the effectiveness of the proposed method using two classes of nonlinearity: the Hammerstein model and Bouc–Wen model with hysteresis. The Hammerstein model is often used as a case study for data– driven control [17], [18] for confirming the effectiveness of the proposed method for nonlinear classes. In contrast, the asymmetric Bouc–Wen model denotes the characteristics of the artificial muscles discussed in Section V. Therefore, the simulations correspond to the experiment presented in Section 5 V and are suitable for confirming the effectiveness of the proposed method in the experiment and for analyzing the frequency domain, which cannot be confirmed experimentally. In addition, we conducted simulations focusing on frequency– domain matching for PL using the proposed E–FRIT method. For the asymmetric Bouc–Wen model, which can denote the hysteresis characteristics, we verified the matching between the frequency characteristics of the designed PL model and those of the designed closed–loop system. A. Simulation: Hammerstein Model To verify the effectiveness and advantages of the proposed method over the conventional E–FRIT method, we conducted a simulation of the following second–order Hammerstein model. The system model in [17], [18] is expressed as    y(k) = 0.6y(k − 1) − 0.1y(k − 2) + 1.2x(k − 1) − 0.1x(k − 2) x(k) = 1.5u(k) − 1.5u 2 (k) + 0.5u 3 (k) . (20) Reference r(k) is given as r(k) =    0.5, 0 ≤ k < 50 1.0, 50 ≤ k < 100 2.0, 100 ≤ k < 150 1.5, 150 ≤ k < 200 . (21) The model in (20) is used solely to generate the I/O data. In the proposed method, we employed the controller configuration shown in Fig. 3. In contrast, the conventional method used the controller configuration shown in Fig. 1. The initial PID value was set to θ¯ = [1.00×10−2 , 1.00×10−2 , 1.00×10−3 ] T, and the controller was designed using the parameter set θ obtained through PL with E–FRIT. Using λ = 1.00 × 104 , the following parameters were obtained by the optimization: θ = [4.58 × 10−10 , 4.13 × 10−1 , 3.63 × 10−12 , 1.67]T. In the conventional method, the controller was designed using a PL model that incorporates Tc. In contrast, in the proposed method, the parameters of the PL model was adopted as predictions for the MPC. Both the prediction horizon Hp and control horizon Hu were set to five steps. The control objective of the proposed method involves tracking the target value as quickly as possible. To this end, the weight matrices were set to Q(i) ≡ 100I5, R(i) ≡ 0, and V (i) ≡ I5, ∀i. As noted in Section III. C, the weight matrix R(i) was set to zero in the simulation. Furthermore, the input constraints were set to umin = 0 and umax = 2 to prevent excessive inputs in the proposed method. Real–time control code was generated using the common convex optimization solver CVXGEN [19]. The simulation results are presented in Figs. 4 and 5. As shown in Fig. 4, the conventional method gradually tracked the reference. This is because the desired output ym(k) was designed according to PL(z) which was optimized without considering a reference, and the control system was designed to follow ym(k). In contrast, the proposed method accurately tracked the reference r(k). Because the control performance of the conventional method strongly depends on the design of Tc, it is generally difficult to design a PL(z) that exhibits the 0 50 100 150 200 Time [s] 0 0.5 1 1.5 2 2.5 Output Fig. 4. Comparison of tracking performance between the proposed and conventional methods in simulations of the Hammerstein model 0 50 100 150 200 Time [s] 0 0.5 1 1.5 2 Control input Fig. 5. Control input u(k) to the plant in simulations of the Hammerstein model same performance as the proposed method. In the proposed method, the control performance does not depend on Tc, and a designer can intuitively design the control performance by tuning certain weights of the cost function as in model– based designs. Furthermore, as shown in Fig. 5, although the proposed method generated a larger input to achieve a rapid response, owing to the large error weight Q(i), it appropriately considered the input constraints, thereby avoiding the wind– up phenomenon. The average root–mean–square error (RMSE) of the output was 1.937 × 10−2 for the proposed method and 1.213 × 10−1 for the conventional method. B. Asymmetric Bouc–Wen Model Several asymmetric Bouc–Wen models have been proposed to describe the hysteresis characteristics of plants [20], [21]. In this study, we use the previously proposed asymmetric Bouc–Wen model [22] because of its simple structure and high 6 accuracy. (22) denotes the asymmetric Bouc–Wen model:    y(k) = Y (k) + h(k) Y (k) = a1y(k − 1) + a2y(k − 2) + b2u(k − 1) h(k) = X 2 i=1 hi(k − i) hi(k − i) = Ai{y(k − i) − y(k − i − 1)} + βi |y(k − i) − y(k − i − 1)|hi(k − i) + γi{y(k − i) − y(k − i − 1)}|hi(k − 1)| + cih(k − i) + diy 2 (k − i) + eiy 3 (k − i), i = 1, 2 . (22) Here, a1, a2, b1 are linear parameters, and Ai , βi , γi , ci , di , and ei (i= 1, 2) are the hysteresis parameters. The nonlinear function h(k) denotes hysteresis. The asymmetric Bouc–Wen model can accurately denote asymmetric hysteresis, such as that observed in artificial muscles [22]. Therefore, by using this model for a simulation study of the experimental setup described later, the proposed method can be analyzed. C. Experimental Setup and Simulation Conditions Here, we discuss the parameters of the asymmetric Bouc– Wen model used in the simulations. To obtain the system parameters, we performed system identification using a tap water–driven artificial muscle actuator, as shown in Figs. 6 and 7. Artificial muscles have attracted considerable attention because of their advantages, such as their low cost, light weight, high power density, high flexibility, and simple structure [23]. In particular, tap water–driven artificial muscles have advantages as aqua drive systems (ADSs), owing to the low environmental load and availability and disposability of the working liquid [24]. At the various pressure levels of an ADS, tap water is easily used because it does not require special equipment such as a compressor, power supply, or water storage tank [25]. However, artificial muscles have strong asymmetric hysteresis owing to the nonlinear contraction behavior and friction between components. Therefore, it is difficult to control artificial muscles with high precision [22], [26]. Several methods that use data–driven control have been applied to artificial muscles [27], [28]. The proposed method is superior to model–based controller [22], [26] in that it does not require explicit mathematical models. On the other hand, it has the advantage of considering input constraints in the same manner as MPC compared to data–driven controllers in [27], [28]. The experimental setup consists of two McKibben–type artificial muscles, four proportional valves, a pulley, a wire, an encoder, and a controller PC. The artificial muscles contract under tap–water pressure. During the operation, one muscle is pressurized and the other is relaxed according to the valve opening, generating a pressure difference. This results in a difference in tension between the two artificial muscles, which drives the wire and causes the pulley connected to the wire to rotate. Proportional valves are a type of flow control wherein the flow rates are controlled by the input voltage u(k). The encoder measures the pulley at a rotational angle of y(k) and Fig. 6. Experimental circuit McKibben muscles Encoder Pulley Load Fig. 7. Experimental equipment sends it to the controller. Because the system is configured to generate inputs for the four proportional valves using a single input, it can be considered as a SISO nonlinear system. Details regarding the experimental setup are presented in Table I. We conducted system identification under these specifications. The obtained parameter sets are presented in Table II. The method used for system identification is based on that described by [22]. D. Simulation: Asymmetric Bouc–Wen Model We evaluated the performance of the control system design using the proposed control method and compared it with that of the conventional method. The Bouc–Wen model described in (22) was adopted as the plant. In the proposed method, we used the controller configurations shown in Fig. 3, and in the conventional method, we used the controller configuration shown in Fig. 1. The reference was sinusoidal, with a width of 25 deg, an offset of 30 deg, and a frequency of 0.2 Hz. The sampling time was Ts = 10 ms. The initial PID parameters were set to θ¯ = [5.00 × 10−2 , 5.00 × 10−2 , 1.00 × 10−2 ] T, and the controller was designed using the parameter set θ obtained via PL with the E–FRIT. With λ = 5.00 × 104 , the parameters obtained through optimization were θ = [1.30 × 10−1 , 1.51, 6.29 × 10−1 , 7.10 × 10−2 ] T. In the conventional method, the controller was designed to match a PL model including Tc, whereas in the proposed method, the parameters of the PL model were used to design a model predictive 7 TABLE I SPECIFICATIONS OF EXPERIMENTAL COMPONENTS Item Specifications Proportional valves KFPV300–2–80, Koganei Corporation. Cv Value: 1.6; Range of input voltage: 0 to 10 V Linear encoder DX–025, MUTOH Industries Ltd. Resolution: 0.01 mm Controller PC Operating system: Windows 10, Microsoft Corporation. CPU: 2.50 GHz, RAM: 16.00 GB Applications: MATLAB/Simulink and dSPACE 1103 Tap water–driven Handmade muscle, muscle Length: 400 mm Tap–water Average supply pressure: 0.15 MPa (G) TABLE II PARAMETER SET OF THE ASYMMETRIC BOUC–WEN MODEL Parameter Value a1 9.95832 × 10−1 a2 1.23972 × 10−3 b1 1.19205 × 10−2 A1 9.94593 × 10−1 β1 4.93442 × 10−1 γ1 −8.00753 × 10−1 c1 −3.34000 × 10−1 d1 2.34191 × 10−3 e1 −1.84394 × 10−5 A2 −1.13653 × 10−1 β2 −4.10528 × 10−1 γ2 6.79071 × 10−1 c2 3.51356 × 10−1 d2 −2.28465 × 10−3 e2 1.80024 × 10−5 controller. The prediction horizon Hp and control horizon Hu were each set to five steps. The weight matrices were Q(i) ≡ 5I5, R(i) ≡ 0, and V (i) ≡ I5, ∀i. Considering the hardware constraints of the experimental device that conducted the system identification, the input constraints of the MPC were set to umin = 0 V and umax = 10 V. The simulation results are presented in Figs. 8 and 9. As shown, both design methods tracked the desired output ym(k) generated by the reference model Gm(z), whereas the output of the proposed method followed the reference signal r(k) with high precision. However, because conventional E–FRIT works to match the reference model Gm(z) and depends strongly on the given Tc, it cannot track r(k). In contrast, the output of the proposed controller tracks r(k). Therefore, the proposed method achieves the control objective without depending on the reference model. The RMSE for the proposed method was 5.65×10−1 deg, whereas that of the conventional method was 2.83 deg. Fig. 8. Comparison of tracking performance between the proposed and conventional methods in simulations of the asymmetric Bouc–Wen model 10 12 14 16 18 20 Time [s] -5 -4 -3 -2 -1 0 1 2 3 4 5 6 Tracking error [deg] Fig. 9. Comparison of tracking error in the steady–state response between the proposed and conventional methods in simulations of the asymmetric Bouc– Wen Model Next, we analyzed the model matching performance of the PL using E–FRIT. Previous research [16] indicated that the PL model is optimal. The optimized PL model minimizes the matching error between the PL model and the closed– loop system during the control process. In the present study, this was confirmed through simulations in the frequency domain. Specifically, using Bode plots, we visually examined the matching between the optimized PL model and adjusted closed–loop system. The simulation conditions and PL model parameters were identical to those used in the control simulations, as shown in Figs. 8 and 9 for the asymmetric Bouc– Wen Model. Fig. 10 presents the Bode plot of the optimized PL model obtained using Tc = 7.10 × 10−2 and an optimized closed–loop system. From Fig. 10, it is evident that the PL model and the closed–loop system match well at an initial data frequency of 0.2 Hz. Moreover, the matching precisions on both sides of the frequencies–both phase and amplitude characteristics–were relatively low, and they were low for high frequencies as well. This is because as indicated by (7), the matching performed by E–FRIT is a local optimization based on the initial output data dependent on the initial reference. 8 10-1 100 101 Frequency [rad/s] -30 -20 -10 0 Magnitude [dB] 10-1 100 101 Frequency [rad/s] -100 -50 0 Phase [deg] Fig. 10. Comparison of Bode diagrams for closed–loop T(θ¯, z) and PL model PL(Tc, z) V. CASE STUDY: WATER–DRIVEN ARTIFICIAL MUSCLE CONTROL SYSTEM We compared the control performance of the conventional and proposed methods through experiments. In the experiments, we used a one–degree–of–freedom water–driven artificial muscle actuator with asymmetric hysteresis nonlinearity. A. Experimental Conditions The artificial muscle actuator is illustrated in Figs. 6 and 7 in the previous section. The specifications are presented in Table I. We evaluated the control performance of the proposed method and compared it with that of conventional methods for sinusoidal and square references. The sinusoidal reference was identical to that mentioned in Section IV. D, and the square reference r(k) was given by r(k) =    0, 0 ≤ k < 10 15, 10 ≤ k < 30 30, 200 ≤ k < 50 60, 300 ≤ k < 70 45, 300 ≤ k < 100 . (23) The sampling time was Ts = 10 ms. The initial PID parameters were set to θ¯ = [1.00 × 10−1 , 1.00 × 10−1 , 1.00 × 10−2 ] T, and the controller was designed using the parameter set θ obtained via PL with E–FRIT. With λ = 5.00 × 102 , the parameters obtained through optimization were θsin = [3.27 × 10−1 , 5.92 × 10−1 , 9.19 × 10−3 , 1.20 × 10−1 ] T for the sinusoidal reference and θsq = [1.45 × 10−1 , 1.52 × 10−1 , 1.11 × 10−1 , 9.10 × 10−1 ] T for the square reference. In the conventional method, the controller was designed to match a PL model including Tc, whereas in the proposed method, the parameters of the PL model were used to design a model predictive controller. The prediction horizon Hp and control horizon Hu were set to five steps. The weight matrices were Q(i) ≡ 4.3I5, R(i) ≡ 2.5I5, and V (i) ≡ 4I5, ∀i for the sinusoidal reference Q(i) ≡ 3I5, R(i) ≡ 0.2I5, TABLE III EVALUATION OF CONTROL PERFORMANCE FOR THE S INUSOIDAL REFERENCE Index Proposed method Conventional method RMSE (0 s to 100 s) 4.85 × 10−1 deg 2.88 deg SD (0 s to 100 s) 6.77 × 10−2 deg 1.36 × 10−2 deg and V (i) ≡ I5, ∀i for the square reference. Considering the hardware constraints of the experimental device that conducted the system identification, the input constraints of the MPC were set to umin = 0 V and umax = 10 V. B. Experimental Results and Discussion First, we discuss the effectiveness of the second term in the cost function of the MPC and its weight R(i) in (18). Figures 11–14 present comparisons of the control results with and without R(i) as a sinusoidal reference. As shown in Fig. 12, the output with R(i) ≡ 0 oscillated around the reference value. This is because the input u(k) to the plant oscillated, as shown in Fig. 13 owing to, for example, sensing noise. However, as shown in Fig. 14, the internal reference v(k) did not oscillate significantly even for R(i) ≡ 0, indicating that the weights V (i) were appropriately designed. Therefore, it is necessary to consider not only the internal reference v(k) but also the internal input u(k) to suppress the oscillations in these cases. The experimental results indicated that in such cases, the second term was necessary, and the appropriate design of the weight R(i) was effective. Next, we compared the control performance of the proposed method with that of the conventional method. As shown in Figs. 15 and 16, the experimental results for the control performance were similar to the simulation results presented in Section IV. D. Both design methods achieved tracking. The output of the conventional method tracked the desired output ym(k) generated by the reference model Gm(z), whereas the output of the proposed method followed the reference signal r(k) with high precision. As conventional E–FRIT works to match the reference model Gm(z), it tracks the desired output ym(k), which depends strongly on the given Tc, not tracking r(k). However, the output of the proposed controller is r(k). Therefore, the proposed method achieves the control objective and does not depend on the reference model. As shown in Figs. 17 and 19, the experimental results for the control performance were similar to the simulation results presented in Section IV. A. In particular, the proposed method considers input constraints. The control performance was evaluated according to the RMSE and standard deviation (SD) from the experiment, and the results are presented in Tables III and IV. As indicated by the quantitative evaluation results in these tables, the proposed method achieved high control performance. VI. CONCLUSION We proposed a novel model predictive controller using an optimized PL model based on E–FRIT, providing a solution 9 TABLE IV EVALUATION OF CONTROL PERFORMANCE FOR THE SQUARE REFERENCE Index Proposed method Conventional method RMSE (0 s to 100 s) 1.23 deg 1.98 deg SD (0 s to 100 s) 3.33 × 10−3 deg 1.11 × 10−2 deg RMSE (80 s to 100 s) 5.76 × 10−3 deg 2.92 × 10−2 deg SD (80 s to 100 s) 2.53 × 10−5 deg 7.16 × 10−3 deg 0 5 10 15 20 Time [s] 0 10 20 30 40 50 60 Rotational angle [deg] Fig. 11. Comparison of tracking performance with and without input variation weights R(i) in the experimental results for the sinusoidal reference 10 12 14 16 18 20 Time [s] -2 -1 0 1 2 3 Tracking error [deg] Fig. 12. Comparison of the increased tracking errors in the steady–state responses with and without input variation weights R(i) in the experimental results for the sinusoidal reference to the problem of control performance degradation due to the design of an improper reference model and consideration of input constraints in the E–FRIT design. The control performance of the conventional E–FRIT strongly depended on the reference model. To solve this problem, we introduced a PL model and construct a model– based controller in the outer loop. This allowed us to separate the design of the PL model from the tracking performance of the reference. Specifically, we designed a model predictive controller as an outer–loop controller. In addition, we considered the input constraints of an unknown plant by estimating the plant input according to an internal reference. Using two nonlinear classes, i.e., the Hammerstein and asymmetric Bouc–Wen models, we conducted simulations and confirmed that PL is more suitable for reference tracking 0 5 10 15 20 Time [s] 0 2 4 6 8 10 Valve input voltage [V] Fig. 13. Comparison of the control inputs (u(k)) to the plant with and without input variation weights R(i) in the experimental results for the sinusoidal reference 0 5 10 15 20 Time [s] 0 10 20 30 40 50 60 Internal reference Fig. 14. Comparison of the internal reference v(k) with and without input variation weights R(i) in the experimental results for the sinusoidal reference Fig. 15. Comparison of tracking performance between the proposed and conventional methods in the experimental results for the sinusoidal reference control than conventional E–FRIT. Furthermore, compared with conventional E–FRIT, it was shown that the control performance did not depend on the PL model or the parameters of the reference model. Moreover, we compared the proposed method with a conventional method that uses a one–degree– of–freedom rotating actuator composed of two artificial muscles. The proposed method achieved high tracking control performance for two types of references; the experimental results were similar to the simulation results. In addition, we confirmed that the proposed method can explicitly consider 10 10 12 14 16 18 20 Time [s] -5 -4 -3 -2 -1 0 1 2 3 4 5 6 Tracking error [deg] Fig. 16. Comparison of tracking errors in the steady–state responses between the proposed and conventional methods in the experimental results for the sinusoidal reference 0 20 40 60 80 100 Time [s] 0 10 20 30 40 50 60 70 Rotational angle [deg] Fig. 17. Comparison of tracking performance between the proposed and conventional methods in the experimental results for the square reference 9 10 11 12 13 14 15 Time [s] 0 5 10 15 20 Rotational angle [deg] Fig. 18. Comparison of tracking performance between the proposed and conventional methods in the experimental results for the square reference, which is an enlargement of Fig. 17 from the initial rise from 9.5 to 15 s input constraints, which are the hardware limitations of the plant. In the future, we will extend the proposed method to robust PL that considers changes in the characteristics of the system during operation and in the reference frequency as well as to a controller structure of the inner loop that explicitly considers nonlinearity.",
		"summary": "To reduce the typical time–consuming routines of plant modeling for model–based controller designs, the fictitious reference iterative tuning (FRIT) has been proposed and has proven to be effective in many applications. However, it is generally difficult to select a reference model properly without information on the plant, which significantly affects the control performance and sometimes leads to considerable performance degradation. To address this problem, we propose a pseudo– linearization (PL) method using FRIT and design a new controller for nonlinear systems that combines data–driven and model–based control. This design considers the input constraints using model predictive control. The effectiveness of the proposed method was evaluated according to several practical references using numerical simulations for nonlinear classes and experiments involving artificial muscles with hysteresis characteristics.",
		"id": "UUID10"
	},
	{
		"document": "I. INTRODUCTION In many control applications, it might not always be feasible to directly measure all states of a dynamical system. In such scenarios, a reliable estimation of these unmeasurable state variables becomes paramount and is usually accomplished through state observers. While the design principles of Luenberger observers [1] are commonly applied to linear systems, addressing this challenge for nonlinear systems is inherently more complex [2]. Numerous efforts have been directed to develop nonlinear observers using extended Kalman filters [3]–[5]. However, most of these methods depend on linearization techniques, and therefore, they offer only local convergence. The theory of Kazantzis-Kravaris/Luenberger (KKL) observers [6] extends the Luenberger observer design principles to nonlinear systems. KKL observers are founded on the existence of an injective mapping and its left inverse, enabling the transformation of a nonlinear system into a set of firstorder linear differential equations subject to the injection of the nonlinear system output. The existing literature provides comprehensive analyses of KKL observers, including the exploration of requirements for the existence of an injective transformation, discussions on the dimensionality of 1 L. Trommer is with the Control Systems Group at the Technical University of Berlin, Germany. {lukas.trommer@campus.tu-berlin.de} 2 H. Y. Oks ¨ uz is with the Control Systems Group and Science of ¨ Intelligence, Research Cluster of Excellence, at the Technical University of Berlin, Germany. {oksuz@tu-berlin.de} * H. Y. Oks ¨ uz acknowledges funding by the Deutsche Forschungsge- ¨ meinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC 2002/1 “Science of Intelligence” – project number 390523135. the linear state space [7], [8], and the extension of these principles to non-autonomous systems [9], [10]. Although it has been demonstrated that the use of an approximation for the state space transformation is sufficient, deriving a concrete formulation of the corresponding mapping and its left inverse for generic problem statements remains a formidable challenge [11]. One viable approach is to leverage data-driven methods, involving the collection of data samples through system simulation or measurement. With increased computational power and a growing interest in machine learning, neural networks have emerged as versatile solutions for addressing highly complex and nonlinear problems in a variety of domains [12], [13], since they are recognized as universal approximators when appropriately dimensioned [14]. Various strategies have been proposed to formulate the approximation of KKL observer transformation maps as regression problems solvable through feedforward neural networks. In [10], the nonlinear system and the linear part of the observer have been simulated forward in time, starting from pre-specified initial system conditions. Pairwise sets of state data, where one state functions as the input and the other as the label for the nonlinear regression problem, have been generated in order to determine the respective transformation maps. In [15], a deep auto-encoder has been introduced for systems in discrete time, which learns the transformation maps by enforcing observer dynamics in a latent linear space and introducing a measure of reconstruction loss following the composite application of forward and inverse transformations during training. An unsupervised learning technique has been utilized in [16], whose aim is to improve convergence accuracy by using a neural network-based ensemble learning approach. By introducing a physics-informed loss component during training, some improvements in robustness, accuracy, and generalization capabilities have been observed in [17]. It is important to emphasize that learning-based approaches for KKL observer design require labeled, nonlinear system state data for training. While the estimation capabilities of these data-driven approaches benefit from substantial datasets, it is not always practical to utilize arbitrary quantities of data for training. Moreover, in certain scenarios requiring accurate state estimation over varying system parameters, training data may only be available for a limited subset of parameter values. The imposed learning challenge in that case furthermore includes the aim for strong model generalization, allowing accurate estimation even in parameter ranges for which no data is accessible during training. Aiming to enhance these capabilities, we resort to ModelarXiv:2310.19489v1 [eess.SY] 30 Oct 2023 Agnostic Meta-Learning (MAML), which facilitates training a model on a multitude of tasks drawn from a task distribution [18]–[20] in order to quickly adapt and generalize to new tasks from that distribution with only a few gradient steps, rendering it highly data-efficient for adaptation. We introduce a novel approach to learning-based KKL observer design, harnessing the fundamental principles of MAML to enhance the precision of nonlinear system state estimation through online adaptation to distinct system attributes. The adaptation process exclusively depends on system output measurements to extract inherent system knowledge and dynamically adjust estimation capabilities. This proposed method deviates from the static learning paradigms prevalent in previous methodologies, marking a transition towards enhanced adaptability. The remainder of the paper is organized as follows. In section II, the theory of KKL observers and the fundamental principles of MAML are provided. In Section III, we present our novel meta-learning-based approach to KKL observer design in detail. We then evaluate the performance of our proposed approach in Section IV, providing empirical results and analysis. Finally, Section V offers our concluding remarks. Notation The set of real numbers is denoted by R, R m represents the m-dimensional Euclidean space, R≥0 the set of nonnegative real numbers. N respectively denotes the set of natural numbers. The Euclidean norm of the vector x ∈ R m is denoted by ||x||. The matrix exponential of matrix A is defined as e A = P∞ k=0 1 k!Ak . Given a finite set S, its cardinality is denoted by |S|. The notions 1N , IN , and diag(1, 2, ..., N) respectively denote the N-dimensional vector with all entries equal to one, the N × N dimensional identity matrix, and the diagonal matrix with entries 1, 2, ..., N. [a, b] m ⊂ R m denotes the m-ary Cartesian power of the finite interval [a, b] ⊂ R. II. BACKGROUND A. KKL Observers We consider a dynamical, autonomous, nonlinear system ( x˙(t) = f (x(t)) y(t) = h (x(t)) (1) where x ∈ X ⊂ R dx denotes the nonlinear system state in state space X , and y ∈ Y ⊂ R dy the system output. f : X 7→ R dx and h : X 7→ Y are smooth functions that articulate the nonlinear system dynamics and measurable outputs, respectively. As elucidated in [6]–[8], the formulation of a KKL observer entails the identification of an injective1 transformation map, denoted by F : X 7→ Z, that serves the purpose of converting the dynamics of the nonlinear system (1) into a linear dynamical system described by z˙(t) = Az(t) + By(t), (2) 1∀x1, x2 ∈ X : F(x1) = F(x2) =⇒ x1 = x2. where z(t) ∈ Z is the linear system state in state space Z ⊂ R dz . Hence, the relationship between the linear state z(t) and the nonlinear state x(t) is expressed as z(t) = F (x(t)). It is imperative that the transformation map F satisfies the following partial differential equation: ∂ ∂x (F (x(t))) f (x(t)) = AF (x(t)) + Bh (x(t)). (3) Since the transformation map F is injective, the KKL observer for the nonlinear system (1) is instantiated through the application of the inverse transformation map F −1 : Z 7→ X to the linear observer state z(t) and is defined as ( z˙(t) = Az(t) + By(t) x(t) = F −1 (z(t)). (4) This formulation incorporates the appropriate initialization of the linear observer state with z(0) = F (x(0)). In the context of learning-based KKL observers, we approximate F and F −1 as depicted by [7] and denote those as Fˆ and Fˆ−1 . In order to ensure the existence of such transformation maps, we make the following assumptions: Assumption 1: Let the state trajectory of (1) with the initial state x(0) = x0 be expressed by x(t; x0). There exists a compact set X ⊂ R dx such that ∀x0 ∈ X , t ∈ R≥0 : x(t; x0) ∈ X . Assumption 2: There exists an open bounded set S ⊃ X wherein (1) is backward S-distinguishable2 . Previous studies such as [2], [7], [17], [21] emphasize that a uniformly injective3 map F satisfying (3) exists if (A, B) is controllable, A is Hurwitz, and Assumptions 1 and 2 hold. For formal proof and a more comprehensive context, we direct readers to the cited works. B. Model-Agnostic Meta-Learning (MAML) The concept of MAML [18] revolves around training a model on a distribution of tasks, enabling it to rapidly adapt to new tasks drawn from the same distribution. The central notion is to acquire transferable intrinsic features across the given task distribution. Consider a task T defined as a tuple T = (u, o), where u ∈ U denotes the input belonging to the input space U and o ∈ O corresponds to the desired output within the output space O. In the context of meta-learning, we consider a model gˆθ ∈ G parameterized by θ. Here, G ⊂ {U 7→ O} denotes the function space containing gˆθ, which maps inputs to outputs. Additionally, a loss function L : O×U ×G 7→ R is employed. Tasks are sampled from a given task distribution T , denoted as T ∼ T . Throughout this distribution, the tasks vary, exposing the model to different inputs and corresponding desired outputs. During the meta-training process, the model 2Given an open set S ⊃ X, (1) is said to be backward S-distinguishable on X if for every pair of distinct initial conditions x0,1, x0,2 ∈ X, there exists τ < 0 such that x(t; x0,1), x(t; x0,2) ∈ S are well-defined for t ∈ [τ, 0], and h(x(τ; x0,1)) ̸= h(x(τ; x0,2)) [17]. 3A function ρ : R≥0 7→ R≥0 is said to be of class K if it is continuous, zero at zero, and strictly increasing. The map F is said to be uniformly injective if there exists a class K function ρ such that ∀x1, x2 ∈ X : ∥x1 − x2∥ ≤ ρ(∥F(x1) − F(x2)∥). [17] parameters evolve to enable adaptation to specific tasks T using only a limited number of samples and gradient updates. MAML accomplishes this behavior through a training algorithm characterized by a nested loop. At the beginning of each outer loop iteration, a meta-batch of tasks {Ti |Ti ∈ T , i = 1, 2, ..., Nbatch,meta} is sampled. Subsequently, an inner loop is entered, where the task-specific model parameters θi are computed by using adaptation data points (ui,a, oi,a) from each task Ti with the update rule θi = θ − α∇θL(oi,a, ui,a, gˆθ) (5) and adaptation learning rate α. Upon completion of the inner loop updates, a metaupdate for the model parameters θ is executed. This update entails the accumulation of losses computed based on a set of predictions for query data points (ui,q, oi,q) which are generated using the updated, task-specific parameter sets θi obtained from the inner loop. Mathematically, this update can be expressed as θ = θ − β∇θ X Ti∼T L(oi,q, ui,q, gˆθi ) (6) with meta learning rate β. The computation of gradients in (6) encompasses second-order gradients through the inner loop gradients. This results in an optimization process where the parameters in the outer loop are adjusted in a manner aimed at minimizing the loss function through consideration of the inner loop parameter updates, which reflect the taskspecific adaptation process. C. Problem Statement In this paper, our objective is to find a more general approximation of transformation maps F and F −1 given in (3) and (4) through Fˆ θ and Fˆ−1 η , realized as feedforward neural network models with learnable parameters θ and η. Given variations in system attributes, such as the initial nonlinear system state x(0) or the intrinsic parameterization of the nonlinear system function f(·), we define a distribution of learning tasks, T , with tasks arising from notable differences in the trajectories x(t) and z(t) of systems (1) and (2) over time t. In this case, employing a meta-learning approach on the task distribution T is expected to allow for both, better generalization across T , and to adapt to a single task T ∼ T as it is being observed. III. METHODOLOGY A. Task-Specific Data Generation For each task T ∼ T , we construct a dataset DT ⊂ X × Z which contains pairs of data samples, denoted as (x(t; T), z(t; T)). Here, x(t; T) represents the state trajectory of the nonlinear system related to task T, while z(t; T) corresponds to the state of the linear system, respectively. This synthetic approach of data generation, in line with methodologies presented in [10], [17], employs the RK4 method [22], [23] (or another suitable solver) to simultaneously forward simulate both systems (1) and (2) for a given task T. The resulting dataset can be expressed as DT = {(x(t; T), z(t; T))|t = k∆t, k = 0, 1, ..., N} (7) where ∆t is the time step size used in the solver and N ∈ N. Given the analytic solution to the system of linear differential equations in (2) [24] z(t) = e Atz(0) + Z t 0 e A(t−τ)By(τ )dτ, (8) the effect of any initial state z(0) ̸= F(x(0)) would eventually diminish due to the Hurwitz property of A. However, an arbitrary initialization for training data generation might introduce regression errors, particularly for simulated data samples near t = 0, depending on the choice of the matrix A. To mitigate such errors resulting from inaccurate training data, we adopt a backward sampling method for (1) and (2). This strategy, as described in prior works such as [17], [25], allows us to obtain the precise initial state z(0) = F(x(0)) at which the linear system has reached the steady state: 1) Given some small ϵ > 0, the system matrix A, its eigendecomposition A = V ΛV −1 , and minimum eigenvalue λA,min, compute the time τ < 0 such that ||e A(−τ) z(τ )|| < ϵ using τ ≤ 1 λA,min ln  ϵ cond(V )∥z(τ )∥  . (9) 2) Simulate x(t) in backward time for t ∈ [τ, 0] given x(0) and compute y(t) = h(x(t)). 3) Simulate z(t) in forward time for t ∈ [τ, 0] given z(τ ) and y(t) from the previous step to obtain z(0) = F (x(0)). B. Forward Computation During Training During the training process, we employ two distinct modes of forward computation. These modes can be categorized as either sequential or parallel in their application of the map approximations Fˆ θ and Fˆ−1 η . The sequential method reconstructs the nonlinear system state approximation, xˆ(t), using the linear system state approximation, zˆ(t), as the basis for computation. This is achieved through zˆ(t) = Fˆ θ (x(t)), xˆ(t) = Fˆ−1 η  Fˆ θ (x(t)) . (10) In contrast, the parallel method avoids the composition of both map approximations and directly utilizes labels for the state of the linear observer system z(t) to compute xˆ(t). The forward computation in this mode can be expressed as zˆ(t) = Fˆ θ (x(t)), xˆ(t) = Fˆ−1 η (z(t)). (11) The sequential approach, depicted in Figure 1 (1), applies Fˆ−1 η to the linear state estimate zˆ(t), aiming to provide a direct inverse to the approximation Fˆ θ. Conversely, the parallel approach, illustrated in Figure 1 (2), applies Fˆ−1 η independently from Fˆ θ to directly invert F. (1) (2) x(t) xˆ(t) z(t) zˆ(t) Fˆ−1 η Fˆθ X Z F −1 F Fˆ−1 η Fig. 1: Modes of forward computation during training, sequential (1) and parallel (2). C. Mixed-Task Learning Methods We consider a set of training tasks Ttrain sampled from the distribution T with task-specific datasets DT for T ∈ Ttrain and a joint dataset D = ∪T ∈TtrainDT , which comprises mixed data points corresponding to different tasks in Ttrain. We address the supervised regression problem to learn Fˆ θ and Fˆ−1 η using (11) on mixed data points from D [10]. This approach, termed parallel mixed-task learning, involves the loss functions Lz  z(t), x(t), Fˆ θ  =       z(t) − Fˆ θ (x(t))       2 Lx  x(t), z(t), Fˆ−1 η  =       x(t) − Fˆ−1 η (z(t))       2 , (12) which are independently addressed with the optimization problems min θ Lz  z(t), x(t), Fˆ θ  (13) and min η Lx  x(t), z(t), Fˆ−1 η  . (14) Similarly, following the methodology proposed in [17] based on the references therein, we denote the same supervised regression problem when using the sequential forward computation of (10). We refer to this approach as sequential mixed-task learning. Induced by the composition of both maps, the loss function Lx in this case becomes Lx  x(t), x(t), Fˆ−1 η ◦ Fˆ θ  =       x(t) − Fˆ−1 η  Fˆ θ (x(t))      2 , (15) which leads to the optimization problem for θ depending on both Lx and Lz with min θ  Lx  x(t), x(t), Fˆ−1 η ◦ Fˆ θ  + Lz  z(t), x(t), Fˆ θ  . (16) The optimization problem for η is then given by min η Lx  x(t), x(t), Fˆ−1 η ◦ Fˆ θ  . (17) D. Meta-Learning for System Output Adaptation We introduce the concept of meta-learning for system output adaptation as a complementary approach to parallel mixed-task learning if h(·) in (1) is known or can be approximated through another data-driven regression approach. We employ the mixed learning method for the approximation Fˆ θ Algorithm 1 Meta-Learning for System Output Adaptation Require: T (Distribution of learning tasks) Require: β (Meta learning rate) 1: Initialize model Fˆ−1 η with parameters η 2: Initialize adaptation learning rate α 3: while not done do 4: for i ← 1, Nbatch,meta do 5: ηi ← η (Initialize task-specific parameters) 6: Ti ∼ T (Sample learning task) 7: Generate DTi 8: for j ← 1, Nadapt do 9: (xi,a, zi,a) ∼ DTi (Sample adaptation data) 10: ηi ← ηi − α∇ηiLy(h(xi,a), zi,a, h ◦ Fˆ−1 ηi ) 11: end for 12: (xi,q, zi,q) ∼ DTi (Sample query data) 13: end for 14: η ← η − β∇η P i Lx(xi,q, zi,q, Fˆ−1 ηi ) 15: α ← α − β∇α P i Lx(xi,q, zi,q, Fˆ−1 ηi ) 16: end while 17: return Fˆ−1 η and propose Algorithm 1 to learn the inverse map approximation Fˆ−1 η . Inspired by the principles of MAML [18], our meta-learning algorithm follows a nested training loop structure, where inner loop iterations involve multiple adaptation updates, while outer loop iterations include meta-updates. During each outer iteration, a meta batch of Nbatch,meta learning tasks Ti is sampled from T . It is important to emphasize that new tasks Ti are sampled from T during each training iteration. However, to maintain consistency with the previously introduced mixed-task learning algorithms, we continue to refer to this set of tasks over the entire execution of the training algorithm collectively as Ttrain. Consequently, we ensure that all learning algorithms are applied to the same training data D to preserve comparability. For each task Ti , the inner loop performs Nadapt optimization steps on a local set of model parameters ηi for adaptation data points (xi,a, zi,a) ∼ DTi , utilizing h(·) on estimations xˆ(t), the system output loss function defined as Ly  y(t), z(t), h ◦ Fˆ−1 η  =      y(t) − h  Fˆ−1 η (z(t))      2 (18) and adaptation learning rate α. Subsequently, the taskspecific model parameters ηi are evaluated using query data points (xi,q, zi,q) ∼ DTi , and the approximation loss for predicting the nonlinear system state is accumulated across all tasks Ti within the meta batch. In the next step, the accumulated loss is used to perform an optimization step in the outer loop, updating the model parameters η based on the meta-learning rate β. In addition to meta-learning the parameters η, the adaptation learning rate α is also introduced as a trainable parameter during the meta-optimization process. This inclusion enhances accuracy for the specified learning problem and the chosen number of adaptation steps, (2) (1) Ly  y(t), z(t), h ◦ Fˆ−1 η  Fˆ−1 η yˆ(t) y(t) xˆ(t) x(t) h(·) z(t) Lx  x(t), z(t), Fˆ−1 η  Fig. 2: Computational structure of meta-learning for system output adaptation algorithm. (1) The approximation loss Lx for the entire system state vector is computed for metaupdates only. (2) The approximation loss Ly for the measurable system output contributes to the adaptation updates. eliminating the need for manual hyperparameter tuning. In essence, the described algorithm seeks to determine a set of model parameters η that enable the model to be updated during the adaptation procedure of the inner loop. The secondorder gradients involved in the meta update process adjust η such that task-specific adaptation steps minimize the overall approximation loss Lx. Importantly, as the adaptation updates are computed based on the measurable system output, this approach supports online adaptation if a system output estimate yˆ(t) can be derived from the estimated system state xˆ(t) following the previously mentioned constraint that h(·) or an approximation is known. The computational structure of meta-learning for system output adaptation is further illustrated in Figure 2. E. Online Adaptation To adapt the meta-learned inverse map approximation Fˆ−1 η online to the specific attributes of the observed nonlinear system, we implement the inner loop adaptation updates from Algorithm 1 using data samples drawn from both y(t) and the estimation xˆ(t) governed by the unadapted metalearning-based KKL observer. It is imperative to emphasize the necessity of an initialization period, denoted as tinit, which arises from this sampling operation, as depicted in Figure 3. The minimum duration of tinit is determined by the number of data points processed during task-specific adaptation and is expressed as: tinit ≥ Nbatch · Nadapt · ∆t, (19) where Nbatch represents the data batch size. The sampling period can be modified by either delaying, extending (which includes randomly sampling the required data points from (1) (2) tinit xˆ(t) y(t) (3) Fig. 3: Online adaptation with illustrated sampling delay (1), sampling period (2) and adaptation updates for inverse transformation model parameters (3). within), or using a combination of both strategies. Consequently, the impact of the chosen sampling period on the estimation accuracy additionally relies on the linear system dynamics of the KKL observer and the initial state of (2) determined through Fˆ θ. IV. EXPERIMENTS Building upon experimental setups given in the literature [10], [15]–[17], [25], we assess the accuracy and performance of our proposed meta-learning algorithm for KKL observer by design employing a variation of the Duffing oscillator as nonlinear system, which is defined as follows:    x˙(t) = ' x˙ 1(t) x˙ 2(t) # = λ ' x 3 2 (t) −x1(t) # + wx(t) y(t) = x1(t) + wy(t). (20) The variable λ denotes an internal parameter that influences the oscillation frequency and the terms wx ∈ R dx and wy ∈ R dy account for system and measurement noise, respectively. The transformation maps Fˆ θ and Fˆ−1 η are approximated through feedforward neural networks with 5 hidden layers of 50 neurons each, ReLU activation functions and normalization layers. This configuration closely follows the framework presented by [17]. We design the linear system (2) of the KKL observer by adopting A = −diag(1, 2, ..., dz), B = 1dz and dz = 2dx + 1 = 5. It is worth noting that, as pointed out by [17], various adjustments such as an H∞ design of matrices A and B, the generation of an arbitrary amount of training data through simulation, or the use of more complex neural network architectures, could potentially enhance accuracy, generalization capability, and robustness against noise. However, we aim to demonstrate the performance of the previously introduced methodologies under conditions where these improvements may be impractical or unattainable. In the following, we evaluate 4 different learning methods4 : sequential mixed-task learning, physicsinformed sequential mixed-task learning as introduced by [17] as supervised PINN, parallel mixed-task learning, and our proposed method, meta-learning for system output adaptation. We use Adam optimization [26] for the former 3 4The code implementation for the experiments described in this paper is available at github.com/lukastrm/metakkl. methods as well as the meta-updates of the meta-learning approach and stochastic gradient-descent optimization for the adaptation updates. The proposed meta-learning approach not only functions as an independent learning algorithm but also exhibits improved performance when the inverse map is pretrained in a parallel mixed-task learning setting. This strategy is incorporated into all our experiments. To quantify the accuracy of our estimations for x(t) and z(t), we use normalized estimation errors as follows: e ∗ x (t) = ∥x(t) − xˆ(t)∥ ∥x(t)∥ , e∗ z (t) = ∥z(t) − zˆ(t)∥ ∥z(t)∥ . (21) We furthermore define the normalized estimation error averaged over a set of tasks T as e¯ ∗ T (t, e∗ , T ) = 1 |T | X T ∈T e ∗ (t; T), (22) and the normalized estimation error for a task T ∈ T averaged over time as e¯ ∗ t (T, e∗ ) = 1 N X N k=0 e ∗ (k∆t; T), (23) where e ∗ (t) refers to either e ∗ x (t) or e ∗ z (t), depending on which state space is under evaluation. We determine these error metrics for a KKL observer with a linear system that has reached the steady state. For online adaptation, we choose the minimum sampling period which also includes drawing the adaptation samples for the metatrained KKL observer from the non-steady phase of z(t), but also consider other sampling periods in section IV-B. A. System Parameter Variation In this experiment, we assess the accuracy of the KKL observer across a distribution of tasks Tλ that incorporate varying values of the parameter λ. The initial nonlinear system state is fixed at x(0) = [0.5, 0.5]T . During training, we employ a small set of 5 distinct parameter values within the range λ ∈ [1, 5] to generate state trajectories. For validation, we extend our analysis to a set of tasks Tλ,val, comprising 200 different parameter values for λ ∈ [1, 5] within the training range. The results, depicted in Figure 4, reveal two significant findings. Firstly, it is evident that the approximation Fˆ θ displays notable inaccuracies when trained on a dataset involving mixed tasks with varying λ. This, in turn, results in substantial estimation errors for the KKL observer with sequential mixed-task learning, primarily due to the inverse map approximation Fˆ−1 η being trained on an imprecise representation of the linear system space. In contrast, parallel mixed-task learning and in particular also our meta-learning approach, utilizing parallel forward computation during training, overcome this limitation. The inverse map Fˆ−1 η is consistently trained on accurately simulated data samples for z(t), thus achieving independence from the accuracy of Fˆ θ. Secondly, we emphasize how, despite having a limited amount of training data encompassing state trajectories for 1 2 3 4 5 Parameter ¸ 0.25 0.50 0.75 Error ¹e ¤ t (T » ¸; e ¤ z ) 1 2 3 4 5 Parameter ¸ 0.0 0.5 1.0 Error ¹e ¤ t (T » ¸; e ¤ x ) 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Parameter ¸ 0.05 0.10 0.15 Error ¹e ¤ t (T » ¸; e ¤ x ) Fig. 4: Simulation results for task distribution Tλ over varying λ for different learning methods: (•) meta-learning with online adaptation, (•) parallel mixed-task learning, (•) sequential mixed-task learning, (•) physics-informed, sequential mixed-task learning. The dashed, black vertical lines in the lower plot indicate the 5 distinct parameter values for λ on which the KKL observer was trained. only 5 distinct values of λ, our proposed approach demonstrates an improvement in overall validation accuracy through enhanced generalization. Despite the inherent challenge of achieving high accuracy across the entire parameter range due to limited training data, meta-learning and online adaptation leverage the intrinsic system properties acquired from the system output. This allows for more precise estimation of the system state, even for previously unexplored parameter ranges. B. Initial Nonlinear System State Variation In this set of experiments, we assess the accuracy of the KKL observer for a distribution of tasks Tx(0) with varying initial nonlinear system states x(0) and fixed parameter value λ = 1. For training, we draw a set of 50 different initial states x(0) ∈ Xx(0),train ∼ [−1, 1]2 using latin hypercube sampling [27] to generate state trajectories. For validation, we consider two different sets of tasks: • Tx(0),val,in for 50 different initial states within the training range, i.e. x(0) ∈ [−1, 1]2 \ Xx(0),train • Tx(0),val,out for 80 different initial states outside of the training range with x(0) ∈ [−2, 2]2 \ [−1, 1]2 To evaluate accuracy under the influence of noise, we introduce Gaussian noise components with wx(t) ∼ N (0, 0.1) and wy(t) ∼ N (0, 0.1). Figures 5a and 5b display the estimated nonlinear system state xˆ(t) using the meta-learning-based KKL observer with online adaptation, clearly highlighting its ability to deliver accurate state estimations, even in the presence of noise. −0.5 0.0 0.5 x1(t) 0 10 20 30 40 50 Time t −1 0 1 x2(t) (a) xˆ(t) (•, solid) and x(t) (•, dotted) with x(0) = [0.5, 0.5]T , wx(t) = 0 and wy(t) = 0. −0.5 0.0 0.5 x1(t) 0 10 20 30 40 50 Time t −1 0 1 x2(t) (b) xˆ(t) (•, solid) and x(t) (•, dotted) with x(0) = [0.5, 0.5]T , wx(t) ∼ N (0, 0.1) and wy(t) ∼ N (0, 0.1). −1 0 1 x1(0) −1.0 −0.5 0.0 0.5 1.0 x2(0) 0.2 0.4 Error ¹e ¤ t (T » x(0); e ¤ x ) (c) Error profile over x(0) ∈ [−1, 1]2 . 0 10 20 30 40 50 Time t 0.02 0.04 0.06 Error ¹e ¤ T (t; e ¤ x ; Tx(0); val; in) (d) Error for validation tasks within training range (Tx(0),val,in). 0 10 20 30 40 50 Time t 0.050 0.075 0.100 0.125 Error ¹e ¤ T (t; e ¤ x ; Tx(0); val; out) (e) Error validation tasks outside of training range (Tx(0),val,out). Fig. 5: Simulation results for task distribution Tx(0) over varying x(0) for different learning methods: (•) meta-learning with online adaptation, (•) parallel mixed-task learning, (•) sequential mixed-task learning, (•) physics-informed, sequential mixed-task learning. In Figure 5d, we present the error (22) for the validation task set associated with initial system states within the training range. Notably, our proposed methodology consistently achieves the highest accuracy estimations on average when compared to other learning methods. Furthermore, our approach demonstrates strong generalization capabilities, evidenced by the notably low estimation error for initial system states outside of the training range, as portrayed in Figure 5e. Figure 5c shows the profile of the error (23) across the entire training range. In can be observed that the normalized error is larger for initial system states around x(0) = [0, 0]T , which can be contributed to numerical imprecisions that arise from a wide normalization range in combination with an absolute loss function during training. This effect has previously also been observed in [10]. We further explore the impact of the chosen adaptation sampling period on the precision of the KKL observer. Figure 6 presents the simulation results for the meta-learningbased KKL observer with online adaptation, employing four different variations of the sampling strategy. The results highlight that, in the case of the periodic Duffing oscillator, opting for the minimum sampling window without delay leads to the least accurate estimation outcomes. As the linear system error, originating from the initialization using Fˆ θ, diminishes over time, expanding the sampling window contributes to improved accuracy. This improvement is due to estimations for xˆ(t) being derived from progressively less erroneous samples in Z. Notably, the most accurate estimation performance is observed for delayed sampling windows, ensuring that any error stemming from an inaccurate initial state in Z has dissipated before the initial adaptation samples are drawn. V. CONCLUSION In this paper, we have introduced a novel meta-learningbased algorithm to approximate the inverse transformation map, making it adaptable to different system attributes, and conducted a comprehensive comparison of various learningbased KKL observer approaches. We have demonstrated through experiments that our proposed method, integrated with the parallel mixed-task learning algorithm, exhibits heightened estimation accuracy for scenarios both within and outside the training range. This underscores the enhanced generalization capability of meta-learning. Our methodology has also displayed adaptability over varying system parameters, even when only limited task-specific data is available. The integration of meta-learning principles, leveraging the 0 10 20 30 40 50 Time t 0.005 0.010 0.015 0.020 Error ¹e ¤ T (t; e ¤ x ; Tx(0); val; in) Fig. 6: Simulation results for task distribution Tx(0) over varying x(0) for meta-learned KKL observer with different sampling periods for online adaptation: (•) minimum sampling period without delay, (•) sampling period of size t = 50 and random sampling of data points within, (•) minimum sampling period delayed by −τ , (•) sampling period of size t = 50 delayed by −τ . ability to comprehend intrinsic features of the addressed task distribution, has exhibited effectiveness, notably in situations where learning the transformation map F proves challenging within a given setup. This especially outpaces approaches incorporating sequential forward computation during training. Our future research will focus on integrating various enhancements, such as those outlined by the authors of [28], which could potentially enhance accuracy and foster a more stable learning behavior. Additionally, the necessity of an adaptation sampling period requires careful evaluation. Metalearning-based KKL observers for systems with dynamically changing attributes may benefit from periodic re-adaptation, but the frequency of attribute changes is constrained by the initialization time of the observer.",
		"summary": "The theory of Kazantzis-Kravaris/Luenberger (KKL) observer design introduces a methodology that uses a nonlinear transformation map and its left inverse to estimate the state of a nonlinear system through the introduction of a linear observer state space. Data-driven approaches using artificial neural networks have demonstrated the ability to accurately approximate these transformation maps. This paper presents a novel approach to observer design for nonlinear dynamical systems through meta-learning, a concept in machine learning that aims to optimize learning models for fast adaptation to a distribution of tasks through an improved focus on the intrinsic properties of the underlying learning problem. We introduce a framework that leverages information from measurements of the system output to design a learning-based KKL observer capable of online adaptation to a variety of system conditions and attributes. To validate the effectiveness of our approach, we present comprehensive experimental results for the estimation of nonlinear system states with varying initial conditions and internal parameters, demonstrating high accur",
		"id": "UUID11"
	},
	{
		"document": "1 Introduction Treatment effect estimation methods that directly use an estimated propensity score (PS) have historically relied on maximum likelihood estimation (MLE) of a standard binary response model, such as logit or probit. While MLE is natural when the goal is to estimate the parameters in the propensity score model, it is not necessarily preferred when the PS estimators are used to obtain an object of interest such as the average treatment effect (ATE) or the average treatment effect on the treated (ATT). In fact, when one allows that the PS model might be misspecified, one can obtain estimates of average treatment effects that are more resilient using PS estimates other than the MLE. Graham et al. (2012) introduce the method of “inverse probability tilting” (IPT), where the parameters in the PS model are obtained from moment conditions that “balance” the means of the control variables, X. Because two potential outcome means are used to obtain the ATE, different PS parameters are estimated using the treated and untreated units. Graham et al. (2012) show that the IPT estimator has a satisfying asymptotic local efficiency property. For our purposes, the asymptotic properties of the IPT estimator are not directly relevant, as we derive algebraic equivalences when IPT weights are used to obtain popular ATE estimators. These are the usual inverse probability weighting (IPW) estimators that originated with Horvitz and Thompson (1952), augmented inverse probability weighting (AIPW) with linear mean functions (Robins, Rotnitzky and Zhao, 1994), and inverse probability weighted regression adjustment (IPWRA) with linear mean functions (Hirano and Imbens, 2001; Wooldridge, 2007; Słoczy´nski and Wooldridge, 2018). At this point, it is useful to head off a point of possible confusion. In studying higher-order properties of IPT and competitors, Graham et al. (2012) present a framework that includes IPT and the typical AIPW and IPWRA estimators. Importantly, the latter two are based on the PS estimated by MLE, and so the three estimators are generally different (and AIPW is not normalized). Here we establish that if the same IPT weights are used in all estimators, there is no algebraic difference between them. An alternative to IPT for estimating the ATE was proposed by Imai and Ratkovic (2014). Rather than estimating separate PS parameters for the treated and untreated units, Imai and Ratkovic (2014) obtain a single set of parameters—just like the MLE approach. The estimates are chosen so that weighted averages of the covariates across treated and untreated groups are the same. Imai and Ratkovic (2014) refer to the resulting estimates of the PS as the “covariate balancing propensity score” (CBPS) estimates. It turns out that the CBPS estimates do not lead to normalized weights when used in IPW, and the IPW, AIPW, and IPWRA procedures are not generally identical. Nevertheless, Imai and Ratkovic (2014) also propose CBPS weights for estimating the ATT. The estimated PS parameters are chosen so that a weighted average of the covariates over the untreated units equals the average of the covariates over the treated units. This property leads 2 to normalized weights for estimating the ATT. As we show in Section 3, it also leads to equivalence of IPW, AIPW, and IPWRA when all three use CBPS weights and the latter two use a linear regression function for the outcome. The rest of the paper is organized as follows. In Section 2, we review the estimation problems solved by the IPT weights for the ATE and the CBPS weights for the ATT, and discuss some simple implications. In Section 3, we derive the equivalences among various IPT-based estimators of the ATE and then the CBPS-based estimators of the ATT. In Section 4, we briefly discuss the implications that the algebraic equivalence results have for estimating the local average treatment effect (LATE) and the local average treatment effect on the treated (LATT). We emphasize that, because in all cases we are establishing numerical equivalences, we need not, and do not, state assumptions under which the estimators are consistent. These have been covered elsewhere and are well known. 2 Balancing Propensity Scores In a general missing data setting, Graham et al. (2012) introduced exponential tilting as a way of estimating the propensity score along with other parameters of interest in the context of missing data. Imai and Ratkovic (2014) introduced the covariate balancing propensity score in the context of treatment effect estimation. In this section we review the estimation problems solved by these methods in the binary treatment case. Let W be the binary treatment indicator and define the propensity score P(W = 1|X = x). Assume an index model, p(xγ), where x is 1 × K, γ is K × 1, and x1 ≡ 1 so that the linear index always includes an intercept. In most applications, p(·) is the logistic function, and this was assumed in Imai and Ratkovic (2014). The potential outcomes are Y(0) and Y(1). Recall that the ATE and the ATT are defined as τate = E[Y(1)−Y(0)] and τatt = E[Y(1)−Y(0)|W = 1]. However, we emphasize that the results of this paper have to do with algebraic equivalences, and so we do not discuss identification of population parameters. When p(xγ) = exp(xγ)/[1+exp(xγ)], given a sample of size N, it is well known that the first-order condition solved by the maximum likelihood estimator, γˆmle, is N ∑ i=1 X ′ i [Wi − p(Xiγˆmle)] = 0. (2.1) 3 If we define residuals as Wi − p(Xiγˆmle), conditions (2.1) imply that the residuals always average to zero and are uncorrelated with each Xj,i in the sample—properties shared by the OLS estimator of a linear model. (Typically, we would think of having a random sample from the population; but, again, the results in this paper are algebraic, and the mechanism generating the data plays no role.) The inverse probability tilting (IPT) moment conditions proposed by Graham et al. (2012) are different. When W is a selection indicator meaning that the elements of X are observed, Graham et al. (2012) propose using the moment conditions E  W p(Xγ) X ′  = E(X ′ ), (2.2) which follow immediately by iterated expectations when p(Xγ) = P(W = 1|X) = E(W|X). (If we were considering identification, we would need to assume, at a minimum, that p(Xγ) > 0 with probability one.) The sample analog of (2.2) is N −1 N ∑ i=1 WiXi p(Xiγˆ1,ate,ipt) = X¯ , (2.3) and these equations define the IPT estimator of γ, γˆ1,ate,ipt. Note that we have put a “1” subscript on γˆ1,ate,ipt because, in the treatment effects setting, there is another set of moment conditions that leads to a different IPT estimator of γ. Again by iterated expectations, E  1−W 1− p(Xγ) X ′  = E(X ′ ), and this leads to the sample analog: N −1 N ∑ i=1 (1−Wi)Xi 1− p(Xiγˆ0,ate,ipt) = X¯ . In general, γˆ0,ate,ipt 6= γˆ1,ate,ipt. However, because 1 ∈ Xi , it follows immediately that N ∑ i=1 Wi p(Xiγˆ1,ate,ipt) = N (2.4) and N ∑ i=1 1−Wi 1− p(Xiγˆ0,ate,ipt) = N. (2.5) These two equations are key, as the summands in (2.4) are the weights for estimating E[Y(1)] in IPW estimation and those in (2.5) are the weights used in estimating E[Y(0)], as we review in Section 3. Equations (2.4) and (2.5) show that the IPT weights are automatically normalized (or self-balancing, or stabilized) for estimating the ATE. That is, the sample mean of the weights is not stochastic but instead equal to one by construction. 4 Imai and Ratkovic (2014) use different moment conditions to obtain a single estimator of γ when the focus is on the ATE. Again, if the propensity score is correctly specified then, by iterated expectations, E(X ′ ) = E  W p(Xγ) X ′  = E  1−W 1− p(Xγ) X ′  . (2.6) Rather than using the implications of (2.6) separately, which is what IPT does, Imai and Ratkovic (2014) use the second equality to obtain the following sample moment conditions: N −1 N ∑ i=1 Wi p(Xiγˆate,cbps) X ′ i = N −1 N ∑ i=1 1−Wi 1− p(Xiγˆate,cbps) X ′ i . (2.7) After simple algebra, the moment conditions can be expressed as N ∑ i=1  Wi − p(Xiγˆate,cbps) p(Xiγˆate,cbps)  1− p(Xiγˆate,cbps)  ! X ′ i = 0. (2.8) Comparing (2.8) with (2.1), we can see that the CBPS approach effectively weights the MLE moment conditions by the estimated inverse conditional variance, Var(Wi |Xi). Because the first element of Xi is unity, (2.7) implies that N ∑ i=1 Wi p(Xiγˆate,cbps) = N ∑ i=1 1−Wi 1− p(Xiγˆate,cbps) . (2.9) Equation (2.9) shows that the weights appearing in the IPW estimates of E[Y(1)] and E[Y(0)] sum to the same value, but that common value is not necessarily the sample size, N. Therefore, when these are used as weights in IPW, the CBPS weights are not automatically normalized. For estimating the ATT, the moment equations used by Imai and Ratkovic (2014) are E(X ′ |W = 1) = 1 ρ ·E(W · X ′ ) = 1 ρ ·E  p(Xγ) (1−W) 1− p(Xγ) · X ′  , where ρ = P(W = 1). Using ρˆ = N1/N, where N1 is the number of treated units, the K sample moment conditions are N −1 1 N ∑ i=1 WiX ′ i =  N1 N −1 N −1 N ∑ i=1 p(Xiγˆatt,cbps) (1−Wi) 1− p(Xiγˆatt,cbps) · X ′ i = N −1 1 N ∑ i=1 p(Xiγˆatt,cbps) (1−Wi) 1− p(Xiγˆatt,cbps) · X ′ i or X¯ ′ 1 = N −1 1 N ∑ i=1 p(Xiγˆatt,cbps) (1−Wi) 1− p(Xiγˆatt,cbps) · X ′ i , (2.10) 5 where X¯ 1 = N −1 1 ∑ N i=1WiXi . Because 1 ∈ Xi , (2.10) implies N1 = N ∑ i=1 p(Xiγˆatt,cbps) (1−Wi) 1− p(Xiγˆatt,cbps) , which implies that the weights used in IPW estimation of the ATT sum to the number of treated units. In other words, the CBPS weights for estimating the ATT are automatically normalized. 3 Equivalence of Estimators We now establish numerical equivalences of three different estimators that incorporate inverse probability weighting, starting with estimators of the ATE. 3.1 Estimators of the ATE The three estimators we consider are among the most popular when unconfoundedness is assumed to hold: IPW, AIPW, and IPWRA. As we are establishing algebraic equivalence, we do not impose assumptions beyond those needed for existence of the estimates for a given sample. This simply means the estimated propensity scores are strictly between zero and one for all i. The IPW estimator of τate using the IPT weights is τˆate,ipt = µˆ1,ipt − µˆ0,ipt = N −1 N ∑ i=1 WiYi p(Xiγˆ1,ipt) − N −1 N ∑ i=1 (1−Wi)Yi 1− p(Xiγˆ0,ipt) , (3.11) where “ate” has been dropped from the estimators of γ for notational simplicity and we use “ipt” to indicate that the IPT weights are being used. See, for example, Wooldridge (2010, Section 21.3) for a variant of this estimator with MLE-based weights and Graham et al. (2012) for IPT. We know by (2.4) and (2.5) that the weights in both of the weighted averages sum to unity and so the weights are already normalized. The AIPW estimator with IPT weights, which we refer to as AIPT, is also the difference in estimates of µ1 ≡ E[Y(1)] and µ0 ≡ E[Y(0)]; that is, τˆate,aipt = µˆ1,aipt − µˆ0,aipt . For µ1, µˆ1,aipt = N −1 N ∑ i=1 Wi  Yi −Xi ˆβ1  p(Xiγˆ1,ipt) + N −1 N ∑ i=1 Xi ˆβ1, (3.12) where remember that 1 ∈ Xi . Although it is not important for the equivalence result, the estimates ˆβ1 typically come from an OLS regression of Yi on Xi using Wi = 1 (treated units). The first term in (3.12) is a weighted average of the resulting residuals over the treated units. The weights are exactly those appearing in µˆ1,ipt and are therefore normalized. 6 For µ0, the AIPT estimator is µˆ0,aipt = N −1 N ∑ i=1 (1−Wi)  Yi −Xi ˆβ0  1− p(Xiγˆ0,ipt) + N −1 N ∑ i=1 Xi ˆβ0, (3.13) where ˆβ0 are probably the OLS estimates from a regression of Yi on Xi using Wi = 0. The third estimator we consider is the IPWRA estimator with IPT weights, which we refer to as IPTRA. For µ1, we first solve a weighted least squares (WLS) problem, min b1 N −1 N ∑ i=1 Wi pˆi (Yi −Xib1) 2 , (3.14) where ˆpi = p(Xiγˆ1,ipt) are the IPT propensity score estimates. Given the WLS estimates ˜β1 from (3.14), µ1 is estimated by averaging the fitted values across all observations, as in the case of linear regression adjustment: µˆ1,iptra = N −1 N ∑ i=1 Xi ˜β1 = X¯ ˜β 1 . (3.15) The IPTRA estimator of µ0, µˆ0,iptra, uses the untreated units with weights(1− pˆi) −1 , and produces ˜β0. The final IPTRA estimator of the ATE is given by τˆate,iptra = µˆ1,iptra − µˆ0,iptra = X¯ ˜β 1 −X¯ ˜β 0 . When the PS weights are obtained using MLE, or some other method of moments procedure, τˆate,ipw, τˆate,aipw, and τˆate,ipwra are generally different. In fact, τˆate,ipw and τˆate,aipw do not generally use normalized weights, and so one could have five different estimates using the same estimated PS weights: IPW, normalized IPW (NIPW), AIPW, normalized AIPW (NAIPW), and IPWRA. (IPWRA is always normalized.) Strikingly, when IPT weights are used instead, all of the estimates are the same. Proposition 3.1. Let γˆ1,ipt be the estimates from the IPT estimation in equation (2.3), with pˆi = p(Xiγˆ1,ipt) > 0 for all i. Then µˆ1,ipt, µˆ1,aipt, and µˆ1,iptra are numerically identical. The same is true of µˆ0,ipt, µˆ0,aipt , and µˆ0,iptra, which means that τˆate,ipt = τˆate,aipt = τˆate,iptra. Proof. See Appendix A. The implication of Proposition 3.1 is that, if one uses the IPT weights in estimating both µ0 and µ1, where conditional means E[Y(0)|X] and E[Y(1)|X] are modeled linear, then three commonly used estimators of the ATE are numerically identical; moreover, the IPW and AIPW versions are automatically normalized. 7 3.2 Estimators of the ATT We now establish equivalence of several common estimators of the ATT when the CBPS weights are used. Recall that τatt = E[Y(1)|W = 1]−E[Y(0)|W = 1] ≡ µ1|1 − µ0|1 , and the first term is always consistently estimated using the sample mean of Yi over the treated units, Y¯ 1. The IPW estimator for the second term, using the CBPS weights (which are automatically normalized), is µˆ0|1,ipwcbps = N −1 1 N ∑ i=1 p(Xiγˆcbps) (1−Wi)Yi 1− p(Xiγˆcbps) . (3.16) As mentioned in Section 2, the weights in (3.16) sum to unity. These are the same weights that appear in the AIPW estimator, and so the unnormalized and normalized AIPW estimators are also the same. Specifically, the AIPW estimator of µ0|1 is µˆ0|1,aipwcbps = N −1 1 N ∑ i=1 pˆi(1−Wi) 1− pˆi  Yi −Xi ˆβ0  + N −1 1 N ∑ i=1 WiXi ˆβ0 = N −1 1 N ∑ i=1 pˆi(1−Wi)Yi 1− pˆi − N −1 1 N ∑ i=1 pˆi(1−Wi)Xi ˆβ0 1− pˆi + X¯ 1 ˆβ0, (3.17) where ˆpi = p(Xiγˆcbps) are now the CBPS propensity score estimates and ˆβ0 is typically the OLS estimator from regressing Yi on Xi using Wi = 0. Finally, the IPWRA estimator of µ0|1 is µˆ0|1,ipwracbps = X¯ 1 ˜β0, where ˜β0 now solves the WLS problem: min b0 N −1 N ∑ i=1 pˆi(1−Wi) 1− pˆi (Yi −Xib0) 2 , (3.18) where ˆpi = p(Xiγˆcbps). We have the following equivalence result. Proposition 3.2. Let γˆcbps be the estimators solving (2.10) with pˆi = p(Xiγˆcbps) < 1 for all i. Then the IPW, AIPW, and IPWRA estimates of µ0|1 using the CBPS weights, and linear conditional means in the latter two cases, are identical. Therefore, the three estimates of τatt are identical. Proof. See Appendix A. 8 4 Implications for Estimators of the LATE and LATT The previous results for the ATE and ATT have implications for estimators of the LATE and LATT when using control variables X; a recent treatment is Słoczy´nski, Uysal and Wooldridge (2022), which we follow here. As before, W is a binary treatment. Now we also have a binary instrumental variable, Z. As discussed in Słoczy´nski et al. (2022), many estimators of the LATE are ratios of estimators of the ATE, τˆlate = τˆate,Y|Z τˆate,W|Z , where τˆate,Y|Z is an estimator of the ATE where Y is the outcome, Z plays the role of the treatment, and the covariates X are used to account for confounders of Z. Again, we are only concerned with equivalences and not statistical properties. The denominator, τˆate,W|Z , is an estimated ATE where W is the outcome and Z again is the treatment indicator, with covariates X. It follows from Proposition 3.1 that when linear conditional means are used for both Y and W, and IPT is used for the PS weights, estimators of the LATE based on IPW, AIPW, and IPWRA are all identical. The propensity score weights in this case, for both the numerator and the denominator, are for the instrument propensity score: P(Z = 1|X) = q(Xδ). It should be noted, however, that unlike in the case of the ATE, where the nature of Y is unspecified, here it may be impractical to use the linear model in the denominator given the binary nature of W. See Słoczy´nski et al. (2022) for using other doubly robust estimators to exploit the binary nature of W and maybe special features of Y. We do not have a general equivalence of the different estimators using CBPS weights, and the normalized and unnormalized estimators of the ATE are generally different. However, in the case of the LATE, the weights used in the numerator and denominator are the same—namely, Zi q(Xi ˆδate,cbps) and 1−Zi 1−q(Xi ˆδate,cbps) . By (2.9), with the obvious change in notation, the weights for Zi = 1 and Zi = 0 sum to the same value (but not unity in general). We can factor this common value out of the expressions for the ATEs in both the numerator and denominator, and then cancel. It follows that the IPW and NIPW estimators of the LATE are identical, as previously observed by Heiler (2022), and the same is true of AIPW and NAIPW. (Recall that IPWRA is implicitly normalized.) Estimators of the LATT that incorporate control variables X can be written as the ratio of estimators of the ATT, where the instrument plays the role of the treatment variable: τˆlatt = τˆatt,Y |Z τˆatt,W|Z , 9 where τˆatt,Y|Z and τˆatt,W|Z are both estimators of the ATT with “treatment” variable Z and outcome variables Y and W, respectively. If these estimators use the appropriate CBPS weights, as in Proposition 3.2, then it follows immediately that the versions of τˆlatt based on IPW, AIPW with linear regression functions, and IPWRA with linear regression functions are all numerically the same. Also, recall that the normalized and unnormalized estimators of the ATTs are identical when using these weights. Appendix A Proofs Proof of Proposition 3.1. Consider estimating µ1; the argument for µ0 follows in the same way. First, with ˆpi ≡ p(Xiγˆ1,ate,ipt), equation (2.4) implies that N −1 N ∑ i=1 Wi/pˆi = 1. The IPT estimator of µ1 is the first term in (3.11). Now, consider the AIPT estimator in (3.12). Simple algebra shows it can be expressed as µˆ1,aipt = N −1 N ∑ i=1 WiYi pˆi − N −1 N ∑ i=1 WiXi ˆβ1 pˆi + N −1 N ∑ i=1 Xi ˆβ1 = µˆ1,ipt − N −1 N ∑ i=1 WiXi ˆβ1 pˆi + X¯ ˆβ 1 = µˆ1,ipt + ' X¯ −N −1 N ∑ i=1 WiXi pˆi # ˆβ1 = µˆ1,ipt, where the last equality uses (2.3) (with an intercept explicitly included). Now, consider the IPTRA estimator in (3.15). Given that 1 ∈ Xi , the first-order condition for the WLS estimator of β1, following from (3.14), is easily seen to imply that N −1 N ∑ i=1 WiYi pˆi =  N −1 N ∑ i=1 WiXi pˆi ! ˜β1. The term on the left is, again, µˆ1,ipt. For the term on the right, use the IPT moment conditions in (2.3), as before: µˆ1,ipt = N −1 N ∑ i=1 WiYi pˆi =  N −1 N ∑ i=1 WiXi pˆi ! ˜β1 = X¯ ˜β 1 = µˆ1,iptra. Repeating the same argument for µ0 completes the proof. 10 Proof of Proposition 3.2. Recall that µ0|1 ≡ E[Y(0)|W = 1]. Redefine ˆpi as ˆpi ≡ p(Xiγˆatt,cbps). Using simple algebra, the AIPW estimator of µ0|1 using CBPS weights, given in (3.17), can be written as µˆ0|1,aipwcbps = N −1 1 N ∑ i=1 pˆi(1−Wi)Yi 1− pˆi − N −1 1 N ∑ i=1 pˆi(1−Wi)Xi ˆβ0 1− pˆi + X¯ 1 ˆβ0 = µˆ0|1,ipwcbps + ' X¯ 1 −N −1 1 N ∑ i=1 pˆi(1−Wi)Xi 1− pˆi # ˆβ0 = µˆ0|1,ipwcbps, where the final equality follows from (2.10), the CBPS moment conditions for estimating the ATT. For IPWRA using CBPS weights, note that the first-order condition for ˜β0, following from (3.18), is N ∑ i=1 pˆi(1−Wi) 1− pˆi X ′ i  Yi −Xi ˜β0  = 0. Focusing on the first element 1 ∈ Xi and dividing by N1, we can write N −1 1 N ∑ i=1 pˆi(1−Wi)Yi 1− pˆi = N −1 1 N ∑ i=1 pˆi(1−Wi)Xi ˜β0 1− pˆi or, again using (2.10), µˆ0|1,ipwcbps = X¯ 1 ˜β0 = µˆ0|1,ipwracbps. This completes the proof.",
		"summary": "We show that when the propensity score is estimated using a suitable covariate balancing procedure, the commonly used inverse probability weighting (IPW) estimator, augmented inverse probability weighting (AIPW) with linear conditional mean, and inverse probability weighted regression adjustment (IPWRA) with linear conditional mean are all numerically the same for estimating the average treatment effect (ATE) or the average treatment effect on the treated (ATT). Further, suitably chosen covariate balancing weights are automatically normalized, which means that normalized and unnormalized versions of IPW and AIPW are identical. For estimating the ATE, the weights that achieve the algebraic equivalence of IPW, AIPW, and IPWRA are based on propensity scores estimated using the inverse probability tilting (IPT) method of Graham, Pinto and Egel (2012). For the ATT, the weights are obtained using the covariate balancing propensity score (CBPS) method developed in Imai and Ratkovic (2014). These equivalences also make covariate balancing methods attractive when the treatment is confounded and one is interested in the local average treatment effect. ",
		"id": "UUID12"
	},
	{
		"document": "I. INTRODUCTION Introduction of a blockchain technology proposed in a seminal paper by Satoshi Nakamoto1 soon gave birth to a completely new financial market on which cryptocurrencies were traded2 . Since its infancy stage in the years 2011-2015, where both prices and the number of traders were relatively small3 this market undergo significant evolution and now it can be considered a large, mature market that couples itself to or decouples from the regular financial markets4,5. Interest in the cryptocurrency market grew enormously during the Covid-19 pandemic, because investors were looking for a safe haven in times of market panic6–10; low interest rates, and quantitative easing11, and this contributed substantially to the cryptocurrency market liquidity12–15. However, even though such expectations were based on actual observation that during 2019 the cryptocurrency market was largely independent from other markets16, they proved futile as the cryptocurrencies had tendency to align with the main financial instruments during the most dramatic market events in 202017–22 and recently in 2022 due to inflation concerns4,23,24 . Cryptocurrencies share many traits with fiat currencies, among which one can point out to the full equivalence (indistinguishability) of units. This is what discerns cryptocurrencies from non-fungible tokens (NFTs)25. A non-fungible token is a type of data stored on a blockchain that contains its unique sign. In contrast to typical cryptocurrency units like bitcoin or ether, which are indistinguishable, interchangeable, and have the same value, each NFT has its individual value and cannot be replaced by another token. Ownership and authenticity of a particular token are permanently recorded and easily verifiable via a relevant blockchain so it is impossible to counterfeit it. This property can be exploited to represent ownership of digital or physical assets, such as art, music, videos, games, collectibles, and much more26–28 . The idea of NFT was put into practice with the adoption of ERC-721 smart-contract protocol that allowed users of Ethereum blockchain to create tokens that could have distinguishable signs29,30. While originally confined to this network only, later on it spread to other blockchains like Solana, Polygon, Flow, Binance Smart Chain, Cardano, Arbitrum, and recently even Bitcoin31. Although the first NFT was presented in May 2014, a significant trigger for the NFT market development was the launch of the CryptoKitties collection in November 2017. CryptoKitties is an online game in which a player can buy, breed, and sell virtual cats represented by cartoon images. Each “cat” is a non-fungible token that differs from other tokens by its visual properties and rarity. The game operates on the Ethereum blockchain network and at one moment it became so popular that it caused an overload of the whole network32,33. Shortly thereafter, the first organized NFT marketplace - OpenSea - began operating. The NFT-market has boomed in recent years even more, especially during the middle stage of the Covid-19 pandemic in 2021, Characteristics of price related fluctuations in Non-Fungible Token (NFT) market 2 with its peak of interest at the turn of 2021 and 2022. In October 2023, while this article was written, daily turnover of the NFT markets oscillated around 30 billion USD, which is a serious downfall if compared with the period of the recordhigh activity in late 2021, when turnover used to exceed 200 billion USD34. There is evidence that the growth of the NFT market has closely been related to the recent bubble on the cryptocurrency market35 . An important part of the NFT market is devoted to the gaming community for selling and buying in-game items, but there are also other notable NFT types, which comprise virtual landscapes, virtual real estate, and digital artwork36. It often happens that tokens and token collections are created and originally sold in order to raise funds for financing new projects. After an initial offer, NFTs can be freely traded, which poses an incentive to consider them as normal investment instruments. Indeed, many token collections allowed their primary buyers to earn a multiple of the invested amounts. The most expensive NFT was Mike Winkelmann’s “Everydays: The First 5000 Days” – a collection of 5000 unique illustrations that the artist had created over 5000 consecutive days, which was sold for 69 million USD in March 202137. This event drew attention to the growing popularity of the NFT market and the significance of digital assets in today’s art world. After a few years of existence of the NFT technology, Ethereum remains the blockchain with the largest market share and it is followed by others: Bitcoin, ImmutableX, Polygon, Solana, Flow, Arbitrum and BNB Chain with a changing share in the market capitalization and trading volume34 . The NFT market is second to the cryptocurrency market in terms of research interest to date25,38. It is because novelty of NFTs and their non-fungible character bear several challenges for NFT market research39,40. The first challenge is that the NFT market is younger and substantially less liquid than the cryptocurrency market. It also reveals extreme volatility that together with infrequent trading seriously limits the data volume that can be analyzed39. The second challenge is high heterogeneity of token prices within a collection. It is partially associated with a token’s rarity, i.e., a relative frequency of occurrence of the traits attributable to this token. It has been reported that token rarity may have an impact not only on its price (rare tokens are expensive) but also on a frequency of trading (rare tokens are seldom traded), amplitude of return (rare tokens are profitable), and negative-return resilience (rare tokens are safer)41. Although these properties of tokens make statistical analysis difficult, some of them may however constitute good predictive factors regarding future price42. The third challenge are lateral swaps, which can distort price evolution of a given token by substituting, for example, a smart contract execution fee that differs substantially from a token’s market price that should actually be listed there. The fourth challenge is wash trading, i.e., a long series of trading of the same token, a goal of which is to deceive other market participants about actual liquidity and/or price of this token. Wash trades are extremely difficult to be filtered out, because they may not differ from the proper trades in terms of price and frequency. It was suggested that a majority of token collections undergo this procedure from time to time43 . High volatility and the related inefficiency of the NFT market was the main outcome of a study of pricing of LAND tokens representing parcels of virtual estate in a metaverse of Decentraland44. This inefficiency manifested itself in dominating antipersistence of daily LAND price which can potentially allow one to make related profits. However, such a behaviour may always be expected in the case of a young market. High volatility of NFT prices does not exclude using these assets for hedging, especially against the more risky cryptocurrencies and DeFis45. It was shown that net volatility spillover in low- and medium-volatility events is largely positive for cryptocurrencies and DeFis, while it is negative for NFTs45. As it is a measure of risk transmission potential, NFTs have a potential for diversifying portfolio. This is not the case, however, if a high-volatility events are considered. A conclusion that goes in parallel direction was given in46 according to which NFTs generate their own shocks and are rather resistant to external ones; this refers even to ether, which is closely related to many NFTs. During turbulent times NFTs tend to absorb external volatility shocks, while during times of normal evolution they behave like transmitters of risk spillovers46 . Somehow opposite conclusion can be learnt from a study of the cryptocurrency, DeFi, and NFT bubbles that occurred during the pandemic47. According to that study, the NFT and DeFi bubbles grow faster and reach higher levels, but also they are less frequent than cryptocurrency bubbles. Also outside the bubbles NFTs show high-return and high-volatility profile that make them ideal for the investors accepting risk in order to make quick gains48. An increased coherence between evolution of NFTs and regular financial assets like bonds, equities, and commodities (gold, oil) was reported in46,49 during the pandemic (2020), which agrees with a parallel conclusion regarding the cryptocurrency market17. A different study suggests that NFTs reveal certain degree of coherence with cryptocurrencies, but it must be limited as volatility transmission between the two is weak50 . Despite the above research works, there is a limited number of quantitative studies focusing on the statistical properties of NFTs. In this article we want to fill this gap partially through an analysis of a few selected financial observables like intertransaction times, return distributions, and transaction volume values2,4,51–53. We focus on a few sample NFT profile-picture collections that are distinguished by their popularity among the NFT investors that was manifested by a high transaction volume around mid-2023. Below there are short descriptions of all collections considered in this study. Each of these collections is operated on the Solana blockchain network. Blocksmith Labs Smyths collection is a set of 4,444 NFTs associated with $FORGE token that have been created and operated on Solana blockchain network. They are represented by digital images depicting certain noble characters with fantastic attributes. The collection was started in March 2022 by the company Blocksmith Labs, which specializes in development of products exploiting the blockchain technology54 . Famous Fox Federation collection is a set of 7,777 NFTs associated with FOXY token. They are represented by digital images of Characteristics of price related fluctuations in Non-Fungible Token (NFT) market 3 stylized cartoon foxes with different characteristics. The collection was developed by a group of Solana network users in order to help them create an ecosystem of various blockchainrelated tools55 . Lifinity Flares collection consists of 10,000 NFTs represented by animated images of flares. It was released in December 2021 by Lifinity Foundation in order to fund development of its market making protocol aimed at improving liquidity and increasing investor revenues56 . Okay Bears is a collection consisting of 10,000 NFTs represented by digital images of cartoon bears with different colour and outfits. The collection, which was launched by two private entrepreneurs, is associated with a broadly planned project whose goal is to be a virtual place for hosting various community and start-up initiatives57. Finally, Solana Monkey Business collection is a set of several thousand NFTs that are represented graphically by pixel-art images of monkeys and grouped in 3 “generations”. These NFTs have been being released in tranches since May 2021 by a MonkeDAO community specializing in development of Web3 projects58 . II. DATA AND METHODS We consider tick-by-tick data representing transactions (price and time) on 5 NFT collections: Blocksmith Labs Smyths (BL), Famous Fox Federation (FF), Lifinity Flares (LF), Okay Bears (OKB), and Solana Monkey Business (SM) and floor prices of the same collections. Each data set covers a period starting on a collection launch day and ending on Sep 1, 2023. We transformed these data sets into time series of 5 observables: collection capitalization C, floor price pfl, transaction value V∆t (volume multiplied by price) in time unit ∆t, the number of transactions in time unit N∆t and intertransaction times δti = ti+1 −ti . Because over a few days after the market debut by a collection trading is incomparably more intense than later on, in order for our analysis to be feasible, we omit the data representing the first week of trading for each collection. Without this step the data would have been too non-stationary. A relatively small number of transactions that are made on NFT collections and long waiting times between the consecutive transactions as well as value variability among the members of the same collection has led to the need for a different measure of collection value than a last transaction price. Floor price is defined for a collection and it refers to the lowest ask price of a token belonging to this collection that sellers can accept at a given moment. Floor price may differ from the lowest transaction price and it is a unique characteristic of the NFT market. This feature makes the NFT market more closely resemble an auction market than traditional financial markets with a book of buy and sell orders. The data has been downloaded from CryptoSlam! portal (tick-by-tick transaction data)59 and Magic Eden portal (floor price data), which is an aggregator of the most important Solana marketplaces: CoralCube, Elixir, Fractal, HadeSwap, OpenSpace, Solanart, TensorSwap60. Selected characteristics of each collection are gathered in Tab. I. Based on transaction prices, for each collection, we calculated total collection capitalization by summing up the last transaction prices of all the tokens expressed in USD and SOL. In order to make the data more stationary, we transformed the tick-bytick data of collection capitalization and floor price into time series of logarithmic returns c∆t(t) = lnC(t +∆t)−lnC(t) and r∆t(t) = ln pf l(t +∆t)−ln pf l(t) with sampling frequency ∆t, which we chose to be 1 hour, because the transaction frequency on the NFT market is rather low. As value of solana, the native cryptocurrency for the considered collections, fluctuates with respect to the US dollar, four return time series were subject to further analysis: collection capitalization returns for USD and SOL and collection floor price returns for USD and SOL. In order to inspect the self-similarity properties of the time series, we applied the multifractal detrended fluctuation analysis (MFDFA), which is a relatively simple and reliable method to detect scaling of the fluctuations61,62. For time series u(j) let X = {X(i)} T i=1 be an integrated time series: X(i) = i ∑ j=1 u(j). (1) For a given scale s, we divide X into Ms disjoint segments of length s starting from both ends, which gives Ms = 2⌊T/s⌋, and fit the mth-degree polynomial P (m) s,ν to X in each segment ν. Then we subtract a local trend defined by this polynomial from the signal profile: xs,ν(i) = Xs,ν(i)−P (m) s,ν (i) (2) and calculate variance of x in each segment separately: f 2 (s,ν) = 1 s s ∑ i=1 (xs,ν(i)− hxs,νi) 2 . (3) In the next step, variance f 2 is averaged over all segments and the qth fluctuation function is calculated: Fq(s) = ( 1 Ms Ms−1 ∑ ν=0  f 2 (s,ν) q/2 )1/q , (4) where q ∈ R. By repeating the above procedure for a number of different scales and values of q, one can observe how the function Fq(s) behaves for different s. It occurs that for fractal signals u(j) the fluctuation function shows a power-law form for all qs and for a broad range of scales s: Fq(s) ∼ s h(q) , (5) where h(q) depends monotonically on q and it is called a generalized Hurst exponent. For a constant h(q) = H, the signal u(j) is monofractal, otherwise it is multifractal. Since its introduction, MFDFA has been widely adopted in financial data analysis. Its application to empirical data from various markets made it possible to show that price returns and intertransaction waiting times reveal multifractal structure63–71 . Recently, it has been shown that the same property can be identified in cryptocurrency market data2,16,52,72–80 . Characteristics of price related fluctuations in Non-Fungible Token (NFT) market 4 0 5 10 15 20 25 C [SOL] in SOL 0 50 100 pfl [SOL] 1 10 100 1000 N 0 5 10 15 20 25 30 V [SOL] 0 2 4 6 8 C [USD] in USD 0 1 2 3 4 5 6 pfl [USD] Jul 2022 Jan 2023 Jul 2023 Time 0 5 10 V [USD] ×104 ×104 ×106 ×103 ×103 a) 0 5 10 15 20 C [SOL] in SOL 0 20 40 60 80 100 pfl [SOL] 1 10 100 1000 N 0 1 2 3 V [SOL] 0 2 4 6 8 C [USD] in USD 0 1 2 3 pfl [USD] Jan 2022 Jul 2022 Jan 2023 Jul 2023 Time 0 5 10 V [USD] ×104 ×104 ×106 ×103 ×103 b) 2 3 4 5 C [SOL] in SOL 0 5 10 15 pfl [SOL] 1 10 100 1000 N 0 2 4 6 8 V [SOL] 3 4 5 C [USD] in USD 0 1 pfl [USD] Jan 2022 Jul 2022 Jan 2023 Jul 2023 Time 0 1 2 3 4 5 V [USD] ×104 ×104 ×106 ×103 ×102 c) 0 5 10 C [SOL] in SOL 0 50 100 150 200 250 pfl [SOL] 1 10 100 1000 N 0 5 10 15 20 V [SOL] 0 2 4 6 C [USD] in USD 0 5 10 15 pfl [USD] Jul 2022 Jan 2023 Jul 2023 Time 0 5 10 15 20 V [USD] ×105 ×105 ×107 ×103 ×103 d) 0 2 4 6 C [SOL] in SOL 0 100 200 300 pfl [SOL] 1 10 100 N 0 5 10 V [SOL] 0 2 4 6 C [USD] in USD 0 1 2 3 pfl [USD] Jan 2022 Jul 2022 Jan 2023 Jul 2023 Time 0 5 10 15 20 25 V [USD] ×105 ×104 ×107 ×104 ×103 e) FIG. 1. Collection characteristics: total capitalization C expressed in SOL and USD, transaction number aggregated hourly N∆t , floor price pfl in SOL and in USD, transaction volume value V∆t in SOL and in USD. All 5 collections are shown: (a) Blocksmith Labs Smthys (BM), (b) Famous Fox Federation (FF), (c) Lifinity Flares (LF), (d) Okay Bears (OKB), and (e) Solana Monkey Business (SM). Characteristics of price related fluctuations in Non-Fungible Token (NFT) market 5 TABLE I. Basic characteristics of the NFT collections considered in this study: the number of circulated tokens K, time series length T, the average inter-transaction time hδti, the average transaction number hN∆ti, the average transaction volume value in SOL hV SOL ∆t i, and the fraction of zero log-returns for total collection capitalization %0c∆t and for floor price %0r∆t , for ∆t = 1 h. Collection Start date K T hδti [s] hN∆ti hV SOL ∆t i %0c∆t %0r∆t BL 03/24/2022 4,444 12,627 1,888 1.23 51.60 0.47 0.56 FF 09/30/2021 7,051 16,826 1,517 1.85 42.94 0.32 0.49 LF 12/25/2021 9,963 14,763 1,926 0.89 4.03 0.71 0.81 OKB 04/26/2022 9,925 11,834 777 2.78 151.10 0.29 0.51 SM 08/16/2021 3,910 17,902 4,102 0.82 48.63 0.62 0.80 Let us begin a presentation of the results by displaying the time series themselves. Fig. 1 show time series representing the analysed observables for each of the 5 NFT collections studied. These are: the total market capitalization C of a collection, the aggregated hourly number of trades N∆t , floor price pfl, and the 1-hour transaction volume value V∆t (please not that, to some approximation, N∆t and δt are variables that are inverse of each other). Given that SOL, like other cryptocurrencies, is one of the speculative assets with high price fluctuations, it is useful to express the capitalization of the collection in both SOL and USD (CSOL, CUSD , respectively, top panels in Fig. 1(a)-(e)). It can clearly be seen that behaviour of the two quantities is fundamentally different. While in the first few months after the debut both CSOL and CUSD increase rapidly over time, which is related to the novelty effect and the token-release-related news, after this initial period the curves representing the two quantities show quite different behaviour. In the case of CSOL , the increase in value of 4 out of 5 collections continues even for about a year after the debut, while for CUSD there was a saturation of the capitalization and then a slow decline. This, of course, was related with the behaviour of the two instruments’ mutual exchange rate: in 2022 there was a downturn in the cryptocurrency market and the value of SOL expressed in USD used to decrease gradually. A different picture emerges in the case of the OK Bears collection, however, where both CSOL and CUSD started to trend downwards after a few initial months. The strong interest in collections just launched is reflected in the number of transactions made per unit time N∆t . For each collection analysed here, this number was many times higher immediately after a collection debut for one or more days than in later periods (upper middle panels in Fig. 1(a)-(e)). After these few days of trading frenzy, traffic on a given collection calms down and then it fluctuates as does the interest in the NFT market as a whole. The number of transactions varies in time, however, with significant amplification of trading frequency at the end of 2021 and Q1 of 2022. Middle panels of Fig. 1(a)-(e) show the evolution of floor price expressed in SOL and USD. In the former case, pfl is more stable in the sense that there is no sharp decline after an initial period of a collection-release-related euphoria. In the latter case, on the other hand, floor price quickly falls to a value several times lower than its maximum. This resembles the behaviour of the total capitalization of individual collections. However, even in the case of floor price expressed in SOL, a strong oscillation around a long-term trend is visible, which is more often horizontal and (less often) rising. The two lowest panels in Fig. 1(a)-(e) show the evolution of hourly transaction volume values in SOL and USD. V∆t experienced four phases in the period under study: in the first half of 2022 and in the period from the end of 2022 to the end of Q1 of 2023, trading activity was relatively high, while in the period from mid 2022 to autumn 2022 and from spring 2023 onwards, the activity decreased markedly. At a glance, one can already see the fluctuation-clustering phenomenon, which resembles a similar effect observed in the other financial markets. We therefore expect that this observable shows a long memory. III. RESULTS We examined the empirical time series presented above for several properties that are observed typically in the other financial markets: fluctuation distributions, autocorrelation functions, and scaling properties. A. Statistical properties of distributions Fig. 2 shows cumulative probability distribution functions P(X > σ), where σ denotes standard deviation, for 8 types of time series: logarithmic increments of total capitalization c SOL ∆t and c USD ∆t (top left and top right panels in Fig. 2(a), respectively), the number of trades per unit time N∆t with ∆t = 1 h (bottom left panels), the inter-transaction times δt (bottom right), floor price returns r SOL ∆t and r USD ∆t (top left and top right panels in Fig. 2(b), respectively) and volume value per unit time V∆t also expressed in SOL and USD (bottom left and bottom right panels in Fig. 2(b), respectively). In each panel, the most plausible model curves for the data sets considered are shown as reference distributions for the respective empirical distributions. These are either distributions with power tails x −γ represented by straight lines (the power laws take a linear form on a double logarithmic scale) or stretched-exponential distributions described by a function f(x) ≈ exp(x β ), where 0 < β < 1 81,82. Depending on a given quantity, the model distributions have different values for the power-law exponent γ and the stretched-exponential function exponent β. For example, for a time series of c SOL ∆t and c USD ∆t , the marked exponential function is of the form P(X > c∆t) ∼ c −3/2 ∆t , while for a time series of V SOL ∆t and V USD ∆t , it is P(X > V∆t) ∼ V −2 for both. These are the only data types for which a power-law behaviour of the empirical distribution tails can be observed; in other cases, no scaling region can be identified. As far as the stretched exponential functions are concerned, the empirical distributions are best described by the exponents β ≈ 0.3 for the total capitalization increments, β ≈ 0.4 for the fluctuation in the number of trades per unit time, β ≈ 0.5 for intertransaction times, β ≈ 0.5 for floor price returns, and β ≈ 0.3 for volume value (Fig. 2). As it can be seen, in neither case Characteristics of price related fluctuations in Non-Fungible Token (NFT) market 6 10-4 10-2 10-2 P(X> σ) 1 10 10-4 10-2 10-2 BL FF LF OKB SM 1 10 σ |c| -3/2 |c| SOL |c| USD ∝exp(|c| 0.3) N δt ∝N −3/2 ∝exp(N 0.4) ∝exp(|c| 0.3) |c| -3/2 ∝exp(δt 0.5) a) 10−4 10−3 10−2 P(X> σ) BL FF LF OKB SM 1 10 10−4 10−3 10−2 1 10 σ ∝exp(-|r| 0.5) |r| SOL |r| USD ∝exp(V 0.3) V SOL V USD ∝exp(V 0.3) ∝V −2 ∝V −2 ∝exp(-|r| 0.5) b) FIG. 2. (a) Probability distribution functions for absolute values of logarithmic increments of collection capitalization expressed in solana |c SOL ∆t | (top left) and US dollar |c USD ∆t | (top right), the number of transactions aggregated hourly N∆t (bottom left), and intertransaction times δt (bottom right). (b) Probability distribution functions for absolute values of floor price returns expressed in SOL |r SOL ∆t | (top left) and USD |r USD ∆t | (top right) and volume value expressed in solana V SOL ∆t (bottom left) and US dollar V USD ∆t (bottom right). The appropriate power law (long-dashed) and stretched-exponential (shortdashed) models are also shown as guides for an eye together with their parameter values γ and β, respectively. can we speak of a normal distribution since the tails of the empirical distributions are leptokurtic. B. Temporal correlations Temporal relationships in time series can involve both linear and non-linear dependencies. The former are quantified in terms of autocorrelation function A(x,∆i) = 1/T ∑ T i=1 [x(i)− hx(i)ii ][x(i+∆i)− hx(i)ii ] σ2 x , (6) where σ 2 x denotes variance of a time series x(i), h·i denotes mean, and ∆i denotes temporal lag that is measured in points of the time series (its clock-time equivalent is τ = ∆i∆t). Fig. 3 shows the autocorrelation functions calculated for each time series and for each of the analysed collections. In all cases, there is a slow decay of A(x,τ), which reflects the existence of long-range autocorrelations. For some time series, it is clear from the graphs that we observe a power-law decay. The lifetime of these long-range autocorrelations is observabledependent – the longest ones are observed for inter-transaction time δt and for the floor price absolute returns |rfl|, reaching approximately 1,000 h (i.e., more than a month). If we now look at Fig. 1, we see that an average cluster lasts for a comparable amount of time, which makes both observations mutually consistent. The strength of autocorrelation varies between the time series and it can be pointed out that the observables for some collections show systematically stronger autocorrelation than for the other ones: these include OK Bears and Blocksmith Lab Smyths. C. Multifractal properties We applied MFDFA to the time series of logarithmic increments of capitalization, floor price returns, transaction volume value aggregated hourly, the number of transactions aggregated hourly, and inter-transaction times for each collection. In any multifractal approach, the fundamental issue is identification of scaling in a plot of the relevant function. In MFDFA it is the fluctuation function given by Eq. (4). We calculated Fq(s) for all 8 types of data for each collection. Let us start from capitalization and floor price return time series. Fig. 4 presents the fluctuation function plots for 3 selected collections: FF, LF, and OKB, which constitute the most characteristic cases. It comes straightforward that there is no time series that would reveal a firm power-law dependence over at least a decade of scales and for all the considered values of q simultaneously. There is only one example of a time series that shows scaling over both positive and negative qs: c USD ∆t for the FF collection; even its counterpart for SOL fails to do so as regards q > 0. However, this time series does not develop a scaling range that would be at least a decade long, which means that fractal structure is present only to a rather limited extent there. It happens, though, that scaling can be seen over a broader scale range but only for q > 0; these are r SOL ∆t and r USD ∆t for the FF collection. As the positive qs allow the algorithm to select medium and large fluctuations, we conclude that they reveal fractality, indeed, while small fluctuations contain only non-stationary noise (because stationary noise could be fractal as well). Such a conclusion is not surprising at all as in many financial markets small orders and the related small variations of observables are associated with noise traders that do not contribute genuinely to the overall dynamics. Apart from this, there are cases where a scaling Characteristics of price related fluctuations in Non-Fungible Token (NFT) market 7 10−2 10−1 A(τ) BL FF LF OKB SM 1 10 102 103 10−2 10−1 1 10 102 103 τ [1h] |c| SOL |c| USD N δt a) 10−2 10−1 A(τ) BL FF LF OKB SM 1 10 102 103 10−2 10−1 1 10 102 103 τ [1h] |r| SOL |r| USD V USD V SOL b) FIG. 3. (a) Autocorrelation function for absolute values of logarithmic increments of collection capitalization expressed in solana |c SOL ∆t | (top left) and US dollar |c USD ∆t | (top right), the number of transactions aggregated hourly N∆t (bottom left), and inter-transaction times δt (bottom right). (b) Autocorrelation function for absolute values of floor price returns expressed in SOL |r SOL ∆t | (top left) and USD |r USD ∆t | (top right) and volume value expressed in solana V SOL ∆t (bottom left) and US dollar V USD ∆t (bottom right). range is too short or not observed at all, so no existence of a uniform fractal structure may be inferred: c SOL ∆t for all the three collections and c USD ∆t for LF and OKB, as well as r SOL ∆t for LF and OKB and r USD ∆t for LF and OKB. The fluctuation functions for the remaining 4 observables: the number of transactions N∆t , inter-transaction times, and transaction volume value in SOL and USD are collected in Fig. 5. There are two time series that show scaling over both positive and negative qs: N∆t for FF and OKB. The time series for which a range of power-law scaling is minimum acceptable are: V SOL ∆t for FF and OKB as well as V USD ∆t and δt for all the three collections. The remaining time series develop either too short scaling range or their fluctuation functions do not scale at all. The nature of the power-law behaviour of Fq(s) for different values of q indicate that the corresponding time series carry some signatures of multifractality. This can be inferred directly from Fig. 6 where singularity spectra f(α) are shown for all the cases in which Fq(s) reveals scaling over at least a decade in Fig. 6. The resulting spectra are typically left-sided which indicates multifractal structure observed for medium and large values of the corresponding time series. Only in two cases, N∆t and c USD ∆t for FF, a right arm of f(α) reflecting the organization of small values of the corresponding observable can be determined. However, this arm is shorter than the left one, which makes the spectra left-side asymmetric. It has to be noted in this context that multifractal organization of certain observables like price returns, inter-transaction times, and transaction volume values is a well-established result in financial data and constitutes one of its stylized facts83–86. The present results obtained for the NFT collections show a less developed multifractality, origin of which we shall attribute to worse market liquidity and poorer data statistics. Somewhat different principles of the NFT market dynamics may also play some role in this connection. The related attributes may however change when this market gains more trader interest in the future. The plots shown in Figs. 4-5 contain two features, which are repetitive across the time series and which ha values of Fq(s) for small scales if q < 0. It comes from the fact that the considered time series change their values less frequently than the assumed sampling frequency ∆t = 1 h, which produces zero-value periods that significantly lower values of the fluctuation function for q < 0 and throw them out of the plots. This can be avoided by appropriately adjusting the minimum scale considered in an analysis if possible. However, in the present case the required cut-offs would be too large for the plots to be even created, so we decided not to do it. The second feature to be pointed out is a broom-like effect for the short scales seen the most clearly for the floor price returns in the LF case (Fig. 4(b)). Such a shape of the fluctuation functions is typical for time series containing outliers if there is no multifractality2,87–90 . IV. CONCLUSIONS Unlike the relatively well-understood cryptocurrency market, the NFT market is still in its early stage of development and does not attract as much research interest. Our work is one of the first to analyze the statistical properties of selected observables characterizing instruments traded on the NFT market. The data consisted of tick-by-tick data and floor price quotes for 5 selected token collections created on the Solana network: Blocksmith Labs Smyths, Famous Fox Federation, Lifinity Flares, Okay Bears, and Solana Monkey Business. From this data, we created for each collection a set of time series representing 7 observables recorded every hour each: logarithmic increments of the total collection capitalization, floor price returns, transaction volume values, the number of Characteristics of price related fluctuations in Non-Fungible Token (NFT) market 8 10−4 10−3 Fq (s) 10 102 10−2 10 102 scale [1h] c SOL c USD r SOL r USD FF a) 10−4 10−3 Fq (s) 10 102 10−2 10 102 scale [1h] c SOL c USD r SOL r USD LF b) 10−4 10−3 Fq (s) 10 102 10−2 10−1 10 102 scale [1h] c SOL c USD r SOL r USD OKB c) FIG. 4. Multifractal analysis of time series representing three NFT collections: (a) FF, (b) LF, and (c) OKB. The respective fluctuation functions Fq(s) with −4 ≤ q ≤ 4 were calculated for the logarithmic increments of collection capitalization expressed in solana c SOL ∆t and US dollar c USD ∆t and floor price returns expressed in the same currencies: r SOL ∆t and r USD ∆t . A range of scales, in which a power-law form of Fq(s) is observed for a range of values of q, is denoted by vertical red dashed lines on each plot. 1 10 Fq (s) 10 102 102 103 104 10 102 scale [1h] δt N V SOL V USD FF a) 0.1 1 10 Fq (s) 10 102 102 103 104 10 102 scale [1h] δt N V SOL V USD LF b) 1 10 102 103 Fq (s) 10 102 102 103 104 105 10 102 scale [1h] δt N V SOL V USD OKB c) FIG. 5. The same as in Fig. 4 but for different quantities: the number of transactions aggregated hourly N∆t , inter-transaction times δt, and transaction volume value aggregated hourly expressed in solana V SOL ∆t and US dollar V USD ∆t . Characteristics of price related fluctuations in Non-Fungible Token (NFT) market 9 -0.2 0 0.2 0.4 0.6 0.8 1 f(α) 0.4 0.6 0.8 -0.2 0 0.2 0.4 0.6 0.8 1 0.4 0.6 0.8 α FF LF OKB 0.4 0.6 α 0.8 -0.2 0 0.2 0.4 0.6 0.8 1 f(α) r USD N V USD c USD δt FIG. 6. Singularity spectra f(α) calculated for time series that reveal scaling in their fluctuation functions Fq(s). In most cases this corresponds to using q > 0. transactions, and inter-transaction times. The values, which are expressed natively in SOL units, were also recalculated and expressed in USD to relate their value to fiat money. The time series we created were examined in terms of probability density distributions, a possible occurrence of autocorrelation, and a potential fractal structure. Probability density distributions turned out to have heavy tails for all the types of time series considered. The behaviour of these tails was modeled with a power function and a stretched exponential function. In the case of the latter, the compliance of the empirical data with the model turned out to be satisfactory for the inter-transaction times (all the collections), the transaction volume value expressed in USD (all the collections) and in SOL (LF and SM), the floor price returns expressed both in USD and SOL (LF), the USD-expressed capitalization increments (BL and FF), and the SOL-expressed capitalization increments (FF). For the SOL-expressed and USD-expressed capitalization increments, a model that was able to reproduce the data for LF relatively well was the power-law model with β ≈ −3/2. In some cases, however, neither of these two models was optimal. These outcomes have to be compared with the corresponding features of the regular financial markets and the cryptocurrency market. On those markets the heavy-tail pdfs are a common property for many quantities like price returns or volume: the tails are the heavier, the shorter is the time scale considered. At the shortest time scales the inverse cubic scaling for price returns is observed which is counted among the financial stylized facts. By taking into consideration how new is the NFT market as compared to the other financial markets and even to the cryptocurrency one, it is noteworthy that the power laws can be observed there for certain data sets. On the other hand, the stretched exponential distribution fits well to the distribution of inter-transaction times, which also occurs in the case of cryptocurrencies. All the tested time series are characterized by long-range correlations, reaching up to two months. This is a much longer period than in the case of other financial instruments, such as stocks or fiat currencies. Such a long correlation duration may be related to the low liquidity of the NFT market. In individual cases, the identified correlations are close to powerlaw. The analysis carried out with MFDFA allowed us for the identification of a fractal structure in the case of the time series of logarithmic increments of capitalization expressed in USD and floor price returns (in both SOL and USD) for FF, the number of transactions for FF and OKB, inter-transaction times and transaction volume values expressed in USD for all the three collections, and the transaction volume values expressed in SOL for FF and OKB. For these fractal time series, the quality of scaling was satisfactory enough that we could identify a trace of multifractality. In all the remaining cases no fractal structure could be found at all. One cannot decide whether this result has to be attributed to the finite-size effects that substantially affect the fluctuation functions calculated with MFDFA or to genuine property of the NFT market dynamics. It requires further studies to resolve this issue. Our analysis has shown that, in some respects, the NFT market somewhat differs from other financial markets, including the cryptocurrency one. In particular, we found neither the inverse cubic scaling of the returns nor consistent, uniform multiscaling of the considered observables. It must be noted in this context, however, that our analysis included only a single sampling frequency of 1 hour, therefore we cannot exclude a possibility that the inverse cubic behavior may be found for some other time scale. We also cannot exclude a possibility that multiscaling could be identified if the studied time series were substantially longer, which would improve statistics. Based on our results, we can conclude that the NFT market is still in its early phase. Regardless of this conclusion, it is also important to stress the limited number of NFT collections examined. It is thus advisable to expand the data set, both on other collections operated by the Solana network and on collections operated by other networks (e.g. Ethereum). This indicates directions of the related future work. ",
		"summary": "Over the last dozen years researchers have had a unique opportunity to observe the emergence and evolution of new financial markets. The cryptocurrency market was created in 2011 and passed through a few stages from infancy, through a stage of being an emergent market with not fully developed features, to a relatively mature stage at present. It has already been a subject of a number of studies which revealed much information about its structure and dynamics. This is not the case of the even younger and less developed non-fungible token (NFT) market, which was established only in 2017. Its short history and low liquidity make the amount of data available not yet spectacularly rich but already sufficient to make rather reliable estimates towards establishing the related analogs of the so-called stylized facts. The present work is among the first that focus on the statistical properties of the NFT market trading.",
		"id": "UUID13"
	},
	{
		"document": "1. INTRODUCTION Speech, as the most natural form of human communication, effectively delivers rich information about a speaker’s emotion, identity, location, or spoken content. Many speech processing algorithms have been developed to extract such information [1–5]. These algorithms are often optimized for clean speech signals, while real-world speech signals are typically contaminated by interfering signals such as noise and irrelevant speakers, which is exemplified as the “cocktail party problem” [6]. Therefore, it is often beneficial to incorporate a pre-processing step to extract the speech signal of interest, a task commonly known as target speech extraction [7]. Target speech extraction algorithms are usually conditioned on an auxiliary reference or cue, to distinguish the target speaker in the case of overlapping speakers. A widely studied case is that of a pre-recorded speech utterance being used as the reference, where the network extracts the speech that sounds similar to the speech signal reference [8–14]. This work was performed while Y. Masuyama was an intern at MERL. A drawback of such an approach is that pre-enrollment of each target speaker is needed, which is cumbersome or even unfeasible in some circumstances. Human attention is known to be multi-modal [15], involving various sensory stimuli that are processed interactively as described by the reentry theory [16]. Notably, studies have shown that watching a speaker significantly improves speech comprehension in a challenging “cocktail party” scenario [17–19]. Motivated by these studies and the robustness of visual cues against acoustic noise, there have been various attempts to condition the target speech algorithm on visual signals. For example, the FaceFilter model explores the face-voice correspondence using a single face image [20], the reentry model explores the speech-lip synchronization using a lip recording [21], and the SEG model explores the speechgesture association using an upper-body recording [22]. Among visual cues, face recordings are generally understood to be the most effective and are the most often used, as visemes provide the exact places of articulation [23–32]. Researchers have explored the use of face recordings in various target speech networks, such as frequency-domain bidirectional long short-term memory (BLSTM) networks [33, 34], time-domain temporal convolutional networks (TCN) [21,26, 35–37], or dual-path recurrent neural networks (DPRNN) [38, 39]. In this work, motivated by the recent success of the timefrequency-domain speaker separation model TF-GridNet, we propose to condition it on face recordings for audio-visual target speech extraction, referring to this model as AV-GridNet. While a target speaker extraction algorithm can work independently to the type of interfering signals, we contend that, the characteristics of speech and noise being very different, it may be more advantageous to individually optimize a model for each interference scenario, as a model separating speech from speech is likely to more heavily rely on the intrinsic structure of speech signals, while a model separating speech from noise is likely to more heavily rely on the differences between the characteristics of speech and noise. In this paper, on top of a universal AV-GridNet that processes the mixture speech signal irrespective of the type of interfering signals, we thus propose a scenario-aware AVGridNet model, SAV-GridNet, that explicitly integrates the different interfering scenarios. SAV-GridNet is a cascaded arXiv:2310.19644v1 [eess.AS] 30 Oct 2023 model that first identifies the type of interfering signals with a classifier model, and then applies a dedicated expert AVGridNet model that is trained specifically for that scenario. To validate the effectiveness of our proposed networks, we participated in the second COG-MHEAR Audio-Visual Speech Enhancement Challenge [40], and achieved state-of-the-art (SOTA) results in terms of objective measures such as perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and scale-invariant signal-to-distortion ratio (SI-SDR), in addition to word intelligibility in a listening test, outperforming the other teams and the baseline [41] by a significant margin. We also perform an extensive analysis of the results under the two scenarios. 2. METHODOLOGY 2.1. Related work: TF-GridNet Our proposed system is built upon the SOTA time-frequencydomain model called TF-GridNet, which has demonstrated promising results in various tasks including speech separation [42] and multi-channel audio-only target speech extraction [43]. TF-GridNet directly estimates the real and imaginary components of the target speech signal from those of the mixture speech. It encodes a speech signal into TF representations of dimension T ×F ×D with a short-time Fourier transform (STFT) followed by a 2-dimensional convolution (Conv2D) and a layer normalization (LN) operation. Then, B repetitions of GridNet blocks are applied to refine the TF representations. Finally, the real and imaginary components of the target speech are obtained by applying 2-dimensional deconvolution to the output of the final GridNet block, followed by an inverse STFT (iSTFT) operation to output the separated speech signals. Each GridNet block consists of three successive modules: 1) An intra-frame spectral module that views the TF representation as T separate sequences of D-dimensional embeddings, each sequence having length F, and applies a BLSTM layer with H units and a 1-dimensional deconvolution layer with kernel size I and stride J; 2) A sub-band temporal module that views the TF representation as F separate sequences of D-dimensional embeddings, each sequence having length T, and performs a similar procedure as in the intra-frame spectral module; 3) A full-band self-attention module that first reshapes the TF representation into a single sequence of length T with F ×D channels, and applies a multi-head selfattention operation with L heads. 2.2. Proposed AV-GridNet Instead of separating all speakers into individual streams, we propose a modified version of TF-GridNet for audio-visual target speaker extraction called AV-GridNet. AV-GridNet is conditioned on the face recording v of the target speaker, and it extracts only the corresponding target speech sˆ from the mixture speech signal x, irrespective of the type of interference signals. The architecture of AV-GridNet is illustrated Extracted Speech Target Speaker's Face Track Images STFT Conv2D+LN Fusion Intra-frame Spectral Module Sub-band Temporal Module Full-band Self-attention Module x B Conv2D+LN iSTFT ResNet 18 Conv3D V-TCN Mixture Speech Clean Speech x R Fig. 1. Our AV-GridNet model extracts the target speech conditioned on the target’s face recording. in Fig. 1, incorporating an additional visual conditioning network to extract visual features V from the face recording v. 2.2.1. Visual conditioning network The visual conditioning network comprises a 3-dimensional convolutional layer (Conv3D), a ResNet 18 layer, and R repetitions of visual temporal convolutional network (V-TCN), as depicted in Fig. 1. The Conv3D and ResNet 18 layers are pre-trained on lip-reading tasks and are kept frozen during the training of AV-GridNet1 . This allows the network to retain the ability to encode viseme movements that synchronize with the phoneme sequence of speech. Additionally, we employ V-TCN layers as an adaptation, similar to [21, 39], to adapt the visual embeddings towards speech extraction. The visual embedding V typically has a lower temporal resolution compared to speech embeddings. To address this mismatch, we linearly interpolate the visual embeddings along the time dimension to match the resolution of the speech embeddings. We fuse the same visual embeddings V using a fusion layer to the start of each GridNet block. Specifically, we concatenate the audio and visual embeddings along the channel dimension, and project them back to the original channel dimension of the audio embeddings before fusion with a linear layer. 2.2.2. Loss function to train the AV-GridNet In time-domain end-to-end speaker extraction network training, the negative SI-SDR loss function [45] has been widely used in most methods. It is formulated as follows: LSI-SDR(s, sˆ) = −20 log10     <s,s> ˆ ∥s∥2 s        sˆ− <s,s> ˆ ∥s∥2 s     . (1) 1The pre-trained visual network can be found at https://github. com/smeetrs/deep_avsr [44] In this work, we adopt the hybrid loss proposed in [41], which reduces the over-suppression error and leads to improved perceptual quality and intelligibility for the extracted speech. The hybrid loss consists of the time-domain SI-SDR loss as shown in Eq. (1), together with a frequency-domain multi-resolution delta spectrum loss2 : Lhybrid(s, sˆ) = LSI-SDR(s, sˆ) + γ 1 M X M m=1 L m freq-∆(s, sˆ), (2) where the delta spectrum loss L m freq-∆ is calculated at M = 3 different resolutions, using the following triplets of parameters for {FFT size, hop size, window length} in samples: {512, 50, 240}, {1024, 120, 600}, and {2048, 240, 1200}. γ is a balancing weight that is set to 1 in this paper. 2.3. Proposed scenario-aware SAV-GridNet 2.3.1. Motivation Speech and noise exhibit very distinct characteristics, and scenarios where one or the other acts as interfering signal of a target speaker may thus require different strategies. Indeed, a model trained to separate speech from speech is likely to heavily rely on the structure of speech, and thus to be different from a model trained to separate speech from noise, which has the opportunity to rely on the intrinsic differences between the characteristics of the two signals to be separated. Therefore, we advocate that a dedicated expert AV-GridNet model that is trained specifically for noise or speech interference may better handle each scenario. To this end, we propose a model, referred to as SAVGridNet, that is aware of the different interference scenarios as depicted in Fig. 2. SAV-GridNet first identifies the type of interfering scenario with a classifier network, and then applies a dedicated expert model that is trained specifically for that scenario. The classifier model and the expert models AV-GridNetn (for noise interference) and AV-GridNets (for speech interference) are trained independently. Note that we consider here scenarios involving either noise or a single speaker as interference because of the particular setting of the COG-MHEAR challenge. A generalized and arguably more realistic setting for practical applications would be to consider noise-only interference on one hand, and one or more speakers with or without background noise on the other. While our proposed classifier-based approach can be readily extended to this setting, with the corresponding expert models, we leave a thorough investigation of the performance of such a system to future work. 2.3.2. Classifier network The classifier network, detailed in the left panel of Fig. 2, accepts both x and v as inputs. Although the task could be performed with only x, it may be beneficial to include the 2Code for the hybrid loss function can be found at https://github. com/zexupan/avse_hybrid_loss [41] Mixture Speech Target Speaker's Face Track Images Classifier network Extracted Speech AV-GridNet AV-GridNet x 2 Conv1D (1/256) [80/40] ReLU & LN AvgPool1D [4/4] AvgPool1D [3/3] V-TCN AvgPool1D [3/3] AdpAvgPool1D Linear (256/1) & Sigmoid TCN stack {2} TCN stack {4} ResNet 18 Conv3D V-TCN x R fusion x 2 Fig. 2. Our scenario-aware SAV-GridNet model is a cascaded model that first classifies the type of interfering signals with a classifier network, and then applies dedicated expert models AV-GridNetn for noise interference or AV-GridNets for speech interference. In the classifier network, the values inside “(/)” represent the input and output feature sizes, the values inside “[/]” represent kernel size and stride, and the value inside “{}” represents the upper-bound of the dilation value in a TCN stack [35]. AvgPool1D and AdpAvgPool1D denote 1D average pooling and 1D adaptive average pooling operations, respectively. visual signals here as it could serve as an anchor point for the target speech. The classifier network design is motivated by the audio-visual SLSyn network in [21], which consists of a visual front-end, a speech front-end, and an audio-visual back-end. It is worth noting that the visual front-end here also consists of Conv3D, ResNet 18, and V-TCN layers. As with the visual conditioning network in Fig. 1, the Conv3D and ResNet 18 layers are pre-trained on lip-reading tasks and are kept frozen during the training of the classifier model. We minimize the following binary cross-entropy loss for the scenario classifier network training: Lbce = −y log(ˆy) − (1 − y) log(1 − yˆ), (3) where y ∈ {0, 1} indicates whether the interfering signal is speech or noise, while yˆ is the predicted probability. We arbitrarily set speech interference to be the negative class and noise interference to be the positive class. 2.3.3. Classifier post-processing If the classifier makes a mistake and the wrong expert model is used, the results may be detrimental as there is a mismatch between training and inference for the AV-GridNet. We empirically find that a model trained only on speech interference generalizes remarkably well on noise interference, but not vice versa. Therefore, we propose two post-processing strategies to mitigate the false-positive cases (i.e., predicting noise while the ground-truth label is speech). For the first post-processing strategy (post-proc1), if the classifier prediction is noise and the criteria in Eq. (4) is met, which indicates that the universal model is more in agreement with the noise expert than with the speech expert, we classify the interference as noise, otherwise as speech: LSI-SDR(ˆs, sˆn) < LSI-SDR(ˆs, sˆs), (4) where sˆn = AV-GridNetn(x, v), (5) sˆs = AV-GridNets(x, v), (6) sˆ = AV-GridNet(x, v). (7) For the second post-processing strategy (post-proc2), if the classifier prediction is noise, and either of the criteria in Eq. (4) or Eq. (8) is met, with this latter criterion indicating that the original mixture is further to the output of the noise expert than to that of the speech expert, we classify the interference as noise, otherwise as speech: LSI-SDR(x, sˆn) > LSI-SDR(x, sˆs) (8) For the samples that are classified as noise interference initially by the classifier but are changed to speech interference by the post-processing, we use the extracted speech from the universal AV-GridNet model. For all other samples, the model indicated by the classifier is used. 3. EXPERIMENTAL SETUP 3.1. Dataset We participated in the second COG-MHEAR Audio-Visual Speech Enhancement Challenge 3 and evaluated our proposed method on its benchmark dataset. The speech dataset is from the Lip Reading Sentences 3 (LRS3) [46], which consists of thousands of spoken sentences from TED videos. The noise datasets are from the Clarity challenge [47], which comprises around 7 hours of domestic noises, the DEMAND dataset [48], which includes recordings of 18 soundscapes that represent over 1 hour of data, and the Deep Noise Suppression (DNS) challenge [49], for which only the noise signals from Freesound [50] are considered. The challenge has two tracks: systems in track 1 can only use the abovementioned provided datasets and unimodal pre-trained models, while systems in track 2 have no limitations in the datasets and pre-trained models used. The training, development, and evaluation sets consist of 34519, 3300, and 2792 scenes respectively. There are two scenarios in total, a target speaker mixed with a competing speaker at random signal-to-noise ratio (SNR) levels that range from −15 dB to +5 dB, or a target speaker mixed with a noise signal at random SNR levels that range from −10 dB to +10 dB. The clean speech signals and scenario labels are only available for the training and development set. The audio signals are sampled at 16 kHz, while the video has a frame rate of 25 per second. The target face tracks are provided for all the samples. 3https://challenge.cogmhear.org/ 3.2. Baselines We use the AV-DPRNN network [39, 41] as our main baseline, as it is currently one of the best-performing audio-visual speech extraction networks. There are three main differences between AV-DPRNN and AV-GridNet: 1) The speech encoder and decoders used by AV-DPRNN are in the time domain, while those of AV-GridNet are in the time-frequency domain; 2) AV-DPRNN is a mask-based method that uses dual-path BLSTM as the extractor, while AV-GridNet directly maps the signals using the GridNet blocks as the extractor; 3) The visual embeddings are only fused at the first repeat of the extractor for AV-DPRNN, while the visual embeddings are fused at every repeat of the extractor for AV-GridNet. We also report results by the official baseline released by the challenge organization. It has a similar architecture to the AV-DPRNN network [39, 41], but with no visual pre-training involved. 3.3. Model and training settings For the baseline AV-DPRNN, the hyperparameter setting follows [39, 41]. For the classifier network in SAV-GridNet, the TCN stack hyperparameter follows [35]. For AV-GridNet, the V-TCN hyperparameter follows [39, 41]. We set D = 48, B = 6, and R = 5. The STFT window size is 256, the hop size is 128, and the square root Hann window is used. A 256-point discrete Fourier transform is applied to extract 129-dimensional complex spectra at each frame. For other hyperparameters in the GridNet block, we set I = 4, J = 1, H = 192, E = 4, and L = 4 [42]. For all model training, we use the Adam optimizer with an initial learning rate of 0.001, the learning rate is halved if the best development loss (BDL) does not improve for 6 consecutive epochs, and the training stops when the BDL does not improve for 20 consecutive epochs. We train the model on 8 GPUs with 48 GB RAM each. To fit the data in the GPU memory during training, the audio clips are truncated to 3 seconds for AV-GridNet, 12 seconds for AV-DPRNN, and 25 seconds for the classifier network. 4. RESULTS We evaluate the speech signals extracted by our proposed networks and the baselines using objective measures PESQ, STOI, and SI-SDR. PESQ measures the perceptual quality of the extracted speech signal and is in the range of −0.5 to 4.5; STOI measures the intelligibility of the extracted speech signal and is in the range of 0 to 1; and SI-SDR measures the signal quality of the extracted speech signal in dB and is unbounded. The higher the better for all three metrics. We use PESQ as our main measure when describing the results, as other measures show similar trends. 4.1. Comparison with baseline and ablation study In Table 1, we present the results of our baseline and proposed models on the development set. For our baseline AV-DPRNN, it is seen that using hybrid loss (Sys. 2) outperforms (Sys. 1) Table 1. Development set results on the 2nd COG-MHEAR Audio-Visual Speech Enhancement Challenge benchmark. Experiments are done for track 1, in which only the provided dataset is used in training. We use the system number (Sys.) to identify different systems. Init. indicates which (if any) other system is used to initialize that system’s parameters (note that the optimizer parameters are always re-initialized). DM stands for dynamic mixing, which involves simulating the mixture utterances on the fly during training using the protocol provided by the challenge organizers. We report performance on the speech+speech scenario, the speech+noise scenario, and overall. The symbol * indicates the model used the oracle scenario labels. Speech+Speech Speech+Noise Overall Sys. Model Init. DM Loss PESQ STOI SI-SDR PESQ STOI SI-SDR PESQ STOI SI-SDR - Noisy - - - 1.17 0.60 −5.0 1.15 0.68 −4.4 1.16 0.64 −4.7 1 AV-DPRNN - ✗ LSI-SDR 1.68 0.84 8.8 1.71 0.82 9.8 1.69 0.83 9.3 2 1 Lhybrid 2.23 0.90 12.6 2.02 0.86 11.4 2.12 0.88 12.0 3 AV-GridNet - ✗ Lhybrid 3.07 0.95 17.3 2.36 0.87 12.8 2.78 0.92 14.6 4 3 ✓ 3.10 0.95 16.7 2.62 0.91 13.9 2.86 0.93 15.3 5 AV-GridNets 3 ✓ Lhybrid 3.23 0.95 17.5 2.56 0.90 13.4 2.89 0.93 15.5 6 AV-GridNetn 1.27 0.61 −4.7 2.68 0.91 14.2 1.98 0.76 4.8 7 SAV-GridNet - ✓ Lhybrid 3.22 0.95 17.4 2.68 0.91 14.2 2.95 0.93 15.8 8 + post-proc1 3.23 0.95 17.5 2.68 0.91 14.2 2.95 0.93 15.8 9 + post-proc2 3.23 0.95 17.5 2.68 0.91 14.2 2.95 0.93 15.8 10 SAV-GridNet* - ✓ Lhybrid 3.23 0.95 17.5 2.68 0.91 14.2 2.95 0.93 15.8 Speech Noise Predict labels Speech Noise Target labels 1623 16 4 1657 (a) System 7 Speech Noise Predict labels Speech Noise Target labels 1635 4 58 1603 (b) System 8 Speech Noise Predict labels Speech Noise Target labels 1635 4 32 1629 (c) System 9 Fig. 3. Confusion matrix of the scenario classification on the development set. by 0.43 on the overall PESQ, showing the effectiveness of the frequency-domain loss on the speech perceptual quality. Our first proposed AV-GridNet (Sys. 3) outperforms AVDPRNN (Sys. 2) by 0.66 for PESQ. With additional dynamic mixing (Sys. 4), the PESQ further improves by 0.08. Our expert models, AV-GridNets and AV-GridNetn, improved PESQ compared to AV-GridNet by 0.13 in the speech+speech scenario and by 0.06 in the speech+noise scenario, respectively. Thanks to adaptively choosing the expert models, our proposed SAV-GridNet (Sys. 7) outperforms the AVGridNet (Sys. 4) regardless of the scenarios. While the postprocessing techniques (Sys. 8 and 9) do not appear to show performance improvements on the averaged metrics, they do reduce outliers, as will be investigated in the next subsection. The performance of our best system is nearly identical to that of SAV-GridNet with the oracle scenario labels (Sys. 10). 4.2. Analysis for speech and noise interfering signals Fig. 3 illustrates the confusion matrix of the scenario classification with and without post-processing. Our scenario classification network achieved accuracy over 99% without postprocessing. Since the false-positive cases (predicting noise when the ground-truth label is speech) severely deteriorate the subsequent target speech extraction performance according to Sys. 6 in Table 1, it is important to reduce the number of false-positive cases, which post-proc1 does successfully. This however increased the false-negative cases to 58, but postproc2 mitigated this increase and performed best overall in terms of target speech extraction. This tendency is also confirmed from the distributions of PESQ shown in Figs. 4–6. SAV-GridNet (Sys. 7, Fig. 5) improved the overall performance from AV-GridNet (Sys. 4, Fig. 4), but it had more outliers in the speech+speech scenario. This is likely because AV-GridNetn was applied to some speech+speech samples due to misclassification. As the post-processing techniques successfully reduced the falsepositive cases, Sys. 9 reduced the number of outliers with low PESQ in the left panel of Fig. 6, while substantially preserving the distribution in the speech+noise scenario. Overall, the number of samples with a PESQ value smaller than 1.5 went down from 62 for Sys. 7 to 54 and 53 for Sys. 8 and 9, respectively, post-proc1 thus reducing the number of such failing samples by 13%, and post-proc2 by 14.5%. Analyzing low performing samples: When informally listening to samples with the lowest objective metrics, we noticed that, while a few samples did have mid-utterance switching between target and interfering speakers in the speech+speech scenario, the main issue was that many of the target speech signals were not very clean, e.g., they contained impulsive disturbances from microphone contact or crowd noises such as cheering and clapping. To quantify objectively the quality of the target speech signals, we used the P808 15.0 12.5 10.0 7.5 5.0 2.5 0.0 2.5 5.0 Input SNR(dB) of speech interferer 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 PESQ 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 Input SNR(dB) of noise interferer 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 PESQ Fig. 4. PESQ of extracted speech signal from system 4 for the speech (left) and noise (right) interfering signals. 15.0 12.5 10.0 7.5 5.0 2.5 0.0 2.5 5.0 Input SNR(dB) of speech interferer 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 PESQ 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 Input SNR(dB) of noise interferer 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 PESQ Fig. 5. PESQ of extracted speech signal from system 7 for the speech (left) and noise (right) interfering signals. 15.0 12.5 10.0 7.5 5.0 2.5 0.0 2.5 5.0 Input SNR(dB) of speech interferer 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 PESQ 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 Input SNR(dB) of noise interferer 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 PESQ Fig. 6. PESQ of extracted speech signal from system 9 for the speech (left) and noise (right) interfering signals. DNSMOS [51] score, a reference-free measure for evaluating overall audio quality. Figure 7 displays the distribution of SI-SDR vs. DNSMOS. We chose SI-SDR over PESQ as the reference metric for this figure because the larger dynamic range makes outliers more visible. We note that all of the lowest SI-SDR samples output by our model have DNSMOS values below 3.0 in Fig. 7, and we found by informal listening that these samples contained noisy target speech. As datasets become larger, using a reference-free speech quality metric could help remove noisy target speech samples. 4.3. Performance on leaderboard In Table 2, we present the performance of our models on the hidden evaluation set, for which the numbers are obtained from the submissions to the leaderboard4 . We can see that our AV-DPRNN baseline (Sys. 2) outperforms the challenge baseline by 0.53 in terms of PESQ, thanks to the combined use of the presented pre-trained models, hybrid loss, and training settings. We also report the results of the top 3 other teams on track 1 in terms of PESQ. Results for track 2 are not reported as they did not improve upon those of track 1. AV-GridNet outperforms AV-DPRNN by 0.65 in terms of 4https://challenge.cogmhear.org/#/results 2.25 2.50 2.75 3.00 3.25 3.50 3.75 4.00 4.25 P808 DNSMOS speech interferer 0 5 10 15 20 25 30 SI-SDR Improvement 2.25 2.50 2.75 3.00 3.25 3.50 3.75 4.00 4.25 P808 DNSMOS noise interferer 10 0 10 20 30 40 SI-SDR Improvement Fig. 7. Development set SI-SDR improvement [dB] vs. P808 DNSMOS for system 9 for speech (left) and noise (right) interfering signals. Many of the lowest performing samples appear to have noisy ground-truth signals based on their low score under the reference-free DNSMOS quality measure. Table 2. Evaluation set results on the second COG-MHEAR Audio-Visual Speech Enhancement Challenge benchmark. We only present the top-3 teams on track 1 out of 8 other teams based on the PESQ ranking. Sys. Model Track PESQ STOI SI-SDR - Noisy - 1.14 0.44 −5.1 Baseline 1.41 0.56 3.7 - Team 1 1 1.61 0.68 8.8 Team 2 1.66 0.68 6.7 Team 3 1.76 0.71 7.7 2 AV-DPRNN 1 1.94 0.73 10.4 4 AV-GridNet 2.59 0.83 13.8 7 SAV-GridNet 2.71 0.84 14.5 8 + post-proc1 2.71 0.84 14.5 9 + post-proc2 2.71 0.84 14.5 PESQ, while SAV-GridNet further outperforms AV-GridNet by 0.12. Similarly to the results obtained on the development set, we cannot see a difference on the averaged metrics from using post-processing with SAV-GridNet (Sys. 8 and 9), but we hope that the number of outliers will again be reduced. This will need to be confirmed when/if the evaluation set is released. Furthermore, in a listening test, we achieved an overall word intelligibility score of 84.54%, compared to 57.56% for the baseline and 80.41% for the next best team. The Fisher’s least significant difference (LSD) was 2.14%, indicating that our model offered statistically significant intelligibility improvements compared to all other systems. 5. CONCLUSION In this work, we explored visually-grounded target speaker extraction based on the TF-GridNet separation architecture. Considering the different characteristics of noise and speech as interfering signals raised by the 2nd COG-MHEAR Audio-Visual Speech Enhancement Challenge, we proposed a scenario-aware model named SAV-GridNet that is capable to apply an expert model to individual scenarios independently. Experimental results show that the scenario-aware model generally improves the quality of the extracted speech, while reducing the number of samples with very low quality",
		"summary": "Target speech extraction aims to extract, based on a given conditioning cue, a target speech signal that is corrupted by interfering sources, such as noise or competing speakers. Building upon the achievements of the state-of-the-art (SOTA) timefrequency speaker separation model TF-GridNet, we propose AV-GridNet, a visual-grounded variant that incorporates the face recording of a target speaker as a conditioning factor during the extraction process. Recognizing the inherent dissimilarities between speech and noise signals as interfering sources, we also propose SAV-GridNet, a scenario-aware model that identifies the type of interfering scenario first and then applies a dedicated expert model trained specifically for that scenario. Our proposed model achieves SOTA results on the second COG-MHEAR Audio-Visual Speech Enhancement Challenge, outperforming other models by a significant margin, objectively and in a listening test. We also perform an extensive analysis of the results under the two scenarios.",
		"id": "UUID14"
	},
	{
		"document": "1. INTRODUCTION While significant advancements in audio-only speech recognition and separation techniques have been witnessed recently, challenges remain in understanding a speech from an individual amidst overlapping sounds. In real-world situations, conversations are often intertwined with other voices or disturbed by a cacophony of noises. Elimination of such disturbances is particularly important in settings like meetings, where one has to focus on the speech of single individual. Humans excel at guiding their attention to a sound source of interest in such environments, naturally de-emphasizing other sounds. On the other hand, when auditory cues contradict visual cues from the speaker’s face, speech sounds are frequently misinterpreted by humans [1], highlighting the importance of visual modality in human’s understanding of spoken communications. Audio-Visual Speech Separation (AVSS) aims to emulate this human capacity, aiming to distinguish each voice from a collective soundscape using the visual information. Beyond enhancing the auditory intelligibility for listeners, this technique can also serve as a pre-processing step for various speech-related tasks, including cascaded speech recognition [2, 3] and speaker diarisation [4]. In consequence, there has been significant advances in the field of audiovisual speech separation, driven by the accessibility of multi-modal datasets and high performance computing. Early works [2, 5] have proposed to combine visual and audio features to distinguish the ∗ These authors contributed equally. Reverse Diffusion Visual Encoder Mixed speech Target speech Speech 1 Speech 2 Fig. 1. Audio-visual speech separation based on diffusion model pipeline. Mixed speech and target speaker’s face crop images are inputs and target speaker’s speech is extracted through a reverse diffusion process. speech of the target speaker in complex and noisy environments. A noteworthy finding in their research is that leveraging the visual modality effectively addresses the label permutation problem, which arises from the challenge of assigning a proper ground truth to the predicted output during training. More recently, VisualVoice [6] utilizes both lip motions and facial attributes (e.g. gender, age, and nationality) as conditions to specify the target speaker. Thus, it is reported that leveraging lip movements is effective for aligning auditory and visual information to extract phonetic information, and incorporating facial attributes aids in distinguishing target speakers using their identity cues. With the advancements in deep learning, there has been successful applications of generative models in AVSS field. Generative AVSS models [7, 8] can produce realistic samples by learning the mapping from latent space to clean speech distribution. Although these approaches have demonstrated successful performance, they face difficulties in generating diverse samples and exact data estimation, frequently producing speech with undesirable artifacts. This indicates the need for generating samples that sound more natural to humans. In response to this, we take advantage of the natural sample generation capabilities of the diffusion model. The diffusion model is known for its potential in generating diverse and natural samples across various domains [9, 10, 11, 12, 13], including the audio-only speech separation [14, 15]. In this work, we propose a diffusion-based AVSS model called AVDiffuSS that reconstructs both natural and intelligible utterances. To effectively incorporate visual information in the separation process, [9] leverages a feature fusion mechanism. However, its fusion strategy based on feature compression is not appropriate in speech domain where time resolution must be preserved. To mitigate this issue, we propose a task-specific feature fusion mechanism arXiv:2310.19581v1 [eess.AS] 30 Oct 2023 � �! �!(�) �� ∥ �!(�) �� Generative Stage � �# Q KV Q KV �#$% ��'� ∥ �$(�) … Predictive Stage Visual Encoder Q KV Q KV �# Mixed speech Target speaker �# … Stage 1 Stage 2 Fig. 2. Model architecture of AVDiffuSS. Face-cropped images of the target speaker are fed to the visual encoder to obtain the visual embedding v. A mixture of two speech signals is transformed into a spectrogram y by STFT, and it goes into two stages: 1) a predictive stage, and 2) a generative stage. Each stage consists of U-net architecture. For the input of the generative stage, the output of first stage Dθ(y) is concatenated with the xT, which is sampled from N (Dθ(y), σ2 I). In the next stage, reverse diffusion is repeated for T steps. At the same time, v is utilized as a key and value and audio feature from U-net is used as a query, in the cross-attention modules of both stages. for visual gudiance. The proposed cross-attention layer does not require any encoder and decoder module, eliminating the risk of performance bottleneck. The layer supports a frequency-domain compression of audio features for reducing memory footprint, without time-domain compression. Our contributions consist of the following: (1) To the best of our knowledge, we are the first to introduce an audio-visual speech separation based on diffusion model, capable of reconstructing both natural and intelligible speech. (2) With the help of the proposed compressing strategy, we successfully mitigate the excessive computational overheads and make our model suitable in speech domain. (3) With various experiments, we demonstrate that the proposed method attains state-of-the-art results on two widely-used benchmark datasets. 2. METHOD As illustrated in Fig. 2, our framework comprises two main stages: the predictive stage and the generative stage. In the predictive stage, the model initially estimates the speech of the target speaker using visual semantics v extracted by the visual encoder. The output of the predictive model, denoted as Dθ(y), is then fed into the generative stage, which employs a diffusion-based model. In this stage, the initial prediction is further enhanced through an iterative denoising process. Note that both stages improve audio-visual alignments by utilizing a task-specific cross-attention module, resulting in the generation of more natural samples. 2.1. Visual Encoder The visual modality plays two pivotal roles for audio-visual speech separation: (1) synchronizing speech with lip movements to capture phonetic details, and (2) identifying the target speaker based on facial attributes, such as gender, age, and nationality. Taking inspiration from a study in active speaker detection [16], a visual encoder capable of both preserving temporal dynamics and incorporating visual cues can be leveraged to achieve aforementioned objectives. We adopt the encoder architecture from [16], which comprises a series of ResNet18 layers and a temporal convolutional network from [17]. On top of those modules, a 1D convolution layer is attached to reduce the channel dimension. The encoder, as a result, outputs framelevel spatio-temporal features. 2.2. Encoder-Decoder Free Conditioning by Cross-Attention To effectively separate the desired speech by exploiting the visual modality, it is essential to maintain the temporal characteristics of both auditory and visual features throughout the fusion process. Based on this, we focus on the cross-attention mechanism, which enables the model to learn the correspondence between sequential information from the two different modalities. Since cross-attention calculates correlations between different modalities by multiplication, it requires heavy computational costs. In response to this, we propose a feature fusion method using cross-attention, eliminating the need for a complex feature compression process involving encoder-decoder architecture. The proposed feature fusion method is conducted in both predictive and generative stages. In each stage, we aim to acquire the correspondence between visual and audio embeddings. As we adopt the U-net architecture as the backbone of both stages, audio embedding can be represented as ea,i ∈ RCi×Ti×Fi . Here, Ci, Ti, and Fi denote the number of channels, frame lengths, and frequency lengths, respectively, in the i-th U-net layer. By applying frequencyaxis pooling to the audio features, we obtain the pooled audio feature denoted as ¯ea,i ∈ R Ci×Ti , which is used as the query, while the visual feature is employed as the key and value. The output of the cross-attention module is repeated Fi times to recover the original shape of the input before averaging across frequencies. Through this process, our model denoises undesired speech while enhancing the voice of the target speaker. 2.3. Audio-Visual Speech Separation with Diffusion Diffusion model. In diffusion process {xτ } T τ=0 , indexed by a continuous time variable τ ∈ [0, T] , data is perturbed with a noise during a forward process. The diffusion model [13, 18] learns to reverse this process to generate a clean data x0 ∼ p0 from a noisy prior xT ∼ pT . Forward process is modeled using a Stochastic Differential Equation (SDE) as follows: dxτ = f(xτ , τ )dτ + g(τ )dw, (1) where w is a Brownian motion, f(xτ , τ ) := γ(y − xτ ) is a drift term following [10, 12] and g(τ ) is a diffusion coefficient. The model learns to solve the reverse-time SDE, given as: dxτ =  f(xτ , τ ) − g(τ ) 2∇xτ log pτ (xτ )  dτ + g(τ )dw, (2) Method A-V Diff PESQ ESTOI SI-SDR DiffSep [14] ✓ 2.2436 0.6291 4.9435 VisualVoice [6] ✓ 1.9708 0.7691 9.6810 AVDiffuSS (Ours) ✓ ✓ 2.5031 0.8122 12.0282 Table 1. Speech separation results on the VoxCeleb2 dataset. For all metrics, higher is better. A-V refers to audio-visual model, and Diff refers to diffusion-based model. where w denotes a Brownian motion for the reverse time flowing from T to 0. By training our generative stage on samples with denoising score matching objective, Gϕ(·) learns to transform a noisy data xT ∼ pT into a clean sample x0 ∼ p0. Training procedure. The architecture of both stages is U-net [19] with cross-attention modules for the integration of audio and visual features [9], as shown in Fig. 2. Audio input y denotes a complexvalued spectrogram of an overlapped speech, and each channel is occupied by the real and imaginary values respectively. In the predictive stage, the model Dθ(·) is trained to directly separate the desired speech x from y. The output of the first stage Dθ(y) is utilized to sample the initial condition xT for the second stage as follows: xT ∼ N (Dθ(y), σ 2 I), (3) where xT can be simply obtained by adding a Gaussian noise to the initial prediction Dθ(y). Dθ(y) is concatenated with xT to construct the input for the generative stage and xT−1 results from the first step through Gϕ(·) as follows: xT−1 = Gϕ(xT ∥ Dθ(y)). (4) xT−1 is again fed to Gϕ, and this process is repeated for T steps. Through this reverse-diffusion process, the target audio prediction x0 could be separated from y. Training objective. For an end-to-end training of predictor Dθ and generator Gϕ, we use a multi-task learning strategy, following [10]. The equations below show the overall training process. The predictor Dθ is trained with L2 loss (Lpred) between the initial prediction Dθ(y) and the ground-truth x, Lpred = E  ∥x − Dθ(y)∥ 2 2  . (5) Our model in the reverse-diffusion stage is trained by utilizing a denoising score matching objective denoted as Ldiff . This is a re-parameterized denoising score matching objective in [20], where t is uniformly sampled within a range from minimal diffusion time τϵ to T. Ldiff = E  ∥sϕ(xτ , y, τ ) + z στ ∥ 2 2  . (6) Two objectives are combined with weight values λ1 and λ2 for balanced training as follows: L = λ1 ∗ Lpred + λ2 ∗ Ldiff . (7) 3. EXPERIMENTS We evaluate our model quantitatively using three established speech evaluation metrics, which are Perceptual Evaluation of Speech Quality (PESQ) [21], Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) [22], Extended Short-Time Objective Intelligibility (ESTOI) [23], and qualitatively through Mean Opinion Score (MOS). Method A-V Diff PESQ ESTOI SI-SDR DiffSep [14] ✓ 2.0625 0.6826 5.3581 VisualVoice [6] ✓ 1.7778 0.7417 7.2270 AVDiffuSS (Ours) ✓ ✓ 2.6803 0.8769 13.6243 Table 2. Speech separation results on the LRS3 dataset. Results are obtained by the models trained on VoxCeleb2 dataset. 3.1. Experimental Setup Datasets. VoxCeleb2 [24] dataset is a widely-used dataset for audiovisual tasks comprising more than 1 million utterances extracted from YouTube videos. This dataset consists of 5,994 identities in the training set, and 118 identities in the test set. Our model is trained on VoxCeleb2 train set, and 10 utterances in the test dataset are randomly chosen for validation. LRS3 [25] dataset is another popular dataset for audio-visual speech recognition and speech separation. The dataset is made up of 4,004 videos for training and validation, and 412 videos for test set, which are from TED and TEDx videos. Implementation details. We utilize NCSN++M for the U-net inside our model and the cross-attention mechanism is adopted from [9]. We follow the details regarding the diffusion process in [10]. The input for the visual encoder is a sequence of face-cropped grayscale images resized to 112 × 112. Our model is updated with Adam optimizer [26] with an exponential moving average of network parameters with a decay of 0.999 [27]. The initial learning rate is initiated to 10−4 . The weight value λ is set to 0.5. We use 4 RTX A5000 GPUs for training with effective batch size 16. We train our network for 15 epochs, which takes approximately 12 days. Comparison methods. We compare our method with two publiclyavailable state-of-the-art speech separation models. DiffSep1 [14] is an audio-only speech separation model extended from SGMSE+ [12], and VisualVoice2 [6] is an audio-visual speech separation model. We train DiffSep on VoxCeleb2 dataset from scratch for 20 epochs for pair comparisons, as this results in approximately the same number of iterations as reported in [14]. We also utilize an official pretrained VisualVoice model and generate a test set using the process introduced in [6]. Note that every model is trained on VoxCeleb2 train set and tested on the test sets of VoxCeleb2 and LRS3. 3.2. Experimental Results Quantitative results. To validate the effectiveness of our methods, we show the experimental results on VoxCeleb2 and LRS3 in Table 1 and Table 2, respectively. In particular, DiffSep exhibits the lowest ESTOI and SI-SDR scores among the three models. This suggests that the utilization of the visual modality enables the models to effectively separate the target speech. Moreover, when compared to VisualVoice, AVDifuSS shows a higher PESQ score. It indicates that our model generates natural-sounding speech. To further simulate one-shot speech separation scenarios, we evaluate every model that is trained with VoxCeleb2 on LRS3 test set in Table 2. This result shows the robustness of our model on a cross-dataset evaluation. It is worth noting that our model is the first to leverage the diffusion mechanism in the task, and yet it significantly outperforms the existing state-of-the-art methods. 1https://github.com/fakufaku/diffusion-separation 2https://github.com/facebookresearch/VisualVoice Method A-V Diff MOS DiffSep [14] ✓ 2.24 ± 0.11 VisualVoice [6] ✓ 2.98 ± 0.10 Ours ✓ ✓ 4.44 ± 0.07 Table 3. MOS comparison with 95% confidence interval computed from the t-distribution. 17 participants rated 20 list of audios randomly selected from the results on VoxCeleb2. Method PESQ ESTOI SI-SDR DiffSep [14] 1.8926 0.5116 0.4439 VisualVoice [6] 1.8063 0.7356 7.8063 Ours 2.1171 0.7566 8.6177 Table 4. Speech separation results tested on the bottom 30% samples sorted by SI-SDR results of VoxCeleb2 dataset. Qualitative results. Generative models often receive low scores using conventional metrics due to the artifacts generated during the generation, even though they sound natural to the human ear. Therefore, we conduct a human evaluation using MOS. 17 participants are each asked to assess 20 pairs of separated outputs on a scale of 1 to 5. We normalize every sample to eliminate the amplitude bias in outputs of each model and the orders of the models are randomly assigned for every pair. Criteria for the evaluation are (1) audio quality relative to the corresponding ground-truth, and (2) degree of separation. As some natural-sounding outcomes involve the other speaker’s speech, it is impossible to evaluate the degree of separation without the ground truth samples. Therefore, the ground truth samples are not included in the MOS evaluation and are provided to the participants as standards for assessing separation capability. As shown in Table 3, MOS for our model is significantly higher compared to previous works. These results demonstrate the ability of our approach to generate samples that sound clear and natural to human hearing, not to mention its intelligibility. 3.3. Discussions Experimental results in difficult cases. In real-world scenarios such as a conversation of two speakers with similar timbre, it is difficult for audio-only speech separation models to accurately distinguish speech of each speaker. Moreover, VisualVoice suffers especially from overlapping speeches of two male speakers with loud background noise due to the difficulty of clearly discriminating lowpitched voice with the babble noise. Thus we show the results from the hardest samples to prove the robustness of our diffusion-based audio-visual approach. Sorted by SI-SDR result of our model, we choose the bottom 30% samples to demonstrate performance of each model in harsh conditions, which is disadvantageous to our method. As shown in Table 4, our model and VisualVoice are both able to isolate the whole intelligible speech even in hard cases by utilizing synchronization cues and facial characteristics. DiffSep cannot identify the target speaker accurately due to the lack of visual information, resulting in lower scores especially on SI-SDR. On the other hand, diffusionbased approaches are able to generate outputs with high naturalness, indicated by PESQ metric. Spectrograms of the separated outputs from each model are Resolutions PESQ ESTOI SI-SDR 32 2.4156 0.8000 11.5506 32, 64 2.4444 0.8028 11.6749 32, 64, 128 (Ours) 2.5031 0.8122 12.0282 Table 5. Ablation results on the VoxCeleb2 dataset depending on the feature resolutions to apply cross-attention choices. Clean DiffSep SI_SDR=10.106 5 Ours SI_SDR= -1.5235 Target 1 Target 2 SI_SDR= 0.2261 SI_SDR=9.373 4 VisualVoice SI_SDR= 6.4575 SI_SDR= 2.6538 Fig. 3. Spectrogram comparison of the outputs of DiffSep [14], VisualVoice [6] our model on the lowest 30% random sample. shown in Fig. 3, including ground truth for a comparison. A pair of speech signals is randomly selected from the bottom 30% results, and the mixture of speech is fed to each model to evaluate the three models. Boxes and circles with identical colors in each row represent regions which should be same with clean spectrogram. Arrows colored in green and blue denote that the non-target speaker’s speech is included in the output of audio-only diffusion model, but not in our results. In VisualVoice, the details of the original speech is ignored, and the speech is over-denoised as shown in the spectrograms. This visualization demonstrates the ability of our model to generate realistic details, not to mention the accurate capturing of the spoken contents. Effectiveness of cross-attention layers. Table 5 shows the results of ablation on the resolution of the U-net features which the crossattention modules are applied to. Starting from 256, the feature resolution of U-net layers are halved for four times from 256 to 32 during the downsampling path, and upsampled to recover the original feature resolution. Among those 8 layers, cross-attention modules are applied on the 6 layers with three smallest resolutions. Increasing the number of cross-attention layers brings a modest improvement in performance, but we do not add cross-attention to every layer in U-net due to memory constraints. 4. CONCLUSION In this work, we present AVDiffuSS, an audio-visual speech separation framework based on the diffusion model. Our approach exploits visual cues to extract the target speaker’s speech accurately, and the diffusion model to produce a highly natural-sounding output. We devise a task-specific feature fusion mechanism for integrating a target speaker’s visual information into the diffusion-based speech separation framework. The proposed model demonstrates state-of-the-art performance for audio-visual speech separation in terms of both naturalness and intelligibility.",
		"summary": "The objective of this work is to extract target speaker’s voice from a mixture of voices using visual cues. Existing works on audio-visual speech separation have demonstrated their performance with promising intelligibility, but maintaining naturalness remains a challenge. To address this issue, we propose AVDiffuSS, an audio-visual speech separation model based on a diffusion mechanism known for its capability in generating natural samples. For an effective fusion of the two modalities for diffusion, we also propose a cross-attention-based feature fusion mechanism. This mechanism is specifically tailored for the speech domain to integrate the phonetic information from audiovisual correspondence in speech generation. In this way, the fusion process maintains the high temporal resolution of the features, without excessive computational requirements. We demonstrate that the proposed framework achieves state-of-the-art results on two benchmarks, including VoxCeleb2 and LRS3, producing speech with notably better naturalness. ",
		"id": "UUID15"
	},
	{
		"document": "1. INTRODUCTION As immersive audio gains popularity and content creators utilize the capabilities of these modern formats, the same content is increasingly available for different multichannel listening setups, such as 5.1 and 7.1.4 [1, 2]. Legacy content is also being transferred to these new formats. To facilitate this, the different versions have to be created manually, or via an automatic process of re-mixing or rendering. While downmixing and rendering are mainly simple linear processes, upmixers have to reconstruct unknown signal components with more complicated algorithms. In the case of blind upmixing especially, the original artistic intent is not guaranteed to be preserved despite the sophistication of such systems, making them less than ideal. Non-blind upmixing on the other hand can be viewed as being synonymous to audio coding. Multichannel audio transmission and storage typically utilizes parametric coding, e.g. preserving the channel covariance structure is effective [1, 2]. Unfortunately, lossy multichannel coding is difficult to optimize perceptually. Perceptual differences are difficult to judge due to their multidimensional nature [3]. While many codecs make good arguments that they achieve transparency after some bitrate, this cannot be fully guaranteed for all possible content due to the limitations of subjective testing. Lossless coding is a viable option to address concerns related to both blind upmixing and parametric coding. As transmission capacities have constantly improved, the need for extremely low bitrates is no longer as major a concern as before. Furthermore, prejudices against lossy coding and transmission have increased as both consumers and content creators become more educated. There is a need for exact control of the immersive audio reproduction process in all situations. Compared to traditional lossless coding, it is not as clear how to most effectively deal with immersive audio. More sophisticated prediction models have been proposed in [4, 5]. However, in the case of multichannel audio, these models have not resulted in major benefits, but rather in small improvements. As we see in Sec. 2, a system using a very simple baseline model of coding channels separately is able to get very close to real codec performance. This paper proposes methods to move toward more comprehensive handling of immersive lossless audio. Our main contribution is to propose a hypothetical audio system, where several (two or more) different mixes are stored simultaneously for the same content. Such a system would be possible to implement by packing the differently coded bitstreams in the same file container etc. Alternatively, the downmix(es) can be assumed to be available a priori at the decoder. However, having mixes in the same container can very effectively control the artistic intent for the content as well, regardless of compression. We construct a controlled experiment showing the attainable benefits of using hierarchical reconstruction of the different formats. The method exploits correlations between the different content versions and reconstructs more elaborate presentations based on the lower-level multichannel formats and a non-trivial signal model. This would then result in decreased storage requirements for the audio format described above. In the use case of streaming a single format at a time, we additionally propose a method combining short-term prediction, SVD, and Rice coding which performs considerably better than the realistic baselines for 5.0 audio, at a computational encoding cost. Details of the methods are presented in Sec. 2. The experiment results, and discussion about their implications follow in Sec. 3 and Sec. 4. 2. METHODS 2.1. Core lossless coding engine Current real-life lossless codecs share many compression principles, techniques, as well as overall performance. FLAC (Free Lossless Audio Codec) [6] is an open source, widelyused codec implementation whose fundamental algorithms are based on earlier Shorten [7]. In this paper, we apply the official FLAC implementation as a reference, and replicate its performance with a simple baseline implementation. The general principle applied in FLAC and our method can be described with the following simplified signal model (time and channel indices omitted): s = f(s ′ ) + e, (1) where the original signal s is represented by a predictor function f() operating on a predictor source signal s ′ , which is often related to s. The prediction residual is noted with e. In standard lossless coding, f() is often a linear predictor (LPC) operated on short frames, giving the signal model: s(t) = Xp k=1 βks(t − k) + e(t). (2) For each time sample of the frame, p (i.e. prediction order) past samples are used as linear combination to model it. The coefficients β = [β1...βp] are typically solved for minimizing the frame residual MSE ||e||2 2 . In real codecs, search procedures and frame signaling are often used to find the best p, as well as sometimes the type of predictor solution (e.g. LPC or a standard template [7]). In this paper, we omit this optimization step, and rather aim to isolate the affect of the predictor source s ′ . The compression ratio achieved by lossless audio codecs is predominantly effected by the entropy coding of the prediction residual e. FLAC and Shorten simply assume that the residual distribution is geometric, with symmetrical focus around value zero. These will have some Golomb code [8] as an optimal prefix code. Rice coding is a subset of such codes where the Golomb parameter is power of 2 for computational efficiency. Without loss of data type generality, the length of the Rice code for an integer number n can be obtained by Algorithm 1. It calculates the codeword length in bits as a function of the Rice parameter r. Bitwise operations are utilized to perform sign-folding to non-negative integers, so that larger absolute values get longer codewords. Overflow checks are omitted here but may be implemented with min operations. The optimal Rice parameter r can be estimated from the signal [7], but we used a brute-force search (e.g. r ∈ [0...20]) and selected the code that minimizes the sum amount of bits in the analysis frame of each channel. Despite the simplicity of this baseline signal model, it already accounts for much of the performance of the current Algorithm 1 Rice code length l for integer n, given r Require: r ∈ [0, 1...] n ← int32(n) x ← n ≪ 1 y ← n ≫ 31 z ← uint32(x|y) ≫ r l ← 1 + r + z where ≪ indicates left-shift, ≫ right-shift, and | XOR operation, all bitwise. real-life lossless codecs (see 3.3). Some further tools, such as efficient handling of silent frames and signal runs is not considered here, but can certainly improve results for sparse material. Replacing Rice coding with arithmetic coding [9], or hybrid entropy coding [4] gains typically few percent in compression efficiency. We also experimented with an additional long-term predictor that tries to find the best matching segment to the current frame from the full history of the signal [4], but did not include it in our models. 2.2. Multichannel modeling With multichannel, or immersive audio, the question becomes whether correlations between channels can be exploited. In the case that s has more than one channel, one can use model 2 for each channel c separately; each sample sc(t) only is predicted from the past samples of that same channel. The number of model parameters is then pC, where C is the number of channels. In contrast, we also construct a vanilla baseline for a multichannel predictor in order to test the hypothesisthat there are easily exploitable correlations between the channels: sc(t) = X C c=1 Xp k=1 βc,ksc(t − k) + ec(t). (3) In effect, the current sample of each channel is predicted using all other channels’ samples looking p timesteps in the past. This increases the amount of model parameters to pC2 . Another possibility for exploiting the correlations between the channels is to utilize a transform with desirable properties. For example, common technique is to use PCA or SVD to find a linear projection of maximal energy compaction, and orthogonality of the transformed components [10, 11]. To our knowledge, this technique has not been well investigated for lossless multichannel audio coding, and it is only approximated with heuristic mid-side channel pairs etc. The SVD projection method of [11] was utilized here. To avoid large values, we found that applying the projection to the prediction residual e and not to the original signal is preferable. It should be notes that while computationally complex at the encoder, the more crucial decoding cost of such transform is only increased by a single matrix multiplication. 2.3. Hierarchical reconstruction from downmix The main contribution of this paper is to suggest a multichannel audio signal model, which when optimized, can be used for efficient prediction in the context of hierarchical reconstruction. Assume that the decoder has available many mixes of the same content, either from the same container, or otherwise. Decoding is traditionally done first on the lowest mix in the hierarchy (aka ”downmix”, typically the mix with the least amount of channels). It is then used to predict the next mix (aka ”upmix”) with the signal model. The whole process can be repeated by using the decoded upmix as the new downmix for the next iteration. We test adding simple additional predictors that utilize the downmix to the previous single-, and multichannel models of (2) and (3): s(t) = Xp k=1 βks(t − k) +X D d=1 γdsd(t) + ec(t), (4) sc(t) = X C c=1 Xp k=1 βc,ksc(t − k) +X D d=1 γdsd(t) + ec(t), (5) where sd indicates channel d of the downmix, and γd the corresponding prediction parameter. It can be seen that these models only utilize the most current sample of the downmix, in addition to predicting from the past of the upmix as in traditional models. We found this worked the best for our tests, as compared to more elaborate utilization of the downmix. Also, the addition of such downmix prediction only introduces D∗C more model parameters. Also important for the hierarchical models is the optimizerselection, as discussed in Sec. 2.4. 2.4. Model optimization Traditionally, lossless coding predictors have been optimized with Levinson recursion [12, 13]. These methods achieve computational efficiency by assuming Toeplitz systems. The Toeplitz assumption however limits the type of predictors that are possible: all solved predictor parameters must originate from a time series of consecutive samples. Another alternative used by e.g. [4] is to use several cascaded Toeplitz models whose parameters are not optimized globally. In matrix form, the minimization becomes: argmin α ||s − S ′α||2 2 (6) where prediction source S ′ is a Toeplitz matrix with different lags of source signal s ′ as columns. In case of standard singlechannel model 2 of order p: α = [β1, . . . , βp]. (7) In contrast, we use well-established solvers for linear systems that are not limited to be Toeplitz, namely the GELSD algorithm available in LAPACK [14]. Despite being slower computationally, it allows optimizing all model parameters globally when using the complicated models of 3 and 5, and include arbitrary columns to the source signal matrix S ′ . When the predictor is based on the model of 5, we have: α = [β1,1, . . . , βc,p, γ1, . . . , γd], (8) for prediction order p, c upmix channels, and d downmix channels, respectively. To enable comparison, GELSD is used for all prediction models. Computational efficiency refinements are largely left for future work. We however utilize Tikhonov regularization [15] in all solvers, except the single-channel baseline 2, by solving for smaller (covariance) matrices, and adding a diagonal component δI: argmin α ||S ′T s − (S ′T S ′ + δI)α||2 2 . (9) As is typical, the 16-bit integer input signals are transformed into double precision float in the range [−1, 1] for optimization computations, For simplicity, all solved model parameters are quantized as 16-bit floats prior to the residual calculation and Rice coding. Mirroring the datatype changes and rounding operations in the decoder ensures lossless reconstruction. 3. EXPERIMENTS 3.1. Dataset We tested the methods for 100 songs that had been mixed and mastered specifically to the 5.1. format. The content included mainly pop/rock, and classical genres from various different performers. In our view, such an ad-hoc dataset represents a general, realistic situation, and the exact content or music style is not a determining factor for the overall performance. All material was utilized with 16 bit depth and 44.1 kHz sample rate in the tests. LFE channel is omitted in the dataset; we only use the 5.0- channels (L, R, C, Ls, Rs) of the mixes. Preliminary experiments indicated that including LFE in the prediction would not help, and thus sending it with a single-channel predictor like (2) would just add the same amount of bitrate for all the methods in the comparisons. Furthermore, LFE channel coding may benefit from advanced silence handling, which was not the focus of this paper. We utilize the ITU standard downmix [16] from 5.0 to 2.0 stereo in order to show the benefit of hierarchical reconstruction in the typical situation where the downmix is correlated to the upmix. Of course, this is an artificial situation; in real life this downmix could be obtained at the decoder without sending it, by applying the known linear operation of [16]. However, we believe the results also indicate that there is a benefit when using an artistic downmix, especially if the processing in mixing consists of linear operations such as panning. This assumption may break down in rarer cases of strong nonlinearities or uncorrelated mixes. It should also be emphasized that we are not in this paper addressing object audio, but channel-based material. The former aims to be agnostic to the rendering setup by sending panning information per object, and thus can in principle account for the upmixing blindly. 3.2. Systems tested The tested methods are listed in Table 1. The models used for prediction discussed in Sec. 2 had their parameters optimized with the Tikhonov regularized (δ = 1e−4) GELSD solver for (9). For the basic single-channel model of (2), GESLD was used to optimize (6) in order to compare this baseline against FLAC with similar solver criterion. We used FLAC with the default parameters, as experimenting with other options resulted in little difference. The prediction order for all models implemented (as well as the default FLAC LPC max order) was p = 8. Unlike in real-life coders, we used constant p for each frame of 4096 samples. The only hyperparameter signaled per frame was the Rice code parameter per channel. SVD projection of [11] prior to residual coding was also applied selectively. In addition to quantizing the residual, the prediction and transform parameters were counted towards the bitrate of each method as discussed in Sec. 2.4. As mentioned in Sec. 2.4, MPEG-ALS includes a more involved multichannel prediction models. The reasons for not comparing against it here are the lack of availability of MPEG-ALS software, and the related fact that FLAC is more widely adopted. See Sec. 3.3 for further discussion. 3.3. Results Table 1 shows the average compression ratios for the 100 songs tested. Ratio per file was calculated as the total number of bits divided by original number of bits of the 16-bit representation. Even though the hierarchical systems rely on sending both 2.0 and 5.0 mixes simultaneously, the 5.0 upmix compression ratio is more interesting for evaluating the model effect. Since the 2.0-mix does not use hierarchical prediction, nor was the use of complex non-hierarchical models found to benefit, it can be sent with established stereo coding, such as FLAC in this experiment. This merely adds the same constant rate for all tested methods when sending both 2.0 and 5.0. It can be seen that in comparison to FLAC with the same database, the use of the baseline single-channel prediction model of (2) gives close to identical performance. The vanilla multichannel model ((3)) does not give notable gains. However, when combined with subsequent residual SVD Upmix compression method Total Upmix FLAC .545 .540 Single-channel, (2) .546 .540 Multi-channel, (3) .544 .538 Multi-channel, (3) + SVD .521 .505 *Single-channel + dmx, (4) + SVD .527 .515 *Multi-channel + dmx, (5) .487 .458 *Multi-channel + dmx, (5) + SVD .387 .318 Table 1. Average compression ratios of various models (smaller is better). Middle column gives total ratio for sending both mixes (2.0 and 5.0), right column shows the effect for upmix (5.0) alone. Methods indicated with * need the downmix available for decoding the upmix. projection, the compression is improved. For the hierarchical methods requiring the presence of the downmix, and using it in prediction, the single-channel method of (4) seems to not work well. The real benefit of using the downmix emerges when using the multichannel model of (5), especially when combined with the SVD projection. This implies that global parameter optimization can be an important factor for the success of complex signal model predictors. Despite a direct comparison against real codecs not being the priority, it should be noted that compression ratios better than ours (or the present FLAC result) were reported for MPEG-ALS with a 5.1 test set [4]. However, the content in [4] may have been sparser and more dynamic, with less surround- and center channel utilization (material being older), and the LFE channel being included. Most importantly, the MPEG-ALS use of cascaded multichannel prediction did not have as significant a relative benefit compared to non-multichannel baseline, as the utilization of the downmix prediction, or SVD projection in this paper. Rather, it was comparable to the difference between our single- and multichannel baseline models, (2) and (3). 4. CONCLUSION The work presents improved methods for the lossless compression of multichannel audio, both with an upmix alone and when the upmix is packed with a downmix, at the cost of computational complexity. Results show approximately a 30% improvement in the compression ratio, over FLAC, when both the downmix and upmix are to be joinly encoded. A 10% gain in compression ratio is achieved over FLAC, by utilizing a combination of multichannel prediction, SVD, and Rice coding, when sending 5.0-content alone. The proposed approaches could yield significant gains for data server storage and transmission of multichannel audio data. Further implementing frame-based method switching, silence handing, and other typical codec features may improve results for specific content.",
		"summary": "In this paper, techniques for improving multichannel lossless coding are examined. A method is proposed for the simultaneous coding of two or more different renderings (mixes) of the same content. The signal model uses both past samples of the upmix, and the current time samples of downmix samples to predict the upmix. Model parameters are optimized via a general linear solver, and the prediction residual is Rice coded. Additionally, the use of an SVD projection prior to residual coding is proposed. A comparison is made against various baselines, including FLAC. The proposed methods show improved compression ratios for the storage and transmission of immersive audio. ",
		"id": "UUID16"
	},
	{
		"document": "1. Introduction Deep learning research has fueled many recent advancements toward solving the automatic speech recognition (ASR) task. The end-to-end (E2E) ASR [1, 2, 3] predicts the textual output from the time-frequency input by a deep stack of convolutional neural networks (CNN) [4], recurrent neural networks (RNN) [5], or attention layers [6]. The large modeling capacity of the E2E ASR model helps learn a direct mapping from the input to the output sequence effectively, as shown in many works [7, 8]. While large models are powerful to achieve impressive performance [9] given a sizeable training set, they tend to memorize examples and become overly confident with incorrect predictions [10, 11]. For low-resource scenarios, overfitting becomes an issue [12, 13] with other challenges like diverse acoustic variations [14, 15] and language mismatch [16, 17]. Data augmentation is one effective way to expand the training data and make models generalize [18, 19]. Developed techniques for ASR create multiple views of the original speech [20] by applying vocal tract length normalization [21], reverberation [22], and tempo variations [20]. Advanced methods synthesize speech directly using the state-of-the-art text-to-speech [23] and voice conversion [24] models, which is shown beneficial for low-resource distant talks [25]. Other methods like SpecAugment [26] randomly crops and modifies the input spectrogram like images along both time and frequency dimensions. Feature mixup [10, 11] is another angle to create artificial examples by exploring the input space through interpolation, where a mixup refers to the convex combination of two training features. One recent work of ASR studies the mixup between melspectrograms of two utterances and trains the E2E model to predict both reference texts from the mixed feature [27]. Since the hidden representation space of an ASR model can encode information (e.g. phoneme, word, and semantics) more abstract than the acoustic features at the input [28, 29], we reason performing the mixup of hidden representations is beneficial. As shown in the previous study [11], the mixup performed at deep layers of a model has regularization effects on the representations. It reduces variations in the dimensions that encode redundant information and also smooths the classification boundaries among representations, which alleviates over-confident predictions for adversarial or ambiguous input. For E2E speech recognition, we hypothesize such regularization would improve the overall learning as the speech input contains many variations caused by low-dimensional factors such as content, speakers, and channels [30]. In this study, we propose a data augmentation method for low-resource ASR based on representation mixup, named MixRep. The contribution of this work is as follows, 1. A data augmentation strategy using the mixup of hidden representations for low-resource speech recognition 1 2. Highlight of the complementary regularization on both time and frequency (feature) dimensions for mixup methods 3. Investigation of other techniques, e.g. SpecAugment [26] and MixSpeech [27], and their comparison to MixRep 2. Related Work The concept of input mixup [10] has been successfully applied to classification tasks because the labels are one-hot and easy for interpolation, e.g. pictures [31], acoustic scenes [32], speakers [33], etc. For ASR acoustic model training [34], the mixup is conducted for the HMM state labels aligned to the speech input. For tasks with label sequences of different lengths, the mixup of training losses is used instead, e.g. for the E2E model training in speech recognition [27] or machine translation [35]. The Manifold Mixup [11] extends input mixup to the hidden representations of a deep neural network, which is the focus of our study. For speech input, this has only been previously studied for sound classification [36] and one recent work on speech translation [37], where the latter applies mixup to representations from two modalities and does not consider a mixup of target sequences. Unlike previous work, we investigate the application of Manifold Mixup to train an E2E ASR model. We intend to learn the behavior of different layers, so we do not search layer combinations as extensively as done in [36]. Our approach is similar to the MixSpeech [27] method but extends it and explores the combination of techniques. 1https://github.com/jiamin1013/mixrep-espnet arXiv:2310.18450v1 [eess.AS] 27 Oct 2023 3. Method In this section, we first review the mixup [11] concept. We then explain the MixSpeech method [27] that applies mixup to E2E ASR. Finally, we describe our proposed method which extends the speech mixup to the hidden representation, and mention its regularization effect on the feature dimension. 3.1. Manifold Mixup The Manifold Mixup [11] is a generalized version of the input mixup [10] that allows representation ouput from any layer of a neural network model to be linearly interpolated (i.e. mixup). For an arbitrary K-layers model, we denote fn,k(·) the underlying function that processes data from the n-th layer input to the k-th layer output, where n = 0 is the model input and f0,0(·) is the identity function. Suppose a supervised learning task has input features X and one-hot labels Y , the Manifold Mixup trains the model by mixing up the hidden representations and labels, Rk = λ ∗ f0,k(Xi) + (1 − λ) ∗ f0,k(Xj ), (1) Ymix = λ ∗ Yi + (1 − λ) ∗ Yj , (2) Lmix = L(fk,K(Rk), Ymix), (3) where λ ∈ [0, 1] ∼ Beta(α, α) with α ∈ (0, ∞) and i and j denote two training examples. The interpolation results in a new training example represented by the hidden dimensions of the model, thus it is an effective data augmentation method. We note the input mixup [10] becomes a special case of the Manifold Mixup [11] when n and k are both 0. 3.2. MixSpeech: Input Mixup MixSpeech [27] is a data augmentation method developed for E2E ASR training based on the input mixup [10]. For a pair of utterances, this method mixes up acoustic features of these utterances in the frequency dimensions frame-by-frame. Because speech input and text output have different lengths with the alignment unknown, mixing two word labels at the same position does not correspond to a simultaneous time when both words are spoken. So, the MixSpeech interpolates the losses of recognizing each textual label sequence instead. 3.3. MixRep: Hidden Representation Mixup We propose MixRep to create artificial examples during training by mixing hidden representations of an E2E ASR model, inspired by the previous methods [11, 27]. Reusing Rk defined in Equation 1, MixRep interpolates sampled utterances i and j frame-by-frame by their respective output from the k-th layer of a model. For the textual label sequences Y , MixRep trains the model to optimize the following loss, LmixRep = λ ∗ L(fk,K(Rk), Yi) + (1 − λ) ∗ L(fk,K(Rk), Yj ), (4) where k is drawn uniformly from a set of eligible layers S on each forward pass. When k = 0, since the hidden representations are mel-spectrograms from the input, MixRep naturally extends the MixSpeech [27] method. We present the detailed steps of our proposed method in Algorithm 1. One key aspect of the mixup methods [10, 11] is their regularization benefits on the feature dimension, aside from data augmentation. By making the interpolation weight in the mixup of features and that of the reference labels match, the method constructs a linear association between the input and output Algorithm 1 Hidden Representation Mixup (MixRep) 1: Given a subset S ∈ {0, 1, . . . , K}, a beta coefficient α, a pre-processing function m(·) 2: procedure MIXUP(x, y, λ) 3: get batchSize from x 4: indArr ← shuffle list [0, 1, . . . , batchSize − 1] 5: x ← λ ∗ x + (1 − λ) ∗ x[indArr, :] // interpolation 6: y˜ ← y[indArr, :] 7: return x, y˜ 8: end procedure 9: for each batch do 10: λ ∼ Beta(α, α) // sample an interpolation weight 11: k ∼ Uniform(S) // sample a layer index 12: x ← batch 13: for (index, layer) in layers do 14: if index = k then 15: x, y˜ ← MIXUP(x, y, λ) 16: end if 17: if index = 0 then 18: x ← m(x) // for masked-based preprocessing 19: end if 20: x ← layer.forward(x) 21: end for 22: backward loss ← λ ∗ L(x, y) + (1 − λ) ∗ L(x, y˜) 23: end for space of the neural network [10]. For Manifold Mixup [11], the linearity is constructed for the hidden representation space. This has shown to regularize the feature dimensions of the hidden representations by capturing salient low-dimensional variations and enforcing smooth classification boundaries for predictions made on the representations. Because MixRep regularizes the representation space but speech contains both time and frequency information, we propose the following two configurations of the MixRep method: • Basic: does not apply any regularization along the time axis of the input, similar to [27] • Time enhanced: applies regularization along the time axis of the input (e.g. time masking or warping, etc.). To explore the Time enhanced approach, we investigate applying regularization to the input (line 18 of Algorithm 1). For deep layers of the model (a large k), the representation encodes much information due to a large receptive field. Masking representations at a deep layer then impacts performance since the masked content can be hardly recovered by the limited modeling capacity which follows. In order to recognize the missing content from masking, applying time regularization to the input is effective for helping the following attention-based layers to learn strong representation that captures meaning than fine details from the input. We consider it is crucial for MixRep since a good hidden representation space needs to be established. 4. Experimental Setup To examine the effectiveness of MixRep, we conduct experiments on ASR benchmarks that evaluate speech from reading newspapers or conversations over the telephone. For the Conformer architecture illustrated in section 4.2, we mix representations from the output of an encoder layer (i.e. after the final LayerNorm [38]) and use the original positional encoding without mixup. We establish SpecAugment [26] as our baselines, which randomly and partially masks out time and frequency content from the input. By mixing the input acoustic feature, we recreate the MixSpeech [27] method. For fair comparisons, we test both the Basic and Time enhanced configurations of these methods in our experiments. We then apply the best configuration to mix representations and compare the performance of MixRep to the SpecAugment baseline and the effective MixSpeech. 4.1. Datasets The Wall Street Journal (WSJ) [39] and Switchboard (SWB) [40] datasets are investigated in our study. The WSJ dataset includes read speech with transcripts drawn from the newspaper. The data is partitioned into 81 hours of training speech (si284), 1 hour for development (dev93), and 0.7 hour for evaluation (eval92). The SWB dataset contains spontaneous speech from two sides of a conversation over the telephone line. To simulate a low-resource setup, we randomly sample the training data into two subsets totaling 40 hours and 80 hours. We use the single-fold train split without any speed or noise perturbation. We use the eval’2000 (LDC2002S09) dataset as evaluation for SWB, where there are Switchboard (swb) and Callhome (chm) parts that are unseen from the SWB training/validation set. 4.2. E2E ASR model For ASR experiments, we follow recipes provided in the ESPnet toolkit [41] to train an E2E ASR model for each dataset, which is further referred to as the Default setup. Our models use the listen, attend, and spell (LAS) architecture [1] that include the Conformer encoder [38] and the Transformer [42] decoder. We extract 80 mel-filterbanks and 3-dimensional pitch features. The input is then passed through an optional SpecAugment [26], followed by 2D-CNNs with a downsampling factor of 4. The SpecAugment uses time warping with a window size of 5, two frequency masks with F = 30, and two time masks with T = 40, unless otherwise stated. The encoder has 12 layers. The decoder has 6 layers and connects to a softmax layer followed by the cross-entropy (CE) loss. The model is trained jointly by Ljoint = α ∗ Lctc + (1 − α) ∗ Lce [43], where α is set to 0.3 in our study. The label smoothing weight is 0.1. The model dimension is 256. The attention modules have 4 attention heads and 2048 linear units with a dropout p = 0.1. We use the warmup learning rate scheduler for all datasets. The learning rate of WSJ peaks at 0.005 after 30k steps and that of SWB peaks at 0.006 after 25k steps. We use character as output to train the WSJ model and byte-pair-encoding (bpe) with 2000 subword units 2 for the SWB model. The number of elements in a batch is 2.5M for WSJ and 10M for SWB. The gradients accumulation is 6 times. We use a CNN kernel size of 15 for WSJ and 31 for SWB. The WSJ is trained for 150 epochs and 300 epochs for SWB. Both experiments finish in 1 day using two or four 2080Ti GPUs. 4.3. Parameters of MixRep We use the beta distribution with a coefficient α = 2 for all experiments using MixRep. This corresponds to a convex-shaped probability distribution with mean equals 0.5 (i.e. E[λ] = 0.5) and about half of the probability mass (56%) falls between 0.3 and 0.7. Following MixSpeech [27], we also use τ = 0.15 for WSJ (means 15% data of a batch uses the mixup), but we find τ = 0.45 to be more suitable for SWB. Since searching all subsets of the layers in the ASR encoder is infeasible (i.e. 2The bpe model is obtained from texts in full SWB training 2 12 = 4096 combinations), we employ the following heuristic: we first apply MixRep to every single layer of the ASR encoder and gather its performance; we then test the set S containing the best-performing layer and the input layer. We report every single-layer performance in section 5.4. 5. Results 5.1. Baselines and Previous Methods Because the ESPnet default setting includes the SpecAugment, we expect it to be the best and make it the baseline. To make a fair comparison to the Time enhanced configuration, we investigate turning off frequency masking for SpecAugment. The original MixSpeech is applied to the Transformer model, so we recreate their method for the Conformer model. The results of these systems are illustrated in Table 1. Table 1: WER of baselines and previous methods. T and F refer to SpecAugment regularization along the time and frequency fimension, respectively. Default setup is explained in Section 4. Dataset Model T F With LM (%) No LM (%) dev eval dev eval WSJ Transformer Espnet [41] ✓ ✓ 7.4 4.9 - - MixSpeech [27] ✗ ✗ - 4.7 - - Conformer Default ✓ ✓ 7.1 4.7 11.2 8.9 Default ✓ ✗ 6.2 4.3 10.4 7.7 + MixSpeech (Ours) ✗ ✗ 6.8 4.5 10.7 8.4 + MixSpeech (Ours) ✓ ✗ 6.3 4.2 9.8 7.5 SWB 40hr Conformer Default ✓ ✓ - - 21.3 34.1 Default ✓ ✗ - - 18.5 31.6 + MixSpeech (Ours) ✗ ✗ - - 20.7 33.0 + MixSpeech (Ours) ✓ ✗ - - 18.9 30.6 SWB 80hr Conformer Default ✓ ✓ - - 13.5 23.3 Default ✓ ✗ - - 13.2 23.3 + MixSpeech (Ours) ✗ ✗ - - 14.7 25.2 + MixSpeech (Ours) ✓ ✗ - - 13.0 22.6 From Table 1, we can observe the frequency content from the input is critical for low-resource setups. Comparing the SpecAugment configurations within the default setups, turning off frequency masking improves performance overall. This shows less significantly in the SWB 80hr setup (the model still improves on the in-domain set, but stagnates on the out-ofdomain one). Comparing our MixSpeech setups, we observe the benefit of regularization on the time axis for the mixup. There is at least 7% relative improvement on the evaluation sets across all datasets, which verifies our hypothesis on the benefits of regularization on the time axis for mixup-based methods (see Section 3.3). Finally, we turn off frequency masking in baselines and use Time enhanced configuration for MixRep. 5.2. Read English speech We compare MixRep to the best baseline and the input mixup for read English ASR. The results of MixRep applied at each layer are displayed in Figure 1. The experimental results are illustrated in Table 2. From Figure 1, we observe mixing up in the deep layers (layer 7 to 10) gives good improvements over the baseline. This finding somewhat corresponds to the previous study [28], which finds middle to deep layers of a CNN-RNN E2E ASR model Table 2: WER on the WSJ corpus of proposed MixRep method. S denotes the set of layers to be selected from (see Section 3.3). Model With LM (%) No LM (%) dev93 eval92 dev93 eval92 Conformer SpecAug. baseline 6.2 4.3 10.4 7.7 + MixRep S = {0} 6.3 4.2 9.8 7.5 + MixRep S = {9} 6.1 4.1 9.4 7.2 + MixRep S = {0, 9} 6.0 4.2 9.8 7.5 trained on LibriSpeech contain more phonetic information than the early to middle layers. We hypothesize that certain layers of the E2E ASR model encode information similar to the output textual space, thus applying MixRep helps enforce this association by the linear relationship imposed. Figure 1: Per-layer improvement of MixRep compared to the SpecAugment baseline on the WSJ corpus. We observe a superior performance using MixRep from the results presented in Table 3. Mixing up the 9-th layer representations outperforms the SpecAugment baseline by +6.5% relative and the input mixup by +4% on the evaluation set. When decoding with the LM, the improvement is diminished slightly, suggesting the benefits of the mixup may come from learning more linguistic knowledge in the encoder representations. 5.3. Spontaneous telephony speech We compare MixRep to other regularization methods for spontaneous telephony ASR. The results of MixRep applied at each layer are displayed in Figure 2. The experimental results are illustrated in Table 3. Figure 2: Per-layer improvement of MixRep compared to the SpecAugment baseline on the eval2000 using 40 hours of SWB. From Figure 2, we observe MixRep achieves significant and consistent gains over the SpecAugment baseline on the 40 hours SWB, which proves MixRep to be an effective method for lowTable 3: WER on the eval’2000 using 40- and 80 hours training data subsets from the SWB corpus of proposed MixRep method. Train Data Model With LM (%) No LM (%) swb chm swb chm SWB 40hr Conformer SpecAug. baseline 16.8 29.6 18.5 31.6 + MixRep S = {0} 17.1 28.4 18.9 30.6 + MixRep S = {5} 16.1 29.1 17.6 30.9 + MixRep S = {0, 5} 16.3 27.7 17.7 29.5 SWB 80hr SpecAug. baseline 12.0 21.8 13.2 23.3 + MixRep S = {0} 12.1 21.1 13.0 22.6 + MixRep S = {0, 5} 11.9 21.3 12.8 22.8 + MixRep S = {0, 9} 11.8 21.2 12.8 22.5 resource training. Moreover, layer 5, being the strongest performance on average, improves over the input mixup at the 0-th layer. Compared to Figure 1, we notice stronger improvements obtained by mixing up early to middle layer for the spontaneous telephony speech. Moreover, we spot a similar downward trend from layer 8 to layer 12, suggesting {8} or {9} can be a safe choice for the hyperparameter S. For the SWB 40hr dataset in Table 3, we verify applying MixRep to multiple layers can achieve better performance than a single layer. Mixing up both the 0-th layer and 5-th layer representations outperforms the SpecAugment baseline by a +6.6% relative on the Callhome set, suggesting complementary learning behavior upon regularizing multiple layers for ASR. This is similar to the previous finding for sound classification [36]. For the SWB 80hr dataset in Table 3, we observe the impact of training data size. The MixRep S = {0, 5} configuration leads the baseline by a +2.1% relative after the training data is doubled. This verifies the data augmentation aspect of MixRep, but also shows the limitation of performance gain when the training data becomes sufficient. On the other hand, using the set S = {0, 9} outperforms S = {0, 5}, which indicates the heuristic to select the optimal set S is not optimal and is open for future work. 6. Conclusions In conclusion, we presented MixRep in this paper, a method to create artificial examples by interpolating hidden representations for E2E ASR training. We proposed an enhanced strategy for mixup-based methods, where a regularization along the time axis at the input is added. This is shown to be complementary to the feature regularization effect of the mixup for ASR. By experimenting on both read and spontaneous telephony styles of speech, we showed a significant and consistent improvement of MixRep over other regularization techniques such as SpecAugment and MixSpeech for low-resource ASR. We discussed the impact of training data size and the heuristic for searching the optimal set of eligible layers, which opens up future work.",
		"summary": "In this paper, we present MixRep, a simple and effective data augmentation strategy based on mixup for low-resource ASR. MixRep interpolates the feature dimensions of hidden representations in the neural network that can be applied to both the acoustic feature input and the output of each layer, which generalizes the previous MixSpeech method. Further, we propose to combine the mixup with a regularization along the time axis of the input, which is shown as complementary. We apply MixRep to a Conformer encoder of an E2E LAS architecture trained with a joint CTC loss. We experiment on the WSJ dataset and subsets of the SWB dataset, covering reading and telephony conversational speech. Experimental results show that MixRep consistently outperforms other regularization methods for lowresource ASR. Compared to a strong SpecAugment baseline, MixRep achieves a +6.5% and a +6.7% relative WER reduction on the eval92 set and the Callhome part of the eval’2000 set. Index Terms: End-to-end Speech Recognition, Low-resource, Mixup, Hidden Representations, Data Augmentation ",
		"id": "UUID17"
	},
	{
		"document": "I. INTRODUCTION Speech enhancement (SE) is a challenging task in the field of voice communication since it aims to recover enhanced speech from speech interference with background noise in life. Many voice communication systems may be influenced by background noise, including automatic speech recognition, vehicles, and multi-party conferencing equipment [1]. In response to this issue, a variety of speech enhancement algorithms have been proposed to somewhat minimize noise interference. To address issues such as performance degradation or the challenge of modeling realistic and complicated circumstances, these speech enhancement methods still need to be improved. Recently, deep neural networks (DNNs) have demonstrated potential for speech enhancement. DNNs for speech enhancement is a crucial front-end approach that takes noisy speech as an input and creates an enhanced speech output for better speech quality and intelligibility. In general, current methods for speech enhancement can be classified into two main categories based on the type of model input. The first category utilizes raw time-domain waveforms, while the latter relies on time-frequency (TF) domain representations. Time-domain methods generally use an end-to-end model to directly estimate and output a clean waveform by taking audio data in the time domain as raw waveform input [2]. For TF domain methods, they first apply a short-time Fourier transform (STFT) algorithm to transform the input mixed signal into a complexvalued spectrogram, which is subsequently decomposed into its magnitude and phase components. Conventional TF domain techniques utilize the magnitude as a training target while ignoring the phase. Estimated audio signals are then reconstructed via the inverse short-time Fourier transform (ISTFT). However, the frameworks of the time domain are unable to accurately capture speech phonetics in the frequency domain due to the absence of direct frequency representation [3]. Besides, most TF-domain methods only take magnitude as input to real-valued parametric models while ignoring complex-valued phases due to the difficulty in estimation, which may limit performance. In this paper, we investigate the potential of fully end-toend speech enhancement systems that combine the fragmented benefits of time-domain and TF-domain audio representations, which specialize in tackling specific types of noise. We design a novel speech enhancement transformer framework that involves two speech signal processing modules: a proposed variant of Swin-Unet, named Deep Complex Swin-Unet (DCSUnet), in the time-frequency domain, and an improved dualpath transformer network (DPTnet) in the time-domain. We also introduce a novel cross-domain loss function combining both time- and time-frequency domain losses. Our contributions are as follows: • We propose a novel complex transformer architecture, deep complex Swin-Unet, which combines the advantages of both complex-valued networks and Swin-Net, achieving state-of-the-art performance. We extend the transformer network operator to the complex domain so that it can efficiently model the correlation between the elements of real and imaginary parts of the complex sequence features for a potentially richer representation. • We improve DPTnet by adding memory-compressed attention and propose a hybrid network that combines complex Swin-Net with improved DPTnet named DCHT. The model has a new loss function and performs better than other state-of-the-art audio denoising algorithms on arXiv:2310.19602v1 [cs.SD] 30 Oct 2023 the Voice Bank+DEMAND dataset and the BirdSoundsDenoising dataset. • Our model utilizes time- and TF-domain representation as input to exploit multi-scale features and effectively extract both time-domain and TF-domain information. The model considers the noise phase information from DCSUnet and extracts local and global audio information from improved DPTnet to exploit the new research techniques for speech enhancement. II. RELATED WORK Time-frequency Domain Speech Enhancement. Most speech enhancement techniques operate in the time-frequency domain. The complex-valued phase has mostly been ignored, and the majority of methods only estimate the speech magnitude spectrum with reused noisy phase data [4]. Recently, applying complex spectrograms with complex-valued neural network blocks has become popular because it offers a rich representation. Deep complex U-Net was proposed by incorporating advanced U-Net structures and complex-valued blocks to deal with complex-valued spectrograms [5]. Particularly, the Deep Complex Convolution Recurrent Network (DCCRN) was designed to simulate complex-valued operations using complex networks [6]. Complex-valued transformers have also been designed for speech enhancement. Tan et al. [7] presented a complex transformer module with sparse attention to lowSNR speech enhancement tasks. Zhang and Li [8] converted the audio denoising problem into an image generation task and proposed a complex image generation SwinTransformer network for audio denoising. Time Domain Speech Enhancement. The time-domain speech enhancement approach is a method that operates directly on the time-domain mixture speech waveform to predict the clean speech waveform [9]. Dario et al. [10] proposed an end-to-end learning method for speech denoising based on Wavenet. Baby and Verhulst [11] introduced a conditional generative adversarial network with a gradient penalty for improved speech enhancement performance. With the popularity of attention mechanisms, Kong et al. [12] presented CleanUNet for speech enhancement, which utilized an encoder-decoder architecture combined with self-attention blocks to refine its bottleneck representations. Transformer can now process input audio sequences in parallel, much like natural language processing. Yu et al. [13] proposed a speech enhancement transformer (SETransformer) that takes advantage of Long Short Term Memory (LSTM) and multihead attention mechanisms to improve speech quality. Hybrid Domain Speech Enhancement. Recently, there has been a trend where speech enhancement models combine time and TF domains. Kim et al. [14] proposed a two-steam neural network with both TF domain and time-domain branches to separate music stems. Using a bi-U-Net structure, Alexandre et al. [15] refined the Demucs architecture and built an endto-end hybrid source separation model blending time and TF domains to improve the quality of music source separation. Furthermore, Simon et al. [16] introduced a transformer architecture to model the hybrid transform Demucs network for Music Source Separation (MSS) tasks based on Hybrid Demucs. III. METHOD We first introduce hybrid transformer models, followed by describing deep complex Swin-Unet and improved DPTNet, respectively. Before getting into details, we assume that the mixture of a speech signal y(t) is the linear sum of the clean speech signal x(t) and the noise speech signal ε(t), so the noisy speech y(t) can be expressed as: y(t) = x(t) + ε(t), (1) A sequence of a mixture signal and a clean signal is defined as Y = {yi} N i=1 and X = {xi} N i=1, where N is the total number of speech signals. Typically, each of the corresponding time-frequency (k, f) noise reduction operates in the timefrequency domain [17]: Yk,f = Xk,f + ϵk,f , (2) where Yk,f , Xk,f , ϵk,f is the STFT representation of the time domain signal y(t), x(t), ε(t) and k, f are the time frame index and frequency bins index. In polar coordinates, (2) becomes: |Yk,f |e iθYk,f = |Xk,f |e θXk,f + |Nk,f |e iθϵk,f , (3) where | · | denotes the magnitude response and θ denotes the phase response. The imaginary unit is represented by i. A complex-valued convolutional filter is defined as W = A + iB with real-valued matrices A and B, which represent the real and imaginary part of a complex convolution kernel, respectively. At the same time, we can get complex output from the complex convolution operation on complex vector h = x+iy with W: W ×h = (A×x−B×y)+i(B×x+A×y). A. Hybrid Transformer Model In this section, we introduce the overall structure of the DCHT model. The overall progress of the proposed model is presented in Figure. 1. The proposed model, which extends transformer network architecture with multi-domain representation, comprises two SE approaches: the complex Swin-Unet and improved DPTnet. The complex Swin-Unet module consists of an encoder, a bottleneck, a decoder, and skip connections. The basic unit of Swin-Unet is the Swin Transformer Block. The proposed complex Swin transformer block is a refined transformer architecture applied in the TF domain. The output of the spectral branch is transformed into a waveform using ISTFT before summed with the output of the temporal branch, giving the actual prediction of the model. The improved DPTnet introduces direct context awareness in the speech sequences modeling by combining memory compression attention, which is efficient for long speech sequence modeling. It learns the order information of the speech sequences without positional encoding by incorporating gated recurrent units (GRU). The 1-D signal from the waveform branch and the 2-D signal from the spectral branch are treated simultaneously. With these modifications, we performed the experiments in Section IV. B. Complex Swin-Unet Module The architecture of the original Swin-Unet is based on the Swin Transformer, which exhibits outstanding performance on computer vision tasks. The modifications made to the original Swin transformer are as follows: The convolutional layers of the Swin transformer blocks are replaced by complex convolutional layers. Complex normalization, complex dropout, and complex linear layers are also implemented to the transformer. To accept complex input and output features, we modified real-value Layernormalization (each token is independently normalized) and real-value softmax into complex Layernormalization and complex softmax. For the activation function, we modified the real-value GeLU into the complexvalue GeLU. We use the encoder to extract the hierarchical feature representations from the transformed patch tokens using complex Swin transformer blocks and patch merging layers. The symmetric decoder reshapes each stage feature map and restores the resolution of the feature maps to the input resolution. Skip connections are used to combine multiscale information from the encoder and context features from the decoder to improve the reconstruction of the denoised images. In practice, the target has to be estimated. Choosing an appropriate training target is crucial for supervised learning since it is directly related to the underlying computational target. The target magnitude spectrum of clean speech is a commonly used training target in typical mapping-based approaches. We consider the complex short-time spectrogram of the noisy speech Y , the noise N, and the clean speech X, obtained via the discrete Fourier transform of windowed frames of the raw signals. The estimated speech spectrogram Yˆ k,f is computed by multiplying the estimated mask Mˆ k,f to the input spectrogram Xk,f as Eq. (4). More formally, we need to train a prediction model F to predict the mask, and the output is Mˆ k,f = F(Xk,f ). When the mask connection is not applied, the network directly outputs the estimated complex spectrum, i.e., Yˆ k,f = Fk,f . Yˆ k,f = Mˆ k,f · Xk,f = |Mˆ k,f | · |Xk,f | · e i(θMˆ k,f +θXk,f ) . (4) By applying an additional bounding process, the proposed complex-valued mask Mˆ k,f is expressed as follows: Mˆ k,f = |Mˆ k,f | · e iθMk,f = Mˆ mag k,f · Mˆ phase k,f , Mˆ mag k,f = tanh(|Fk,f |), Mˆ phase k,f = Fk,f /|Fk,f |. (5) Complex Swin Transformer Block The complex Swin transformer block (CSTB) is responsible for feature representation learning. Unlike the conventional multi-head selfattention-based transformer, the Swin Transformer [18] has a hierarchical architecture whose representation is computed using shifted-windows multi-head self-attention. In Figure. 2, each CSTB mainly consists of two successive units: the complex window multi-head self-attention (complex W-MSA) unit and the complex shifted-window multi-head self-attention (complex SW-MSA) unit. Each complex LayerNorm (complex LN) is implemented before the complex MSA module, and each complex 2-layer MLP with complex non-linearity GELU and the remaining connections are implemented before and after each module. On the basis of the shifted window partitioning mechanism, the whole sequential complex Swin transformer blocks are computed as: Yˆ L = complex W − MSA(cLN(Y L−1 )) + Y L−1 , Y L = complex MLP(cLN(Yˆ L )) + Yˆ L , Yˆ L+1 = complex SW − MSA(cLN(Y L )) + Y L , Y L+1 = complex MLP(cLN(Yˆ L+1)) + Yˆ L+1 . (6) C. Improve Dual-path Transformer module In this section, we use the improve Dual-path transformer neural network to train the raw speech waveform input [19]. This network is suggested for processing long-distance speech sequences. It is inspired by the transformer’s ability to interpret sequences and the dual-path network’s ability to gather contextual data [20]. DPTnet consists of an encoder, a DPTnet module (DPTM), a mask module, and a decoder, as shown in Figure 1. DPTM is composed of four stacked dual-path transformer blocks to efficiently extract local and global information. Due to its few trainable parameters, the model complexity of DPTnet is significantly lower. Dual-Path Transformer Block. In the DPTnet extraction module, we substitute the Dual-Path Transformer Block (DPTB) for the traditional transformer block. As shown in Figure 3(a), DPTB is between the encoder and decoder, which has a local transformer and a global transformer to extract local and global contextual information from long-range speech sequences. The input is a 3-D tensor([C, N, F′ ]), and in order to process local information in parallel, the local transformer is first applied to individual chunks. This process works on the input tensor’s final dimension F ′ . Subsequently, the global transformer is employed to combine the data from the local transformer’s output in order to ascertain global dependency, which is applied to the tensor’s dimension N. For the DPTB structure, the transformer only uses the encoder part since the input mixtures and output-enhanced sequences have the same length in speech denoising. Since the speech sequence is time-dependent, the transformer structure in DPTB removes the positional encoding part. As shown in Figure 3(b), the first fully connected layers of feed-forward and linear normalization are replaced with a GRU layer and layer normalization, respectively. The procedures are defined as follows: zˆi = LayerNorm(MSA(zi−1) + zi−1), zi = LayerNorm(ˆzi + ReLU(GRU(ˆzi)W + b)). (7) D. Cross-domain Loss Function In this study, our composite model integrates T-F and time streams to extract a set of complementary audio features. Therefore, our loss function consists of time-domain loss Fig. 1. A overall progress of our proposed hybrid transformer architecture for speech enhancement. Fig. 2. The architecture of Complex Swin Transformer block module. (a) The Improve Dual-path transformer block. (b) Transformer architecture Fig. 3. (a) The Improve Dual-path transformer block module; (b) Transformer architecture. and TF-domain loss to fully utilize both feature information. Inspired by the L1 norm of a complex number in [21], we take the STFT of the audio and use L1 loss over the L1 norm of the STFT coefficients. The frequency-domain loss is given by: lossL1,T F (y, yˆ) = 1 T F T X−1 t=0 F X−1 t=0 [(|yr(t, f)| − |yˆr(t, f)|) + (|yi(t, f)| − |yˆi(t, f)|)], (8) where y and yˆ denote the spectrum of the clean speech and the spectrum of the enhanced speech, respectively. r and i are the real and imaginary parts of the complex spectrogram. T and F are the number of frames and the number of frequency bins, respectively. The time-domain loss is based on the energy-conserving loss function proposed, which simultaneously considers clean speech and noise signals. The time-domain loss is defined as follows: lossL1,T (y, y, x, n ˆ ) = ∥y − yˆ∥1 + ∥n − nˆ∥1, (9) where y and yˆ are the samples of the clean speech and the enhanced speech, respectively. nˆ = x − yˆ represents the estimated noise and ∥ · ∥ denotes L1 norm. To properly balance the contribution of these two loss terms and address the scale insensitivity problem, we weigh (α) each term proportionally to the energy of each speech. The final form of the loss function is as follows: lossT otal(y, yˆ) = αlossL1,T F + (1 − α)lossL1,T . (10) IV. EXPERIMENTS A. Dataset VCTK+DEMAND dataset. We validate the effectiveness of our proposed model on a small-scale standard speech dataset from [22]. In this widely used noisy speech database, clean speech datasets are selected from the Voice Bank Corpus [23], including a training set of 11,572 utterances from 28 speakers and a test set of 872 utterances from 2 speakers. BirdSoundsDenoising. We also train our proposed model on BirdSoundsDenoising. This dataset replaces the usual artificially added noise with natural noises, including wind, waterfalls, rain, etc. In particular, the dataset contains 14,120 audios from one second to fifteen seconds and is a large-scale dataset of bird sounds collected, containing 10,000/1,400/2,720 in training, validation, and testing datasets, respectively [24]. B. Implementation details Our model is an end-to-end trainable model without any pretrained networks and implemented by PyTorch with a single NVIDIA GTX 3060 GPU. All the utterances are resampled to 16 kHz. Each frame has a size of 512 samples (32ms) with an overlap of 256 samples (16ms). Within a batch, the smaller utterances are zero-padded to match the size of the largest utterance. We select the best model using the validation dataset. TABLE I COMPARISON RESULTS ON THE VOICEBANK-DEMAND DATASET. “−” MEANS NOT APPLICABLE. Methods Domain PESQ STOI CSIG CBAK COVL CP-GAN [25] T 2.64 0.942 3.93 3.33 3.28 PGGAN [26] T 2.81 0.944 3.99 3.59 3.36 DCCRGAN [27] TF 2.82 0.949 4.01 3.48 3.40 S-DCCRN [28] TF 2.84 0.940 4.03 2.97 3.43 DCU-Net [5] TF 2.93 0.930 4.10 3.77 3.52 PHASEN [29] TF 2.99 − 4.18 3.45 3.50 MetricGAN+ [30] TF 3.15 0.927 4.14 3.12 3.52 TSTNN [19] T 2.96 0.950 4.33 3.53 3.67 MANNER [31] T 3.21 0.950 4.53 3.65 3.91 DCHT T+TF 3.41 0.954 4.78 3.82 4.22 TABLE II RESULTS COMPARISONS OF DIFFERENT METHODS (F1, IoU, AND Dice SCORES ARE MULTIPLIED BY 100. “−” MEANS NOT APPLICABLE. Networks Validation Test F1 IoU Dice SDR F1 IoU Dice SDR U 2 -Net [33] 60.8 45.2 60.6 7.85 60.2 44.8 59.9 7.70 MTU-NeT [34] 69.1 56.5 69.0 8.17 68.3 55.7 68.3 7.96 Segmenter [35] 72.6 59.6 72.5 9.24 70.8 57.7 70.7 8.52 U-Net [36] 75.7 64.3 75.7 9.44 74.4 62.9 74.4 8.92 SegNet [37] 77.5 66.9 77.5 9.55 76.1 65.3 76.2 9.43 DVAD [24] 82.6 73.5 82.6 10.33 81.6 72.3 81.6 9.96 DCHT - - - 10.49 - - - 10.43 At the training stage, we train our model for 100 epochs and optimize it using the Adam algorithm. We use gradient clipping with a maximum of L2-norm of 5 to avoid gradient explosion [19]. For learning rate, we use the dynamic strategies during the training stage [32]. C. Evaluation metrics We evaluate the proposed speech enhancement model on the VCTK+DEMAND dataset using several objective metrics: PESQ, STOI, CSIG, CBAK, and COVL, for overall quality evaluation. A structure similarity is also used. For evaluation on the Birdsoundsdenoising dataset, we apply signal-todistortion ratio (SDR) [24] to evaluate our model. Fig. 4. Results comparison on VCTK+DEMAND dataset. Raw audio is the original mixture of audio. Ground Truth is the clean audio. TABLE III ABLATION RESULTS. Model Metric ComplexSwinUnet-only 60.8 Improve DPTNET-only 69.1 Full Model 72.6 D. Results We compare our proposed model with several state-of-theart baseline models. The results on the BirdSoundsDenoising dataset are shown in Table II [38], with the best results in each metric highlighted in boldface. The second to fifth columns are the results of validation, and the latter results are of testing. The results demonstrate that our model outperforms other state-of-the-art methods in terms of SDR. Since these metrics are employed for the audio image segmentation task, the results of F1, IoU, and Dice are ignored [39]. We also conduct extra experiments on the VCTK+DEMAND dataset. As shown in Table I, the proposed method is compared with other methods in terms of PESQ, STOL, CSIG, CBAK, COVL, and SSIM. The results indicate that our proposed model outperforms other DNNsbased models and achieves state-of-the-art performance in metrics. The comparisons of raw bird audio, ground truth labeled clean audio, and the estimation audio from several models are displayed in Figure 4. Our model produces results more similar to the labeled clean signal. E. Ablation Studies Experimental results in the previous subsection show that our method improves SE performance. To further verify the effectiveness of our proposed model and show the importance of each component of our model, we perform an ablation study by gradually replacing the components in our model. We designed three experiments labeled the DPTnet-only model, the DCSUnet-only model, and the full model. Table III shows ablation results for the DPTnet-only model and the CSwinUnet-only model, comparing to the full model. Note that the model performance degrades significantly without the CSwin-Unet module. V. CONCLUSION In this paper, we propose a deep complex hybrid transformer for speech enhancement. The DCHT model combines two types of methods: the deep complex Swin-Unet transformer and the dual-path transformer neural network, performing in parallel to exploit a complementary set of features for speech enhancement. The deep complex Swin-Unet transformer improves the Swin-Unet networks for complex-valued spectrum modeling. Similarly, the improved DPTnet applies memorycompress attention to reduce memory usage. The proposed DCHT model is evaluated and compared with several current mainstream deep learning-based SE methods using different datasets. The experimental results show that our model can better remove noise in speech and show significant improvements in evaluation metrics.",
		"summary": "Most of the current deep learning-based approaches for speech enhancement only operate in the spectrogram or waveform domain. Although a cross-domain transformer combining waveform- and spectrogram-domain inputs has been proposed, its performance can be further improved. In this paper, we present a novel deep complex hybrid transformer that integrates both spectrogram and waveform domains approaches to improve the performance of speech enhancement. The proposed model consists of two parts: a complex Swin-Unet in the spectrogram domain and a dual-path transformer network (DPTnet) in the waveform domain. We first construct a complex SwinUnet network in the spectrogram domain and perform speech enhancement in the complex audio spectrum. We then introduce improved DPT by adding memory-compressed attention. Our model is capable of learning multi-domain features to reduce existing noise on different domains in a complementary way. The experimental results on the BirdSoundsDenoising dataset and the VCTK+DEMAND dataset indicate that our method can achieve better performance compared to state-of-the-art methods.",
		"id": "UUID18"
	},
	{
		"document": "I. INTRODUCTION Speech signals are inevitably accompanied by various types of background noise in daily environments, such as automatic speech recognition systems, hearing aids, vehicles and mobile phones, aircraft cockpits, and multi-party conferencing devices [1]. In the field of speech communication, constructing efficient models to eliminate background noise is still a difficult challenge. The goal of speech enhancement learning is to find a transformation that makes clean speech readily available from the original audio. Recent achievements in transfer learning from extensive generative language models serve as a major source of inspiration for our work. There are two main challenges: (1) audio signals are continuous while textual representations are discrete; and (2) the decoder is responsible for generating text that is very different from traditional speech representation. Our efforts focus on directly creating an audio-denoising model that can learn efficiently and applied to a variety of datasets. The transformer was originally proposed for natural language processing (NLP) tasks [2] and has recently become popular in the fields of computer vision (CV) [3] and audio processing (AP) [4]. The transformer model can effectively solve the long-term dependency problem and can run well in parallel, showing good performance on many natural language processing tasks. Transformer-based methods also show promising performance in audio denoising. Yu et al. [5] proposed a dual-branch federative magnitude and phase estimation framework, named DBT-Net, for monaural speech enhancement, aiming at recovering the coarse- and fine-grained regions of the overall spectrum in parallel. Wang et al. [6] proposed a two-stage transformer neural network for end-to-end speech denoising in the time domain. Yu et al. [7] proposed a cognitive computing-based speech enhancement model termed SETransformer to improve speech quality in unknown noisy environments, which takes advantage of the LSTM and multi-head attention mechanisms. Dand et al. [8] proposed a dual-path transformer-based full-band and subband fusion network (DPT-FSNet) for speech enhancement in the frequency domain. Speech typically requires long-range sequence modeling, convolutional neural networks necessitate a greater number of convolutional layers to expand the receptive field and thus continuously increase model complexity. In many natural language processing and visual tasks, self-attention transformerbased neural networks outperform deep learning models, which are constructed based on convolutional neural networks (CNNs) [9]. Additionally, RNN models can be commonly utilized for modeling long-term sequences with sequential information, such as the Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) [10]. However, RNN-based models are unable to be processed in parallel due to the temporal structure of RNNs, leading to suboptimal computational efficiency and increased complexity. Recent studies have demonstrated that a self-attention methodology can be used for audio denoising, which will relieve the challenges associated with modeling long-sequence speech signals [4]. The ability to learn efficiently from raw audio is crucial for constructing speech models for speech enhancement. Inspired by the capability of the transformer in sequence modeling, we aim to develop an efficient transformer approach that enhances audio quality by learning general and meaningful speech features. We propose a dual-phase audio transformer for denoising (DPATD) that incorporates explainable attention and memory-compressed attention. The intuition is to divide the input audio into shorter chunks and interleave arXiv:2310.19588v1 [cs.SD] 30 Oct 2023 dual-phase transformers, a local-chunk transformer and a global-chunk transformer for local and global modeling, respectively. Additionally, to solve low-occupancy or unneeded shared memory reads and writes on the GPU, we employ FlashAttention-2, a unique attention algorithm with superior work partitioning [11]. The contributions of this work are threefold: 1) We propose a dual-phase audio transformer that facilitates the modeling of audio signals by organizing transformer layers. It first segments the input audio into shorter chunks and employs dual-phase transformers in an interleaved manner, namely, a local-chunk transformer for local modeling and a global-chunk transformer for global modeling. 2) We develop a novel transformer block that combines explainable attention and memory-compressed attention to conserve memory resources and provide interpretable attention maps that are noise-resistant and align with informative input patterns naturally. 3) Our extensive experiments on two benchmark datasets demonstrate that the DPATD outperforms state-of-the-art methods across all evaluation criteria while maintaining a relatively low model complexity. II. RELATED WORK Traditional speech denoising techniques. Traditional speech denoising techniques primarily rely on statistical techniques that can be utilized to build relevant denoising models and extract clean audio from noisy input signals. The denoising performance can be improved by the Wiener filter [12]. Linden et al. [13] decomposed a spectral graph into a spectral basis matrix and an encoding matrix. After the different sound sources are reconstructed based on the clustering of the basis matrix and the corresponding encoding information, the noise components are removed to facilitate more accurate monitoring of biological sounds. Paliwal and Basu [14] proposed a Kalman filtering method to improve speech enhancement performance. Ali et al. [15] focused on the denoising of phonocardiogram (PCG) signals using different families of discrete wavelet transforms, thresholding types and techniques, and signal decomposition levels. Deep learning for speech enhancement. Different speech enhancement models based on deep neural networks have steadily taken center stage with the advancement of deep learning technology. Based on different model inputs, current voice augmentation techniques for DNNs can be broadly divided into two groups: time-domain (T) techniques and time-frequency domain (TF) techniques. Time-domain techniques employ an end-to-end model that directly estimates clean waveforms using audio data in the time domain as raw waveform inputs. The architecture foundation for timedomain approaches is WaveNet [16]. The majority of speech enhancement techniques currently focus on the time-frequency domain of speech. The TF techniques use the short-time Fourier transform (STFT) and the inverse short-time Fourier transform (ISTFT). The latest frequency-domain model, BandSplit RNN, explicitly splits the spectrogram of the mixture into subbands and performs interleaved band-level and sequencelevel modeling for speech enhancement [17]. III. MOTIVATION Most existing deep learning-based audio denoising methods study the magnitude spectrum of images for audio denoising. However, these methods can be constrained by computing power or limited filtering image regions, resulting in low denoising performance. The transformer applications in audio enhancement are still limited. Inspired by neural network approaches to text, our model encodes audio information and is trained to understand what clean audio should look like. Basically, we regard the audio signal as an ”audio sequence” and further segment it into smaller chunks. The attention of each audio chunk will be calculated based on other chunks in the given audio sequence. IV. MODEL In this section, we first review the audio denoising task, provide the motivation for our model, then conduct an indepth analysis of the architecture of our dual-phase audio transformer for denoising (DPATD). Our model first splits the input audio into several audio chunks and encodes the audio sequence. Sequentially, generated sequence vectors are fed into a dual-phase transformer to train to minimize the difference between denoised audio and clean audio. Finally, we get the denoised audio as shown in Fig. 1. We assume that the mixture speech signal y(t) is a linear sum of the clean speech signal x(t) and noise ε(t), and the noisy speech y(t) can be typically expressed as Eq. (1): y(t) = x(t) + ε(t). (1) A. Segmentation Our DPATD framework first splits the input audio into several local chunks,then calculates both representations and their relationship. The segmentation stage encodes the audio sequence using patch embedding with the maximum audio sequence size. The smaller utterances are zero-padded to match the size of the maximum audio sequence. Given a sequence of acoustic input vectors Y (y1, y2, · · · , yL) ∈ R1×L where L is the audio sequence length, the segmentation stage splits Y into chunks of length K and hop size P. Every sample in Y appears and only appears in chunks, generating M equal size chunks Dm ∈ R1×K, m = 1, · · · , M. All chunks are then concatenated together to form a 2-D tensor T = [D1, · · · , DM] ∈ RK×M. Positional embeddings are added to patch embeddings to preserve positional information. We use standard learnable 1D position embeddings since the audio input is an ordered sequence. The segmentation output tensor T is then passed to the stack of N transformer blocks. Each block converts a 2-D tensor input to another tensor with the same shape. We denote the input tensor for block Z = 1, · · · , Z as Tz ∈ RK×M, where T1 = T. The segmentation phase of the model is shown in Fig. 1. Fig. 1. System flowchart of the DPATD model. The segmentation stage splits an audio input into chunks without overlaps and concatenates them to form a 2-D tensor. The transformer block, which consists of local-chunk and global-chunk transformers, is applied to individual chunks in parallel to process information. Multiple blocks are stacked to increase the total depth of the network. A 2-D output of the last block is converted back to an audio output. B. Dual-Phase Audio Transformer Basic Architecture. Since the input noisy audio and the output enhanced audio have the same length, we introduce a simple but effective modification to the transformer for audio sequences by removing the encoder module (almost reducing model parameters by half for a given hyperparameter set). Each transformer block contains two submodules. As shown in Fig. 2, the first module is a MemoryCompressed Explainable Multi-Heads Attention (MCE-MSA). This attention consists of explainable multi-heads attention and memory-compressed attention. The second module is a simple, position-wise, fully connected feed-forward network. In addition, inspired by the effectiveness of RNNs in tracking ordered sequential information, we replace the first fully connected layer of the feed-forward network with a GRU layer to learn more positional information. A residual connection was employed around each of the two sub-modules, followed by layer normalization. C. Explainable Memory-Compressed Attention. Memory-Compressed Attention To handle longer sequences, we modify the multi-head attention to reduce memory usage by limiting the dot products between Q and K in Eq. (3). To achieve this goal, we take advantages of a strided convolution with convolution kernels of size 3 with stride 3. Since the memory cost of attention is constant for each block, this alteration allows us to maintain the linear relationship between the number of activations and the length of the sequence [18]. Explainable Multi-Heads Attention. In our work, attention blocks employ h = 8 heads (the number of parallel attention layers) and map the input h times to get Q, K, and V representations, respectively, as described in Eq. (2). Given an input Y , each head Hh holds an explainable attention weight Ah ∈ RN×d that represents the relative importance of input features. Ah aims to learn explainable features for the output through the MCE-MSA mechanism. Qi = Y WQ i , Ki = Y W K i , Vi = Y WV i , (2) where Y ∈ Rd×k is the input with sequence of length L and dimension d, i = 1, 2, · · · , h and Qi , Ki , Vi ∈ Fig. 2. The architecture of the DPAT block. The Explainable MultiHead Attention (E-MHA) module is capable of providing interpretable attention maps that are noise-resistant and align with informative input patterns naturally. We utilize a strided convolution to limit the dot products between Q and K. We replace the first fully connected layer of the feed-forward network with a GRU layer to learn more positional information. Rl×d/h are the mapped queries, keys and values respectively. W Q i , W K i , WV i ∈ Rd×d/h denote the i-th linear transformation matrix for queries, keys, and values, respectively. The self-attention operation is constructed by Eq. (3). W implies how much attention is paid to each token. Att(Q, K, V ) = sof tmax( QKT √ dk )V = sof tmax(W)V. (3) The attention weight A is defined as: A = L(W + b) T , (4) where b is a trainable bias term, which is introduced as an initial alignment for the input patterns. L is a non-linear function that scales the L2 norm of its input. In follows, the self-attention feature P is formally expressed as: P = A T V, (5) According to Eq. (4), ∥A∥ ≤ 1. There P in Eq. (5) is upperbounded as follows: P = ∥A∥∥|V |∥cos(A, V ) ≤ ∥V ∥. (6) When Eq. (6) is optimized, the attention weight A is proportional to V . In order to achieve maximal output, A is driven to align with the discriminative features in V , instead of the uninformative noise. Therefore, P can only achieve this upper bound if all possible solutions of v ∈ V are encoded as eigenvectors in the weight A. This maximization suggests that with the attention weight A, we will obtain an inherently explainable decomposition of input patterns. In our work, whole sequential explainable and memorycompressed transformer blocks are computed as: S l = MCE − MSA(Z l−1 ), (7) Z l = LayerNorm(Z + S l ), (8) F F N(Z l ) = ReLU(GRU(Z l )W1 + b1)W2 + b2, (9) Output = LN(Z l + F F N(Z l )), (10) where LayerNorm(·) is the LayerNorm layer, F F N(·) denotes the output of the position-wise feed-forward network, and W1 ∈ Rdff ×d , b1 ∈ Rd and df f = 4 × d. D. Dual-phase Audio Transformer module The dual-phase audio transformer module consists of four stacked dual-chunk transformer blocks. Each block converts an input 2-D tensor into another tensor with the same shape. We propose a dual-phase transformer block based on explainable and memory-compressed attention. As shown in Fig.1, it has a local-chunk transformer and a global-chunk transformer, which extract local and global information, respectively. More specifically, the input is a 2-D tensor ([K, S]), and the localchunk transformer is first applied to individual chunks to parallelly process inter information, which performs on the last dimension F of the input tensor. Then, the global-chunk transformer is used to fuse the information of the output from the local-chunk transformer to learn global dependency, which is implemented on the dimension of the tensor. Besides, each transformer is followed by the group normalization operation and utilizes residual connections. Decoder. We use the patch-expanding layer in the decoder to upsample the extracted deep features. The 2-D convolution with a filter size of (1, 1) recovers the channel dimension of the enhanced speech feature into 1 and produces the enhanced speech waveform by an overlap-add method. Loss Function. The time-domain loss is based on the mean square error (MSE) between the input clean audio (x1, x2, · · · , xN ) and the predicted audio(ˆx1, xˆ2, · · · , xˆN ). The model is optimized by minimizing the MSE, which is defined as: Loss = 1 N N X−1 i−1 (xi − xˆi) 2 , (11) where N denotes the number of samples. The overall training algorithm is shown in Alg. 1. Algorithm 1 DPATD: Dual-Phase Audio Transformer for Denoising. Batch of audio input: B(Y ) = {Y 1 , ..., Y nB }, and their clean audio input B(X) = {X1 , ..., XnB }, where nB is the total number of batch. I is the number of iterations 1: Input: Mixture audio signals Y = {yi} N i=1 and clean audio input X = {xi} N i=1, where N is the total sample number of audios. 2: Output: Denoised audio signals: Xˆ 3: Segmentation splits the input audio into a 2-D tensor T = [D1, · · · , DM] using 1-D CNNs. 4: for iter = 1 to I do 5: Derive batch-wise data: Y k and Xk sampled from B(Y ) and B(X) 6: Optimize our audio generation model DPATD using Eq. (11) 7: end for 8: Output the denoised audio signals Fig. 3. Denoising results comparisons. Raw audio is the original noisy audio. V. EXPERIMENTAL PROCEDURES A. Dataset VCTK+DEMAND dataset. We validate the effectiveness of our proposed model on a standard speech dataset [19]. The clean speech datasets are selected from the Voice Bank Corpus, including a training set of 11572 utterances from 28 speakers and a test set of 872 utterances from 2 speakers. BirdSoundsDenoising. This dataset uses a variety of natural noises, such as wind, rain, waterfalls, etc., in place of the normal intentionally generated noise [20]. In particular, the dataset contains 14,120 audios from one second to fifteen seconds and is a large-scale dataset of bird sounds collected, containing 10,000/1,400/2,720 in training, validation, and testing sets, respectively. B. Implementation details We train the DPATD model for 100 epochs on mini-batches of 8 random samples, where the segment chunk size is set to 1000. We used the Adam optimization scheme with a maximum learning rate of 2.5e-4. Since the layernorm layer is extensively used throughout the model, a simple weight initialization of N(0, 0.02) was adequate. For the activation TABLE I RESULTS COMPARISONS OF DIFFERENT METHODS (F1, IoU, AND Dice SCORES ARE MULTIPLIED BY 100. “−” MEANS NOT APPLICABLE. Networks Validation Test F1 IoU Dice SDR F1 IoU Dice SDR U2 -Net [22] 60.8 45.2 60.6 7.85 60.2 44.8 59.9 7.70 MTU-NeT [23] 69.1 56.5 69.0 8.17 68.3 55.7 68.3 7.96 Segmenter [24] 72.6 59.6 72.5 9.24 70.8 57.7 70.7 8.52 SegNet [25] 77.5 66.9 77.5 9.55 76.1 65.3 76.2 9.43 DVAD [20] 82.6 73.5 82.6 10.33 81.6 72.3 81.6 9.96 R-CED [26] − − − 2.38 − − − 1.93 Noise2Noise [27] − − − 2.40 − − − 1.96 TS-U-Net [28] − − − 2.48 − − − 1.98 DPATD − − − 10.49 − − − 10.43 function, we used the Gaussian Error Linear Unit (GELU). We used the learned position embeddings instead of the sinusoidal version proposed in the original work. With a single NVIDIA GTX 3060 GPU and PyTorch to implement our model, it took around 180 hours of GPU time to train our model. The smaller utterances in a batch are zero-padded to the largest size of utterances. A dynamic strategy is used to adjust the learning rate during the training stage [21]. Model configurations. The configuration of our model structure partially follows the original transformer architecture. We trained a 12-layer decoder-only transformer with selfattention heads (1000 dimensional states and 12 attention heads). Here, we focus on explainable and memory compression attention to effectively compute and conserve memory. For the position-wise feed-forward networks, we used 4000- dimensional inner states. Consider the sum of the input audio length in a signal block denoted by K × S where the hop size is set to be the same as the embedding size. It is simple to see that S = [L/K] where [·] is the ceiling function. To achieve the minimum total input length K +S = K + [L/K], K is selected such that K ≈ sqrt(5L). This gives us sublinear input length(O(sqrt(L)) rather than the original linear input length (O(L)). C. Result Tab. II shows the comparison results of the VCTK+DEMAND dataset. Our model surpasses most waveform-based approaches now in use in terms of the PESQ score and achieves performance that is equivalent to other methods in other evaluation metrics. For the BirdSoundsDenoising dataset, we report the performance of eight state-of-the-art baselines. The results are shown in Tab. I, where the bold text indicates the best outcomes for each statistic. The results demonstrate that our model outperforms other state-of-the-art methods in terms of the SDR. Results of F1, IoU, and Dice are not included because these metrics are used for the audio image segmentation task [29], [30]. The comparisons of raw bird audio, ground truth labeled denoised audio, and denoised audio of other models are shown in Fig. 3. Additionally, our model bears more resemblance to the labeled denoised signal. As a consequence, our model enhances the audio-denoising capabilities of the BirdSoundDenoising dataset. TABLE II COMPARISON RESULTS ON THE VOICEBANK-DEMAND DATASET. “−” MEANS NOT APPLICABLE. Methods Domain PESQ STOI CSIG CBAK COVL PGGAN [31] T 2.81 0.944 3.99 3.59 3.36 DCCRGAN [32] TF 2.82 0.949 4.01 3.48 3.40 S-DCCRN [33] TF 2.84 0.940 4.03 2.97 3.43 TSTNN [6] T 2.96 0.950 4.33 3.53 3.67 PHASEN [34] TF 2.99 − 4.18 3.45 3.50 DEMUCS [35] T 3.07 0.95 4.31 3.40 3.63 SE-Conformer [36] T 3.13 0.95 4.45 3.55 3.82 MetricGAN+ [37] TF 3.15 0.927 4.14 3.12 3.52 MANNER [38] T 3.21 0.950 4.53 3.65 3.91 CMGAN [39] T 3.41 0.96 4.63 3.94 4.12 DPATD T 3.55 0.97 4.78 3.96 4.22 TABLE III EFFECT OF LAYERS AND CHUNKS IN TRANSFORMER BLOCK IN THE DPATD. (a) Heads 6 8 12 16 PESQ 2.99 3.23 3.45 3.15 (b) Chunks 500 1000 2000 PESQ 3.26 3.45 3.38 Ablation study. First, we examine the performance of our method by comparing it with different transformer layers and the effect of chunks in segmentation. In order to further demonstrate the effectiveness of our proposed transformer block, we also designed another architecture for comparison. In this architecture, we use different transformer blocks rather than the 12 blocks in the DPATD, and we increase or reduce the number of heads. In addition, we also set chunk sizes at 500 and 2000, while only 1000 in our DPATD. From Tab.III(a), a 12-transformer-block DPATD has better scores than other models. As shown in Tab.III(b), the chunk has only a slight influence on the performance, and we use chunk=1000 by default for its efficiency. VI. CONCLUSION In this study, we present a framework for using a dualphase audio transformer for denoising (DPATD) to provide robust speech enhancement. The DPATD splits the audio input into non-overlapping chunks in the segmentation stage, which are then passed as input to the transformer model. In a DPAT block, the local-chunk transformer and globalchunk transformer process the local chunks and all the chunks, respectively. We modified the transformer model using explainable multi-head attention and memory-compressed attention. Extensive experiments on datasets have demonstrated the effectiveness and superiority of the proposed DPATD architecture. Finally, our method is still computationally demanding, and future directions of the work could improve on these limitations. ",
		"summary": "Recent high-performance transformer-based speech enhancement models demonstrate that time domain methods could achieve similar performance as time-frequency domain methods. However, time-domain speech enhancement systems typically receive input audio sequences consisting of a large number of time steps, making it challenging to model extremely long sequences and train models to perform adequately. In this paper, we utilize smaller audio chunks as input to achieve efficient utilization of audio information to address the above challenges. We propose a dual-phase audio transformer for denoising (DPATD), a novel model to organize transformer layers in a deep structure to learn clean audio sequences for denoising. DPATD splits the audio input into smaller chunks, where the input length can be proportional to the square root of the original sequence length. Our memory-compressed explainable attention is efficient and converges faster compared to the frequently used self-attention module. Extensive experiments demonstrate that our model outperforms state-of-the-art methods. ",
		"id": "UUID19"
	},
	{
		"document": "1 Introduction 1 Story understanding and telling is one of the important but challenging problems in natural language processing and machine learning, and there have been extensive studies on this problem for decades (Harrison et al., 2017; Kocisk ˇ y` et al., 2018). Many prior researches have focused on building models that understand stories such as utilizing pre-trained model (Chen et al., 1 * These authors contributed equally to this work. 2022; Wang et al., 2023; He et al., 2023), eventcentric understanding (Chen et al., 2021) and using multiway-sampler (Xu et al., 2023). Also, lots of approaches are proposed that aim to understand the stories within the provided text (Mostafazadeh et al., 2016; Fan et al., 2018; Akoury et al., 2020), image (Huang et al., 2016; Bensaid et al., 2021; Krojer et al., 2022) and video (Rohrbach et al., 2015; Xu et al., 2016; Tapaswi et al., 2016; Li et al., 2019; Bain et al., 2020; Huang et al., 2020). Despite there being many benchmarks for story understanding and telling, most of them less focus on audio even if it is one of the key elements for humans to understand stories (Lang et al., 1998; Lake et al., 2017). Therefore, we propose to extend story understanding and telling areas by establishing a new component called background sound which is story context-based audio without any linguistic information. For this purpose, we introduce a novel dataset called the Sound of Story (SoS), which aims to leverage audio information for story understanding, distinguishing it from other existing story or audio datasets. To create the SoS dataset, all the modalities are extracted from Condensed Movie Dataset (CMD) (Bain et al., 2020) and Large Scale Movie Description Challenge datasets (LSMDC) (Rohrbach et al., 2015). Image and text data extracted from the same movie clip have their contexts and both of them have the possibility to infer the story from the same movie clip. And in the case of audio such as background sound and music, it has the ability to enrich storytelling by representing and recognizing the context of the situations (Eronen et al., 2005; Stowell et al., 2015). Due to these characteristics, audio, image, and text extracted from the same movie clip become correlated and complementary. So, our SoS dataset allows three modalities generated from the same movie clip to be used for story understanding as a pair. In addition, we propose new story underarXiv:2310.19264v1 [cs.MM] 30 Oct 2023 Speech-decoupled Audio Merged & Refined Movie Dataset Audio Clustering Descriptions Speech He reaches for the script.Harry backs away, holds the script to his chest. Bo Catlett notices this.Bo Catlett eyes him a beat, then pushes out of his chair. Bo Catlett nods, gets up.Ronnie sits up straight.Ronnie opens his coat so that Harry can see a gun tucked in his belt.Bo Catlett gives Ronnie a look. Bo Catlett glances once more at Chili then follows Ronnie out the door…. 5.5 cm Decoupling Video Frames Image Sequence . . . . . . <Image Sequence> <Speech-decoupled Audio> <Descriptions> Figure 1: Overall data generating process flow for our Sound of Story (SoS) dataset. All the modalities are extracted from the same movie clips. We exclude the speech from audio to make it contain only pure audio data, which does not contain linguistic information. standing benchmarks and their baseline models by introducing audio retrieval tasks and audio generation tasks using the SoS dataset. These background sound tasks can be utilized in various applications. For example, for movies and cartoons, background sound is often produced according to a designated scene using various props or objects for a sense of reality. In the case of the retrieval task, among the sound sources made, it is possible to recommend the sound source which fits the best to the scene. Furthermore, background sound generation can produce more realistic audio while reducing human resource costs by directly generating the right audio for a given scene. The overall data generating process can be shown in Figure 1. In summary, our contributions are as follows: • We propose to extend story understanding and telling areas by establishing a new component called background sound which is story context-based audio without any linguistic information, which has not been well explored. For this purpose, we introduce a new dataset named Sound of Story (SoS) consisting of text and image sequences and audio for a story. To the best of our knowledge, this is the largest well-curated dataset for storytelling with sound. • As benchmark tasks to show the significance and relevance of sound in a story, we introduce retrieval tasks between audio and other modalities and background sound generation tasks, and baseline models for each of them. • We will release the dataset and code to generate them so that the community can generate more data for a variety of tasks to utilize sound in storytelling and story understanding research. 2 Related Work Story understanding and telling Storytelling tasks pose significant challenges in the domain of natural language processing (NLP), and researchers have proposed various datasets and benchmarks to tackle these problems. Wikiplots, WritingPrompts (Fan et al., 2018), and ROCStories (Mostafazadeh et al., 2016) introduce datasets that revolve around text-based story plots. Wikiplots comprises a collection of approximately 113k story plots extracted from English Wikipedia. WritingPrompts (Fan et al., 2018) assembled their dataset by gathering pairs of human-written stories and corresponding prompts from online sources. ROCStories (Mostafazadeh et al., 2016) employed the use of Amazon Mechanical Turk (AMT) to generate short stories that incorporate common-sense knowledge, with a focus on everyday topics. These datasets are usually used for story understanding or story generation tasks. In addition to text-based story plots, there exist numerous story datasets that incorporate images or videos. Visual Storytelling (Huang et al., 2016) introduces the first sequential vision-to-language dataset and explores its applicability in various tasks. It goes beyond simple object understanding or captioning for concrete scenes by considering input that encompasses temporal events. Similarly, ImageCoDe dataset (Krojer et al., 2022) also contains stories but is designed to enable models to Example Clip 1 (CMD) Example Clip 2 (CMD) Example Clip 3 (LSMDC) Example Clip 4 (LSMDC) … … The football coach, the assistants, and Alabama players cheer for Forrest.Forrest runs across the field.He speeds past the defending players.Forrest runs past the opposite players.The football coach runs along the sidelines as he yells. Forrest … Jackie breaks into tears.They share a helpless half-laugh -- then Frank appears in the doorway.She starts to hand him a large mirror from her purse -- then thinks better of it.But he holds out his hand and she gives him the mirror.Simon holds … … … Dora parties with her friends over the end credits. Po's biological father Li shows up at his village. Figure 2: Example of SoS dataset. CMD and LSMDC have different lengths of descriptions. learn detailed situations from finer visual differences within similar sets of images. Movie data inherently encompasses storytelling. Various datasets such as MovieQA (Tapaswi et al., 2016) and MovieNet (Huang et al., 2020) have been collected by using movie data and have been used in various multimodal domains. Additionally, datasets like CMD (Bain et al., 2020) and LSMDC (Liu et al., 2019) not only provide movie videos but also include descriptions for the corresponding clips. This offers researchers an opportunity to explore models from a multimodal perspective. However, there are few datasets for story with sound. Existing benchmarks for audio mostly consist of music (Bertin-Mahieux et al., 2011; Sturm, 2012; Ferraro et al., 2021) or animal/environmental sounds (Salamon et al., 2014; Piczak, 2015; Gemmeke et al., 2017; Kim et al., 2019), and are designed to evaluate simple matching performance between objects (or sentiment) and audio input. Multi-modal retrieval tasks Multi-modal retrieval models are trained to find similar features across different modalities by fusing. These models are tasked with understanding both the context of a given input and the interplay between features provided by other modalities. As a result, retrieval becomes a highly challenging and crucial task in order to comprehend the distinctive characteristics of multiple modalities. The multi-modal retrieval task has been studied using various modalities such as image-text retrieval (Wang et al., 2020; Zhang et al., 2020; Cheng et al., 2022; Luo et al., 2022; Xuan and Chen, 2023), video-text (Ma et al., 2022; Gorti et al., 2022; Zhu et al., 2023), audio-image (Xu, 2020; Yang et al., 2022; Nakatsuka et al., 2023), video-audio (Surís et al., 2018; Gu et al., 2023; Cheng et al., 2023) and audio-text (Kim et al., 2022; Xin et al., 2023). Particularly, CLIP4CLIP (Luo et al., 2022), which performs well in the videotext retrieval task by calculating the similarities between the features of each modality obtained from the encoder, and X-CLIP (Ma et al., 2022) expands CLIP4CLIP and proposes a multi-grained regulation function to improve performance. Furthermore, Wav2CLIP (Wu et al., 2022) conducts retrieval for audio-image modalities based on CLIP architecture (Radford et al., 2021), while CLAP (Elizalde et al., 2023) focuses on audio-text retrieval. In this paper, we propose a story-based audio retrieval task for retrieving proper modal features like image sequences or descriptions. Multi-modal audio generation There have been prior studies on generating audio by receiving cer- Dataset Audio Type # Story # Images / Story # Audio Hours Avg. text length Text Type ImageCode - 9,402 17 - 23.3 Captions VIST - 50,136 4.2 - 10.2 Description Video Storytelling open 105 - 22 162.6 Captions MovieNet open 1,100 - 633 2004 Description MSR-VTT open 10,000 - 41.2 9.6 Description Ours speech-decoupled audio 27,354 19.6 984 88 Description Table 1: Statistics compared to different storytelling datasets. In the audio types, 'open' represents raw audio from the video, and 'speech-decoupled audio' represents the speech-excluded audio. In the text types, 'Caption' represents the text that explains only the image, and 'Description' represents the text that includes the story. tain modality information as input (Vasquez and Lewis, 2019; Yu et al., 2022; Borsos et al., 2022; Neves et al., 2022; Schneider et al., 2023; Yang et al., 2023; Agostinelli et al., 2023). Among several audio generation tasks, recently text-audio generation has been actively studied. AUDIOGEN (Kreuk et al., 2022) uses an augmentation technique that mixes different audio samples when generating audio samples from captions. Riffusion (Forsgren and Martiros, 2022) fine-tunes the model to generate Spectrogram images using the image-to-text based on Stable Diffusion (Rombach et al., 2022). MUSICGEN (Copet et al., 2023) uses a single-stage language model with efficient token interleaving patterns and an unsupervised melody conditioning method. In this paper, we are interested in generating relevant audio given a story. 3 Dataset We propose a novel dataset named the SoS dataset with 27,354 audio-image-text pairs. 22,330 pairs are created from CMD (Bain et al., 2020) and 5,024 from LSMDC dataset (Rohrbach et al., 2015). We extract the image and audio from the same video and pair them with the description accordingly so that the images and text share the same storyline. In the case of audio, it is related to the other modalities due to being extracted from the same clip so can be used in complementary. The examples of the SoS dataset are shown in Figure 2. 3.1 Movie Clips CMD (Bain et al., 2020) provides the key clips and their high-level semantic descriptions from 3,605 movies. Each key clip has a duration of around 150 seconds and can be automatically obtained from YouTube, making it easily accessible. LSMDC (Rohrbach et al., 2015) consists of 118,081 short clips extracted from 202 movies. Each clip in LSMDC has a duration of around 3 to 12 seconds and is accompanied by captions generated from Description Video Services (DVS). In our work, we merge these two movie clip datasets to create the audio-image-text paired dataset. To bridge the gap between CMD and LSMDC, we adapt the LSMDC dataset to match the format of CMD. Since the short movie clips in LSMDC are contiguous, we concatenate them in sequential order until their combined length is similar to that of CMD. Specifically, we concatenate around 20 short movie clips together to create the raw movie clip data. 3.2 Audio processing To facilitate audio processing, we first utilized ffmpeg (Tomar, 2006) to extract audio from the video, ensuring that the length of the extracted audio matched that of the raw movie clip video. Subsequently, we employed the bytesep framework proposed by (Kong et al., 2021) to decouple speech from mixed audio containing various sounds. The results of decoupled sounds may be considered as audio data that share the storyline with the raw movie clip video without any linguistic information. Speech-decoupling is proceeded because the speech itself includes the storyline and the model can learn the storyline focused on the speech, not the audio which is not our expectation. Duration distribution of the extracted audios can be seen in Figure 3. 3.3 Image processing Since videos include many duplicated and similar images, only the key salient images that may represent storyline were extracted through image clustering. We conduct image clustering 2 using image fingerprints, which are representations for uniquely identifying an image generated based on the visual features. From all image fingerprints 2 https://github.com/elcorto/imagecluster (a) Frames Distribution (b) Duration Distribution (c) Number of words from CMD (d) Number of words from LSMDC Figure 3: Distributions of each modality. (a) the number of extracted images from a movie video; (b) the duration of decoupled and processed audio for each story (c) the number of words in each story from CMD (d) the number of words in each story from LSMDC. Note that due to video clip concatenation to match video lengths, descriptions have larger lengths for stories from LSMDC. This text variation leads to the model requiring high-level text understanding. extracted from frames of the movie clips, we gradually removed images with a fingerprint similarity of greater than 0.5 which was set empirically found. Through this process, fewer frames for static and more frames for dynamic scenes could be retained. The extracted image sequences can represent the entire video by capturing salient scenes with significant variations, even though it does not include all of the frames. By utilizing this approach, we can create essential image sequences of the video by focusing on scenes with meaningful changes. These processed image sequences provide a novel dataset that highlights the storyline with fewer frames and differentiates it from traditional video datasets. Figure 3 shows the distribution of extracted frames for each movie clip. 3.4 Text processing There exist slight differences in clip lengths between CMD and LSMDC even though both of them provide short descriptions for each movie clip. Because we concatenate LSMDC movie clips to match the length of CMD movie clips, descriptions also must be formatted by concatenating the same number of descriptions with the video. As a result, the concatenated descriptions exhibit variations in sentence lengths as in Figure 3. Furthermore, there are also inherent differences between the two datasets in how the descriptions were generated. CMD utilizes descriptions generated by users from YouTube, while LSMDC employs descriptive video services to generate descriptions. Because of this text variation difference between the two datasets, it makes our task more challenging by requiring high-level text understanding. We can check that the descriptions are well-aligned with other modalities in Figure 2. 3.5 Data Statistics and Analysis Our SoS dataset is derived from 3,807 movies covered in the CMD and LSMDC datasets, resulting in a total of 27,354 clips. After processing, each clip has an average of 20 images, and the total number of images in our dataset is 535,707. The accumulated length of the extracted audio is around 984 hours, and each audio consists of around 150 seconds on average. Each clip’s text description consists of 88 words on average. If divided into two different datasets, CMD and LSMDC consist of an average of 83.2 words and 1,378.7 words, respectively, with medians of 80 and 130.8, respectively. Quantitative comparison with other storybased datasets is shown in Table 1, and it can be seen that our proposed SoS dataset is the largest well-curated storytelling dataset with extracted audios. We generate paired decoupled audio data that can be used complementarily for story understanding and telling with the image and text in the SoS dataset. The extracted decoupled audio, which is in-the-wild audio, contains abundant information. Specifically, we decoupled speech information to leave only pure audio consisting of background sound on the audio dataset. These remaining audios contain a mixture of various daily-life sounds and background music. This distinctive characteristic sets it apart from other audio datasets, making it an adequate audio dataset for story understanding. 4 Experiment 4.1 Retrieval Tasks In this section, we introduce four retrieval tasks: audio-to-video, audio-to-text, text-to-audio, and video-to-audio retrieval using our proposed dataset. The retrieval tasks focus on finding proper audio Similarity A2T & T2A Similarity A2V & V2A ••• The Stranger leads … Text Encoder ••• Description Audio Audio Embedding ••• Audio Encoder ••• Frame Encoder Temporal Encoder Video Frames ••• e ••• ••• Mean Pool Audio-Sentence Score Audio-Words Score Audio Sequence-Sentence Score Audio Sequence-Words Score Audio-Video Score Audio Sequence-Frame Score Audio Sequence-Video Score Audio Sequence-Video Score Figure 4: Proposed retrieval model architecture. Encoded features are obtained from the encoder corresponding to each modality. Features are divided into context-level features and sequence-level features. Then we calculate the score for each task and use the score to calculate the final similarity Sim. corresponding to image sequences or text information containing a story, or matching image sequence or text given audio, which may be used as recommendation for background sound suitable for image sequence or storyline. 4.1.1 Retrieval Model In this section, we introduce the model used in our proposed retrieval task. We are inspired when we create a baseline model in that the Attention Over Simplicity Matrix method proposed by XCLIP (Ma et al., 2022) can be flexibly applied to Audio as well as Video-Text. Our overall architecture is illustrated in Figure 4. For descriptions, we use the CLIP text encoder ET (Radford et al., 2021). The information obtained from the Et is defined as sentence feature fs and word feature fw. In the case of video, the CLIP image encoder EI is used and the temporal encoder Et is added to obtain the temporal sequence feature. The obtained feature is defined as frame feature ff , and the mean pooling of the frame feature is defined as video feature fv. And in the case of the audio, audio information is extracted by performing audio embedding on Log Mel-filter bank coefficients and fed into the EA using pretrained Audio Spectrogram Transformer (Gong et al., 2021). In this case, the first token among the acquired feature is defined as audio fa, and the remaining tokens are defined as audio sequence feature fas. In the case of audio-video retrieval, we first calculate the similarity between audio-video(Sa,v), audio-frame(Sa,f ), audio sequence-video(Sas,v), and audio sequence-frame(Sas,f ). These similarities are calculated following Equation 1 to 4: Sa,v = (fa) T (fv) ∈ R 1 (1) Sa,f = (ff fa) T ∈ R 1×m (2) Sas,v = fasfv ∈ R n×1 (3) Sas,f = (fas)(ff ) T ∈ R n×m (4) where m is the number of video frames and n is the number of audio sequences. Except for audio-video similarity, an aggregation process is required to make it an instance-level similarity. Therefore, the audio-frame and audio sequence-video similarity is calculated through the following equations: S ′ a,f = Xm i=1 exp(Sa,f(1,i)) Pm j=1 exp(Sa,f(1,j)) Sa,f (5) S ′ as,v = Xn i=1 exp(Sas,v(i,1)) Pn j=1 exp(Sas,v(j,1)) Sas,v (6) The audio-level similarity(S ′ aud) and video-level similarity(S ′ vid) are calculated to obtain the audio sequence-frame instance-level similarity: Saud = Xn i=1 exp(Sas,f(i,∗)) Pn j=1 exp(Sas,f(j,∗)) Sas,f(i,) ∈ R 1×m (7) S ′ aud = Xm i=1 exp(Saud(1,i)) Pm j=1 exp(Saud(1,j)) Saud(1,i) (8) Svid = Xm i=1 exp(Sas,f(,i)) Pm j=1 exp(Sas,f(,j)) Sas,f(,i) ∈ R n×1 (9) S ′ vid = Xn i=1 exp(Svid(i,1)) Pn j=1 exp(Svid(j,1)) Svid(1,1) (10) A2V V2A A2T T2A Eval Ours Wav2CLIP Ours Wav2CLIP Ours CLAP Ours CLAP Random R@1(↑) 7.615 4.231 6.462 5.154 7.462 6.538 7.077 5.769 0.076 R@5(↑) 18.682 12.615 17.923 16.077 17.462 17.231 19.385 16.077 0.378 R@10(↑) 28.000 19.538 27.000 23.462 24.692 24.615 25.692 23.538 0.750 Median R(↓) 40.615 31.231 38.923 33.692 33.000 34.615 32.769 34.385 631.800 Mean R(↓) 32.000 52.000 33.000 47.000 60.000 52.000 63.500 53.500 639.132 Table 2: Retrieval performances with the proposed SoS dataset, which are measured on 1300 test samples. Random means the average of the results of five random retrievals. where * denotes all elements in that dimension. In addition, S ′ as,f are obtained from the means of S ′ aud and S ′ vid obtained by Equation 8 and 10. Therefore, the similarity between video and audio is obtained as the average of the obtained similarities, as shown in Equation 11. SimAV = (Sa,f + S ′ a,f + S ′ as,v + S ′ as,f )/4 (11) For the audio-text retrieval, the similarity SimAT can be calculated by replacing videorelated features with text-related features. 4.1.2 Implementation Details Of the 27,354 stories in our SoS dataset, 26,054 are split into train and 1,300 into test sets. In the case of video, a total of 40 images are used as input. At this time, if there are fewer than 40 images, 40 images are filled with duplicate sampling in accordance with the time order, and if there are more than 40, 40 images are obtained by uniform sampling. In the case of audio, the entire audio is used for learning, and random cropping is performed at this time. In the test, the same audio can be used in all tests by cropping and using the information-rich area of the audio. In the case of text, after tokenizing the description with a CLIP tokenizer, we crop 77 tokens from the front and use them similar to other CLIP-based models (Luo et al., 2022; Ma et al., 2022; Li et al., 2023). The batch size is 128 in the audio-video and 256 in the audio-text retrieval task, respectively. We conducted the experiment using 2 NVIDIA A100 80GB GPUs using the PyTorch library for 100 epochs, which took about 20 hours. 4.1.3 Results and Analysis To evaluate the performance of the proposed baseline model with our SoS dataset, we compare it with Wav2CLIP (Wu et al., 2022) for A2V and V2A, and CLAP (Elizalde et al., 2023) for A2T and T2A. Both Wav2CLIP and CLAP are fine-tuned to our SoS dataset from their pretrained weights. For the evaluation metrics, we utilize Recall@1(R@1), Recall@5(R@5), Recall@10(R@10), Mean Rank(Mean R), and Median Rank(Median R). Table 2 shows the performances for audio-to-video (A2V), video-toaudio (V2A), audio-to-text (A2T), and text-toaudio (T2A). For A2V and V2A, each story of the SoS dataset consists of fairly long image sequences by using clustering from the video which has an average length of 215 seconds, unlike other datasets. In addition, our audio dataset includes various sounds to express a story and thus can better represent storylines compared to audio composed of simple sounds. However, it also presents greater challenges. Therefore, compared to retrieving tasks using short videos and simple audio, it can be considered a challenging task. Nevertheless, with R@1 of 7.615 and 6.462 for both A2V and V2A, respectively, our baseline model performed better than its comparative model, Wav2CLIP, and showed reasonable evaluation results. In addition, our text data consists of a storyline that implicitly represents the whole sequence, not a description of the scene, so it is challenging because the model has to understand the context of the scenes rather than a simpler understanding of the scene. Therefore, looking at the R@1 performance 7.462 and 7.077 in A2T and T2A, respectively, the performance of our proposed baseline model is better than the comparison model CLAP. Also, the results of retrievals can be found in our supplementary or GitHub link 3 . 4.2 Audio Generation Task Composing and arranging background music and sound effects that exquisitely match the current scene’s story is a highly challenging task that even human creators struggle (Thom, 1999). We propose an audio generation model called SoSGen, which 3 https://github.com/Sosdatasets/SoS_Dataset is designed to generate appropriate audio for the movie scene, given the scene’s descriptions, frames, or both as the condition. 4.2.1 Audio Generation Model SoSgen takes advantage of the architecture of diffusion models (Sohl-Dickstein et al., 2015), which learn a data distribution through gradual denoising. Stable diffusion models (Rombach et al., 2022), a variant of diffusion models, enable effective model conditioning by training the model to learn latent features. Following the same approach, we train SoSgen as a text-to-audio spectrogram diffusion model, using the following objective function: L = Ex,ϵ∼N (0,1),t,c ∥ϵ − ϵθ( ˜zt , t, c(d))∥ 2 2  (12) where c is a trainable CLIP text encoder and d is the SoS description. For image+text-to-audio training, we follow ControlNet (Zhang and Agrawala, 2023) architecture, a lightweight and powerful method that enables additional conditioning of a diffusion model by freezing the original model’s parameters. We freeze the text-to-audio model’s parameters and resume training using a similar objective function: L =Ex,ϵ∼N (0,1),t,c,cimg  ∥ϵ − ϵθ( ˜zt , t, c(d), cimg(f))∥ 2 2  (13) where cimg is an additional CLIP image encoder that embeds the SoS frame sequence f. 4.2.2 Implementation Details Audio in movies rarely remains consistent until the end of a single scene. Also, using long audio as an input or output for a deep learning model is burdening in terms of hardware capacity. Thus, we slice the audio data into a length of approximately 15 seconds. Since extracted frames partially reflect semantic changes within the scene, audio slicing was performed treating audio data between two extracted frames as a base unit. The unit audios are accumulated and concatenated until their cumulative duration exceeds 15 seconds, ensuring that each sliced audio is never longer than 15 seconds. Audios that are originally longer than 15 seconds are not sliced because it implies the absence of significant visual changes for the duration, suggesting a high probability of long, consistent audio. Sliced audio files containing only a little sound were eliminated using the PyDub4 library. 4 https://github.com/jiaaro/pydub To this end, we assessed the mean frequency of each segment and excluded any with a frequency below 78 Hz, a threshold determined through manual inspection. This process conducted 185,341 audio data of roughly 15 seconds each, which were then converted into spectrograms. From this, we randomly split the dataset into 184,462 and 879 as training and testing, respectively. For our base model, we leveraged Riffusion (Forsgren and Martiros, 2022), a checkpoint of Stable Diffusion v1-5 (Rombach et al., 2022), fine-tuned on pairs of music descriptions and spectrograms. Following the same approach, we finetuned the diffusion model from the Riffusion checkpoint with pairs of SoS descriptions and corresponding sliced audio spectrograms. In addition, we further trained the model in the setting of image and text conditioning, utilizing the ControlNet (Zhang and Agrawala, 2023) architecture. We use a sequence of frames extracted through clustering that correspond to the sliced audio as the image condition. SoStext+image was additionally trained from SoStext checkpoint, while SoSimage resumed from the Riffusion checkpoint and used a redundant text condition in both training and testing. The specific model architecture can be found in Figure 5. For training our models, we used a batch size of 1, 16 gradient accumulation steps, an initial learning rate of 1e-6, and a cosine learning rate scheduler. We fine-tuned the text-to-audio model for 406,000 steps and additional 525,000 steps to further train the text+image-to-audio model. Separately, the image-to-audio model was trained for 525,000 steps. We ran fine-tuning for 2 days and ControlNet training for another 2 days on a single NVIDIA GeForce RTX 3090 GPU. 4.2.3 Results and Analysis Baselines We compare SoSgen to two baselines: Riffusion (Forsgren and Martiros, 2022) and MUSICGEN (Copet et al., 2023), the state-of-the-art open source text-to-music generation model. The performance of the models is evaluated based on the Fréchet Audio Distance (FAD) (Kilgour et al., 2019). FAD is a reference-free evaluation metric that is computed using means and covariances of VGGish (Hershey et al., 2017) embeddings. We used a lightweight pytorch implementation5 of FAD for more efficient evaluation. 5 https://github.com/Sosdatasets/SoS_Dataset Denoising Diffusion 𝓔 𝓓 Prediction Ground Truth 𝓩𝑻 Latent Space Pixel Space 3.67cm 𝓩 𝓩𝑻 𝓩 Image+Text-to-Audio (Sequence of Frames) CLIP Text Encoder CLIP Image Encoder OR Description Word “Audio” Image-to-Audio (Redundant Text Condition) Text-to-Audio (SoS Description) 1.55 cm + ControlNet Figure 5: SoSgen architecture for training. The components and arrows drawn with dash lines are only used in the ControlNet-involved training, i.e. image-to-audio and image+text-to-audio. To train a text-to-audio model, we condition the denoising model with CLIP text embeddings of SoS descriptions. Once the text-to-audio model is sufficiently trained, we freeze the model and add image embeddings as a condition to train the text+image-to-audio model using ControlNet. Alternatively, an image-to-audio model can be trained from the Riffusion checkpoint, using a redundant text condition 'Audio' for each training data. E denotes the encoder, D denotes the decoder, ZT denotes latent representation at timestep T . Table 3: FAD (lower is better) of SoSgen and the baselines. SoSgentext+image is the only model given with the image and text condition. The rest are only given with SoS descriptions as the condition. Model F ADV GGish ↓ Riffusion 20.114 MUSICGEN 11.680 SoSgentext 10.935 SoSgenimage 18.324 SoSgentext+image 9.099 Table 3 shows the comparison of SoSgen against Riffusion and MUSICGEN. SoSgentext, Riffusion, and MUSICGEN generate an audio spectrogram given descriptions. For SoSgenimage and SoSgentext+image, an image sequence is also given as an additional condition. The results indicate that SoSgentext and SoSgentext+image outperform Riffusion and MUSICGEN in terms of FAD. Although SoSgenimage only shows a slight improvement from Riffusion, combining two modalities exceptionally enhances audio generation capability. The generated audio samples can be listened to in our GitHub link 6 and are also included in the supplementary material. The samples show SoSgen’s superior performance on story audio generation tasks. 6 https://github.com/Sosdatasets/SoS_Dataset 5 Conclusion In this paper, we emphasize that sound also conveys meaningful semantics of the story. Thus, we propose a novel dataset named Sound of Story (SoS) dataset, audio-image-text pair dataset. One main feature of our dataset is that all the linguistic information inside the audio is eliminated. We named the audio without any linguistic information as 'Background sound'. Background sounds can be utilized in various applications and we propose two related tasks, retrieval task and background sound generation task. To give an example of using it from videos such as movies and cartoons, background sound is often produced according to a designated scene using various props or objects for a sense of reality. In the case of the retrieval task, among the different sound sources made, it is possible to recommend the sound source which fits the best to the scene. Furthermore, background sound generation can produce more realistic audio while reducing human resource costs by directly generating the right audio for a given scene. To the best of our knowledge, it is the first attempt to do multimodal tasks with the speech-decoupled background sound relevant to the storyline. Limitations To use audio data in story understanding, there are some limitations to be solved. First, we create audio data by decoupling the speech to exclude lin- guistic information and preserving only pure audio. However, the generated audio may be yet distant from what people encounter in daily life because there is a possibility that various noises caused by the generation from the movie clip remain. Second, the audio-image-text pair is strongly connected, but it is not perfect due to the nature of the audio. Since audio includes a lot of subjective elements, various correct answers may exist for one story. To deal with these limitations, more strict and cautious policies to process not only audio but also image and text for story understanding should continue to be studied as future work.",
		"summary": "Storytelling is multi-modal in the real world. When one tells a story, one may use all of the visualizations and sounds along with the story itself. However, prior studies on storytelling datasets and tasks have paid little attention to sound even though sound also conveys meaningful semantics of the story. Therefore, we propose to extend story understanding and telling areas by establishing a new component called background sound which is story context-based audio without any linguistic information. For this purpose, we introduce a new dataset, called Sound of Story (SoS), which has paired image and text sequences with corresponding sound or background music for a story. To the best of our knowledge, this is the largest well-curated dataset for storytelling with sound. Our SoS dataset consists of 27,354 stories with 19.6 images per story and 984 hours of speech-decoupled audio such as background music and other sounds. As benchmark tasks for storytelling with sound and the dataset, we propose retrieval tasks between modalities, and audio generation tasks from image-text sequences, introducing strong baselines for them. We believe the proposed dataset and tasks may shed light on the multi-modal understanding of storytelling in terms of sound.",
		"id": "UUID20"
	},
	{
		"document": "1 INTRODUCTION With the rapid development of generative modeling, AI-driven music generation has become an emerging task that creates value for both research communities and the music industry. Pioneering works like Music Transformer (Huang et al., 2018) and MuseNet (Payne, 2019) operated on symbolic representations (Engel et al., 2017). Although capable of conditioning on textual description, their generated MIDI-style outputs tend to heavily depend on pre-defined virtual synthesizers, resulting in an unrealistic audio quality and limited diversity. More recent text-to-music approaches like MusicGen (Copet et al., 2023), MusicLM (Agostinelli et al., 2023), and Jen-1 (Li et al., 2023) have streamlined the procedure by by directly creating authentic audio waveforms based on textual prompts. This advancement enhances versatility and diversity without necessitating a deep understanding of music theory. Nonetheless, the results they produce consist of composite mixes rather than individual tracks (e.g., bass, drum, instrument, melody tracks), limiting fine-grained control in comparison to the creative processes employed by human composers. Additionally, their choice of instruments and musical styles is influenced by the data on which they were trained, occasionally leading to unconventional combinations. ∗ equal contribution 1 arXiv:2310.19180v1 [cs.SD] 29 Oct 2023 Preprint. The advent of multi-track recording technology has ushered in a new era of musical creativity, enabling composers to delve into intricate harmonies, melodies, and rhythms that go beyond what can be achieved with individual instruments (Zhu et al., 2020). Digital audio workstations provide artists with the means to expand their musical ideas without being constrained by temporal or spatial limitations. The wide range of available timbres grants composers greater freedom to explore their creative concepts. The practice of composing music one track at a time aligns well with the realworld workflows of musicians and producers. This approach allows for the iterative refinement of specific tracks, taking into consideration the impact of other tracks, thereby facilitating collaboration between humans and artificial intelligence (Frid et al., 2020). Nonetheless, creating separate models for diverse combinations of tracks comes with a prohibitively high cost. Our objective is to combine the flexibility of text-to-music generation with the control offered by multi-track modeling, in order to harmonize with versatile creative workflows. To this end, we develop a unified generative framework, namely JEN-1 Composer, to jointly model the marginal, conditional, and joint distributions over multi-track music using one single model. By extending off-shelf text-to-music diffusion models with minimal modification, our method fits all distributions simultaneously without extra training or inference overhead. To be specific, we make the following modifications to Jen-1 (Li et al., 2023): (a) We expand the input-output architecture to encompass latent representations for multiple music tracks. This expansion enables the model to capture relationships between these tracks. (b) We introduce timestep vectors to govern the generation of each individual track. This inclusion provides flexibility for conditional generation, allowing for fine-grained control. (c) Special prompt tokens have been added to indicate specific generation tasks, reducing ambiguity and enhancing the model’s performance. In addition, we propose a curriculum training strategy to progressively train the model on increasingly challenging tasks. This training regimen begins with generating a single track, then advances to handling multiple tracks, and ultimately culminates in the generation of diverse combinations of multiple music tracks. On the other hand, current models lack the flexibility necessary for users to easily incorporate their artistic preferences into the music generation process. We contend that a more seamless integration of human creativity and AI capabilities can enhance music composition. To accomplish this, we propose the implementation of a Human-AI co-composition workflow during the model’s inference phase. As illustrated in Figure 1, producers and artists collaboratively curate and blend AI-generated tracks to realize their creative visions. More specifically, our model enables the generation of tracks based on both textual prompts and satisfactory audio segments from previous iterations. Through selective re-generation guided by feedback, users can engage in an iterative collaboration with the AI until all tracks meet their desired standards. This approach complements individual artistic imagination with the generative power of AI, offering precise control tailored to individual preferences. Our evaluations demonstrate that JEN-1 Composer excels in generating a wide range of track combinations with state-of-the-art quality and flexibility. To summarize, the contributions of this work are four-fold: 1. For the first time, we introduce an innovative workflow for collaborative music generation involving both humans and AI. This workflow is designed for the iterative creation of multitrack music. 2. We present JEN-1 Composer, a unified framework that effectively models marginal, conditional, and joint probability distributions for generating multi-track music. 3. We design an intuitive curriculum training strategy to enhance the model capacity by progressively reducing the required conditioning music information. 4. Through quantitative and qualitative assessments, we demonstrate that JEN-1 Composer achieves state-of-the-art quality and alignment in generating conditional multi-track music. 2 RELATED WORK In this section, we introduce the research background related to this work from two aspects. First, we discuss the technical advances in conditional music generation. Then, we review key works in the field of multi-track music generation. 2 Preprint. JENCOMPOSER Prompt(s): genre, style, instruments, tags, bpm, etc. Huamn Feedback Interactive Selection & Editing selected tracks (optional) Generated Multi-Track Music Mix&Arrange Output Bass Drum Instrument Melody yes to select no to re-generate Figure 1: The Human-AI co-composition workflow of JEN-1 Composer. JEN-1 Composer generates multiple music tracks conditioned on two forms of human feedback: 1) text prompts indicating desired genres, eras, rhythms etc., and 2) iterative selection/editing of satisfactory track subsets from previous generations. The selected subsets can serve as conditional signals to guide JEN-1 Composer in generating remaining tracks, ensuring contextual consistency between different tracks. This collaborative loop of human curation and AI generation is repeated until all tracks are deemed satisfactory. Finally, the tracks are mixed into a complete cohesive musical piece. 2.1 CONDITIONAL MUSIC GENERATION In the field of music generation, conditional models have been widely applied for various tasks. Based on the temporal alignment of the conditioning signals, they can be categorized into two types: One uses low-level control signals that are highly aligned with the audio output, such as lyrics (Yu et al., 2021) and MIDI sequences (Muhamed et al., 2021). The other leverages high-level semantic representations like text (Kreuk et al., 2022; Agostinelli et al., 2023; Liu et al., 2023) or images (Huang et al., 2023b) to provide overall coherence without tight alignment. Considering the scarcity of aligned data, many models adopt self-supervised training (Marafioti et al., 2019; Borsos et al., 2023) to improve generalization. Another challenge is the high complexity of raw waveforms (Garbacea et al., 2019), making direct generation intractable. Thus, various feature extraction ˆ and representation techniques have been extensively studied, such as VQ-VAE/GAN-based methods using mel-spectrograms (Van Den Oord et al., 2017; Creswell et al., 2018; Huang et al., 2023a), and quantization-based methods that convert waveforms into more compact discrete representations (Zeghidour et al., 2021; Defossez et al., 2022). Recently, non-autoregressive generation based ´ on diffusion models (Ho et al., 2020) has emerged as a promising approach, where MeLoDy (Lam et al., 2023) and Jen-1 (Li et al., 2023) achieve high-fidelity music generation. Our proposed JEN-1 Composer follows this line of work, but differs from existing conditional music generation models in two aspects. In addition to textual prompts that provide high-level control over global styles, our model utilizes cross-track dependencies as an additional type of tight alignment conditioning. Specifically, we generate missing music tracks conditioned on any given combination of existing tracks, exploiting their temporal alignments to improve harmony and mixing. Furthermore, instead of directly producing a full mix, our generation target is separated music tracks, aligned with realworld music production workflows for track-wise editing and creation. The vectorized timesteps and prompt prefixes guide the model to generate each track conditioned on others, facilitating iterative refinement and Human-AI collaborative creation. In summary, our model explores multi-level conditioning signals and targets separated tracks for enhanced coherence, controllability and creativity. 2.2 MULTI-TRACK MUSIC GENERATION Multi-track music generation is an emerging research direction. Pioneering works have explored multi-track symbolic music generation with various generative models. MuseGAN (Dong et al., 2018) presented one of the first attempts using GANs, though the results were limited in diversity. Subsequent works explore more powerful generator architectures. For instance, MIDISandwich2 (Liang et al., 2019) adopted hierarchical RNNs and VAEs to model long-term track dependencies. MMM (Ens & Pasquier, 2020) and MTMG (Jin et al., 2020) incorporated the trans3 Preprint. former’s attention mechanisms. More recent approaches like MTT-GAN (Jin et al., 2022) combined GANs and transformers to produce multi-track music that conforms to the music rules. Unlike these approaches, we propose a diffusion model for multi-track generation. Compared to VAEs, GANs and transformers, diffusion models have shown superior performance on generative modeling of music (Kong et al., 2020; Liu et al., 2023). Moreover, we use source separation tools like Spleeter (Hennequin et al., 2020) and Demucs (Defossez, 2021; Rouard et al., 2023) to massively ´ augment our waveform training data instead of relying on professional MIDI annotations, greatly reducing the data acquisition difficulty. In addition, we do not explicitly incorporate music theory modeling. This provides more creative freedom without confining the results to a single style. We believe this simpler framework can empower users to compose multi-track music in a more intuitive and unrestrained way. Our model JEN-1 Composer learns the inter-track dependencies and relationships in an implicit manner from the training data. By operating directly on raw audio and empowering unrestrained generation, JEN-1 Composer explores the possibility of multi-track music creation beyond the boundaries set by existing MIDI-based methods and music rules. 3 PRELIMINARY 3.1 DIFFUSION MODEL Diffusion models (Ho et al., 2020) are a type of generative model that can generate high-quality samples via iterative denoising. A noise prediction model parameterized by θ takes the timestep t and the corrupted sample xt as input. It is trained to estimate the conditional expectation E [ϵt|xt] by minimizing the following regression loss: min θ Et,x0,ϵt ∥ϵt − ϵθ (xt, t)∥ 2 2 (1) where t is uniformly sampled from {1, 2, . . . , T} and ϵt is the injected standard Gaussian noise that perturbs the original data x0 as: xt = √ α¯tx0 + √ 1 − α¯tϵt (2) Here, α¯t = Qt i=1 αi , αt = 1 − βt, and βt denotes the noise schedule controlling the noise levels over time. With an optimized noise predictor, we can reversely approximate x0 by sampling from a Gaussian model p (xt−1|xt) = N  xt−1|µt (xt), σ2 t I  in a stepwise manner (Bao et al., 2023), where the optimal mean under maximal likelihood estimation is: µ ∗ t (xt) = 1 √ αt  xt − βt √ 1 − α¯t ϵθ (xt, t)  (3) 3.2 AUDIO LATENT REPRESENTATION Directly modeling raw audio waveforms is intractable due to high dimensionality, where x ∈ R c×s represents the waveform with c channels and s being the sequence length. To obtain a more compact representation, we first encode x into the latent space z ∈ R d×ˆs using a pretrained autoencoder, where ˆs ≪ s is the compressed sequence length and d is the latent dimension: z = fϕ(x), xb = gψ(z) (4) Here fϕ and gψ denote the encoder and decoder networks respectively. By compressing the original high-dimensional waveform x into the lower-dimensional latent variable z, we obtain a more compact and tractable representation for subsequent processing. In this work, we pretrain our own autoencoder model for audio reconstruction, following the Jen-1 architecture proposed in Li et al. (2023). While other external pre-trained models like SoundStream (Zeghidour et al., 2021) and EnCodec (Defossez et al., 2022) could also be compatible, we do not test them in this paper. The ´ diffusion process operates on the latent space. 4 METHOD In this section, we introduce the proposed methodology of JEN-1 Composer for flexible multitrack music generation. We first describe the key modifications to the Jen-1 model architecture in Section 4.1. This is followed by the curriculum training strategy in Section 4.3 and the interactive inference approach in Section 4.4.  Preprint. Figure 2: Illustration depicting the three distinct modes employed by JEN-1 Composer to generate one of the multi-track variables x1. These modes encompass marginal, conditional, and joint generation. By introducing distinct noise perturbations into their respective input tracks, our JEN-1 Composer learns the art of reconstructing and generating clean tracks across diverse settings. Specifically, for marginal generation, we introduce noise with a maximum timestep of T into all other tracks. In contrast, for conditional generation, we maintain a noise-adding level of 0 and utilize the original or generated audio from all other tracks as the condition. In the case of joint generation, all noise is independently sampled. Note that here we only present a scenario involving two tracks in this illustration for simplicity. 4.1 MULTI-TRACK MUSIC GENERATION To enable JEN-1 Composer to handle multi-track input and output for joint modeling, we make minimal extensions to its original single-track architecture. As elaborated below, the input-output representation, timestep vectors, and prompt prefixes are adapted to fit multi-track distributions efficiently using a single model. 4.1.1 MULTI-TRACK INPUT-OUTPUT REPRESENTATION We extend the single-track input x ∈ R c×s of Jen-1 to multi-track inputs X =  x 1 , x 2 , . . . , x k  , where x i ∈ R c×s is the waveform for the i-th track and k is the total number of tracks. The waveform of each track x i is encoded into the latent space using the pretrained encoder fϕ, namely z i = fϕ(x i ) ∈ R d×ˆs . The input tracks are concatenated along the channel dimension to form the final input Z ∈ R kd×ˆs . Correspondingly, the single-track output in Jen-1 is expanded to kd channels, then producing separate waveforms for k tracks. Extending the input-output representation to multi-track allows explicitly modeling the inter-dependencies and consistency between different tracks, which is essential for high-quality multi-track generation but lacking in single-track models. The concatenated latent representations align the structure with the multi-track waveform outputs, enabling synchronized generation across tracks. Modeling relationships among tracks also facilitates generating certain tracks conditioned on others, a key capability in flexible music creation workflows. 4.1.2 INDIVIDUAL TIMESTEP VECTORS Along with the expanded input-output structure, we introduce separate timesteps for each track to gain fine-grained control over the generation process. To be specific, the scalar timestep t in the original Jen-1 is extended to a multi-dimensional timestep vector [t1, . . . , tk], where each element ti ∈ {0, 1, . . . , T} corresponds to the noise level for the i-th track. In particular, ti = 0 indicates the i-th track is given as conditional input without noise. ti > 0 means the corresponding track needs to be generated by the model based on the conditional tracks. ti = T represents the maximum noise level that cannot provide any conditioning signal. As shown in Figure 2, by controlling the timestep vectors, our model can flexibly specify the tracks to reconstruct or generate for a given input, avoiding the need to retrain models for every combination of conditional tracks. This greatly improves the flexibility and reduces the training overhead. Varying timesteps for different tracks 5 Preprint. also allows controlling the noise levels independently, making the model adaptive to more diverse generation tasks. 4.2 INTEGRATING TASK TOKENS AS PREFIX PROMPTS In addition to the conventional text prompts describing the music content and style, we incorporate task-specific tokens as prefixes to guide the generation process. These task tokens serve as explicit directives for the model, offering clear instructions regarding the current generation task, akin to the use of text prompts for controlling musical style. By utilizing these task-specific prefixes, we enhance the model’s capability to focus its generative efforts on producing content that aligns with the specified task, thus reducing ambiguity and elevating the quality of output. To illustrate this concept, consider the utilization of prompt prefixes such as “[bass & drum generation]”. These prefixes effectively communicate to the model the immediate generation objective, in this case, the generation of bass and drum tracks. This explicit task signaling enables the model to concentrate its generative capacity on crafting these missing tracks while taking into account the existing conditional tracks. Through the integration of task-specific prefixes, accompanied by enhanced individual timestep vectors, our proposed JEN-1 Composer demonstrates a remarkable capacity to efficiently model the marginal, conditional, and joint probability distributions associated with the various tracks. All these tasks are addressed within a single, unified model, a testament to the versatility and adaptability of our approach in handling multifaceted generative challenges. 4.3 PROGRESSIVE CURRICULUM TRAINING STRATEGY We propose a curriculum training strategy to progressively enhance the model’s capability in modeling joint and conditional distributions over k tracks. The strategy starts by reconstructing audio with only one missing track. It then steadily increases the number of tracks to be generated in each training step, thus enhancing the difficulty. Critically, instead of completely replacing easier stages, we gradually increase the probability that more challenging stages are selected during training. All stages, representing tasks with varying difficulties, are trained with designated probabilities. In this manner, the model is steadily presented with more difficult modeling tasks, while continually being trained on simpler tasks to avoid forgetting. The schedule consists of k stages: • Stage 1: Reconstruct 1 random track out of k per step, with k−1 tracks given as conditional inputs. • Stage 2: Generate 2 random tracks out of k per step, conditioned on the other k − 2 tracks. • ... • Stage k: Free generation of all k tracks without any conditional tracks. This curriculum not only ensures the model learns basic reconstruction skills but also gently enhances its capacity in coordinating more tracks simultaneously. By incrementally growing the task difficulty, it prevents the model from overfitting simple cases while forgetting more complex generation behaviors, a common issue in conventional training. The progressive schedule allows smooth transitioning of the model from reconstructing existing combinations to freely imagining novel mixtures of tracks. 4.4 INTERACTIVE HUMAN-AI CO-COMPOSITION WORKFLOW During inference, our model supports conditional generation of multiple tracks given 0 to k−1 tracks as input conditions. To enable Human-AI collaborative music creation, we devise an interactive generation procedure, detailed in Algorithm 1. The proposed interactive inference approach seamlessly combines human creativity with AI capabilities to enable collaborative music generation. During the iterative process, humans can focus on improvising particular tracks that pique their interest, while maintaining harmony and consistency with the overall generation guided by the model. This complementary Human-AI workflow is aligned with real-world music composition practices, and provides the following benefits: 6 Preprint. Algorithm 1 Interactive Human-AI Co-composition Workflow User provides a text prompt p Model generates k-track audio (xˆ 1 , xˆ 2 , . . . , xˆ k ) conditioned on p User selects satisfactory track subset S repeat Model generates tracks conditioned on S and p User selects satisfactory tracks from generation and adds them to S until all k tracks selected • It allows progressively layering and polishing each track with a closed-loop human feedback mechanism, facilitating nuanced refinement difficult for pure AI generation. • With humans picking satisfactory samples at each iteration, it helps filter out low-quality samples and steer the generation towards desirable directions. • By interacting with human creators and incorporating their inputs, the model can keep improving its understanding of human aesthetic preferences and sound quality standards. • The generation can leverage both human ingenuity and AI capabilities. Humans excel at creative improvisation while AI provides helpful cues to ensure coherence and promptconsistency. • The collaborative experience enhances engagement and sense of control for human producers. It enables realizing their creative visions through an AI assistant. In summary, the interactive inference paradigm organically couples human creativity with AI generation to enable enhanced music co-creation. It balances open-ended improvisation and overall structural coherence, combining the strengths of both to take music generation to the next level. 5 EXPERIMENT 5.1 SETUP Datasets. We employ a private studio recording dataset containing 800 hours of high-quality multitrack audio data to train JEN-1 Composer. The dataset consists of 5 types of audio tracks that are temporally aligned, including bass, drums, instrument, melody, and the final mixed composition. All tracks are annotated with unified metadata tags describing the genre, instruments, moods/themes, tempo, etc. To construct the training and test sets, we first randomly split the dataset into a 4:1 ratio. We then extract aligned segment snippets from the 5 tracks using the same start and end times to preserve temporal consistency. This process ensures the multi-track snippets in our dataset are temporally synchronized for training the model to learn cross-track dependencies and consistency. The training set encompasses 640 hours of audio data, spanning a diverse array of musical styles and instrumentation. In contrast, the remaining test set comprises 160 hours of audio, serving as the basis for evaluating the model’s ability to generalize. With the presence of comprehensive annotations and temporal alignment, our dataset plays a pivotal role in training JEN-1 Composer. It equips the model with the capability to generate high-quality multi-track music in response to text prompts that convey desired attributes. Evaluation Metrics. We have conducted a comprehensive evaluation of our methodology, encompassing both quantitative and qualitative dimensions. For quantitative metrics, we adopt the CLAP score (Elizalde et al., 2023) to measure the alignment between text and music tracks. More specifically, we have computed CLAP scores for both the mixed track and each individual separated track. In the case of JEN-1 Composer, we have simply summed the four generated tracks to derive the mixed track and subsequently computed the Mixed CLAP score. For state-of-the-art models that directly generate mixed audio, we adopt Demucs (Defossez, 2021; Rouard et al., 2023) to separate ´ the mixed tracks prior to calculating per-track CLAP scores. For qualitative analysis, we employ a Relative Preference Ratio (RPR) from human evaluation to assess the quality of mixed audio generated by different models. Specifically, we generated samples from various models in response to text prompts, and had multiple human raters compare these sample pairs, recording the percentage of times a model’s generation was preferred over JEN-1 Composer’s mix. A higher RPR (ranging from 7 Preprint. Table 1: Multi-track text-to-music generation. We compare objective and subjective metrics for JEN-1 Composer against a number of state-of-the-art baselines. We utilize the open-source model whenever feasible, and for MusicLM, we rely on the publicly accessible API. CLAP↑ RPR↑ METHODS BASS DRUMS INSTRUMENT MELODY MIXED MIXED MusicLM 0.16 0.17 0.23 0.28 0.28 27% MusicGen 0.17 0.15 0.25 0.33 0.35 36% JEN-1 0.19 0.16 0.29 0.32 0.36 40% JEN-1 Composer 0.21 0.18 0.29 0.36 0.39 − 0% to 100%) indicates a stronger preference for a given model over JEN-1 Composer’s mix. Our evaluation process emphasized aspects including coherence, logical consistency, and smoothness of quality across the generated tracks. Implementation Details. Our multi-track music generation task encompasses four distinct tracks: bass, drums, instrument, and melody, as well as the composite mixed track. All audio data are highfidelity stereo audio sampled at a rate of 48 kHz. Specifically, we employ a hop size of 320 to encode the audio, resulting in a latent space representation of 150 frames per second, each comprising 128 dimensions. The intermediate dimension within the cross-attention layers is configured to be 1024. Prior to compression into the latent space, we adjust the volumes of individual tracks by scaling them in accordance with the mixing volumes, ensuring that their relative loudness remains consistent. Semantic understanding of the text prompts is achieved through the utilization of the pre-trained FLAN-T5 model (Chung et al., 2022). Regarding model architecture, we make minimal modifications to Jen-1 (Li et al., 2023). As described in Section 4.1, the primary changes pertain to the input-output handling, where we concatenate the four tracks in a channel-wise manner. These tracks collectively share a 1D UNet backbone (Ronneberger et al., 2015). The single-track timestep is expanded into a timestep vector, allowing the addition of varied noise levels to each track. In the training process, for each batch, we first uniformly sample one of the four tracks at random, then assign it a non-zero timestep ti , sampled from {1, . . . , T − 1}, which determines the strength of Gaussian noise injected into the track’s latent embedding. The timesteps and noise levels for the other three tracks are stochastically drawn from {0, ti , T}. Specifically, a timestep of 0 represents a clean track, which serves as the conditional signal for guided generation. A timestep of T signifies maximum noise level, so this track does not provide conditional guidance and hence supports unconstrained generation from the marginal distribution. Lastly, a timestep of ti indicates that this track is jointly optimized as one of the generation targets together with the selected i-th track. This unified framework comprehensively covers all permutations of multi-track generation tasks. Additionally, we employ classifier-free guidance (Ho & Salimans, 2022) to enhance the correlation between generated tracks and text prompts. JEN-1 Composer is trained on two A100 GPUs, with other hyperparameters including AdamW optimizer (Loshchilov & Hutter, 2017), a linear decay learning rate initialized at 3e −5 , batch size of 12, β1 = 0.9, β2 = 0.95, weight decay of 0.1, and gradient clipping threshold of 0.7. 5.2 COMPARISON WITH STATE-OF-THE-ARTS To the best of our knowledge, our proposed JEN-1 Composer represents the first attempt to address the challenging task of authentic multi-track music generation. In this context, we undertake a comparative examination with other state-of-the-art text-to-music generation approaches, namely MusicLM (Agostinelli et al., 2023), MusicGen (Copet et al., 2023), and Jen-1 (Li et al., 2023). It is worth noting that all these methods are limited to generating single-track music with mixed attributes. As demonstrated in Table 1, JEN-1 Composer achieves superior performance over other state-of-the-art methods. Benefiting from its track-wise generation and flexible conditional modeling capabilities, JEN-1 Composer obtains consistently higher CLAP scores on each individual track, indicating stronger fine-grained control and alignment during multi-track generation. Consequently, the overall mixing and composition quality of JEN-1 Composer is markedly higher based on both human evaluation and quantitative metrics. Specifically, JEN-1 Composer outperforms other mod8 Preprint. Table 2: Ablation studies evaluation. Starting from the baseline, we incrementally modify the configuration to investigate the effect of each component. CLAP↑ RPR↑ METHODS BASS DRUMS INSTRUMENT MELODY MIXED MIXED baseline 0.20 0.18 0.20 0.28 0.28 16% + individual timestep vector 0.19 0.18 0.22 0.32 0.33 20% + curriculum training strategy 0.21 0.17 0.26 0.35 0.37 35% + interactive inference 0.21 0.18 0.29 0.36 0.39 − els by a substantial margin in the CLAP score of the mixed track, demonstrating its advantage in coherently coordinating different tracks guided by the text prompts. Meanwhile, the results on the RPR metric also show users’ strong preference towards mixes generated by JEN-1 Composer compared to other models. In summary, conditional multi-track generation enables JEN-1 Composer to achieve state-of-the-art performance and generate satisfying music aligned with the textual descriptions. The unified modeling approach provides an elegant solution for controlling inter-track relationships. 5.3 ABLATION STUDIES We have conducted ablation studies to ascertain the effectiveness of key components within JEN-1 Composer. The findings, detailed in Table 2, originate from an initial vanilla baseline model featuring a four-track input/output structure inspired by Jen-1. We then progressively add the proposed techniques row by row. First, using individual timestep vectors for each track is crucial for modeling marginal and conditional distributions, instead of only joint distribution in the baseline. This leads to substantially higher CLAP scores on individual tracks. Second, the curriculum training strategy facilitates a smooth transition from learning simple conditional models to complex joint generation, further improving results, especially on challenging tracks like melody and instrument. Finally, interactively combining with the Human-AI co-composition workflow yields the best mixing quality, as the model can flexibly switch between modes with multiple injections of human preference. The extra conditional signals from feedback guide the model to overcome weaknesses and generate high-quality results for all tracks. For example, it can first generate drums and bass, then leverage the conditional distribution to produce satisfactory melody and instrument conditioned on them. In summary, benefiting from the dedicated design, JEN-1 Composer boasts flexibility in fine-grained conditional control and achieves promising generation quality for multi-track music synthesis. 6 CONCLUSION In this study, we introduce JEN-1 Composer, a comprehensive framework for multi-track music generation that harnesses the capabilities of diffusion models. This framework extends the single-track architecture of Jen-1, enabling efficient handling of marginal, joint, and conditional distributions across multiple tracks within a unified model. Moreover, we propose a curriculum training strategy designed to promote stable training, progressing from basic reconstruction to unconstrained composition. Notably, our work also presents a novel interactive Human-AI co-composition workflow. Comprehensive evaluations, including quantitative metrics and human assessments, demonstrate its exceptional performance in high-fidelity music generation while offering versatile control over the creative process. Although our generative modeling of JEN-1 Composer has made significant advances, limitations remain, particularly in its ability to produce audio that meets specific aesthetic and music theory directives compared to professional music production. Truly realizing AI-aided music creativity necessitates deeper collaboration between engineering, design, and art to create intuitive HumanAI co-creation interfaces and experiences. Moving forward, we are enthusiastic about exploring this landscape and jointly developing innovative techniques and workflows to unlock the creative potential of human-machine partnerships. By enhancing the connections between technology and artistry, we envision AI as an inspiring collaborator for limitless musical creativity.",
		"summary": "With rapid advances in generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models exhibit strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. JEN-1 Composer framework exhibits the capacity to seamlessly incorporate any diffusion-based music generation system, e.g. Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible generation of multi-track combinations. During the inference, users have the ability to iteratively produce and choose music tracks that meet their preferences, subsequently creating an entire musical composition incrementally following the proposed Human-AI co-composition workflow. Quantitative and qualitative assessments demonstrate state-of-the-art performance in controllable and high-fidelity multi-track music synthesis. The proposed JEN-1 Composer represents a significant advance toward interactive AI-facilitated music creation and composition. ",
		"id": "UUID21"
	},
	{
		"document": "1. Introduction Representing the latest breakthrough in Artificial Intelligence (AI), large-scale Foundation Models (FoMos) are capable of performing tasks in the domain of human intelligence, including producing original songs, poems, essays, digital photos, and drawings at professional levels. In particular, ChatGPT, a prevalent FoMo focusing on natural language processing, has excelled in career exams designed for people, e.g., performing in the 90th percentile in the American Uniform Bar Examination. Furthermore, multi-modal FoMos incorporating sensing, speech and vision are under active development to control robots and navigate drones and vehicles. The emergence of FoMos' capabilities is the joint effect of four factors: 1) an enormous model size (e.g., 175 billion parameters for GPT-3), 2) billions of self-supervised training runs, 3) astonishing quantities of high-quality unlabelled data collected across the entire Internet, and 4) an attention network to efficiently learn the relations between words and concepts [1, 2]. The promise of FoMos attracts tech giants like Alphabet, Amazon, and Nvidia to invest heavily in training their own FoMos for incorporation into their services. In the area of mobile networks, researchers are also inspired to deploy FoMos in the sixth-generation (6G) mobile networks to automate tasks of mobile devices such as human semantic communications, personal assistants, auto-pilot, and robotic control [3]. 1 The authors are with Dept. of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong. Contact: K. Huang (email: huangkb@eee.hku.hk). 2 FoMos pre-trained on generic data are required to be fine-tuned based on specific tasks to maximize the models' effectiveness, e.g., personalized conversation by learning a user's background, personality, and life habits. Training a FoMo to a specific task, however, requires enormous computation resources and storage space. For example, 1.3 gigawatt-hours of electricity and USD 4.6 million were spent on training ChatGPT-3. A more practical approach is called parameter-efficient fine-tuning (PEFT), referring to a class of techniques that vary only a small fraction of model weights or architecture to achieve maximum task effectiveness. A recent study by OpenAI, the company that created ChatGPT, reveals that fine-tuned FoMos are capable of undertaking 19,000 tasks involving 1,016 occupations. Research on PEFT leads to a fast-growing area with rich literature (§ 2). In mobile networks, on-device execution of FoMos is impractical but they can be deployed at edge servers to allow low-latency remote access. This is closely aligned with one key mission of 6G to realize an intelligent network edge featuring ubiquitous AI and learning as a platform for supporting next-generation mobile applications [4]. In return, FoMos at the edge can also benefit from real-time mobile data in proximity. This helps to overcome the limit of training data that stymies their continued improvement as the data in the public domain have been exhausted. FoMos at the 6G edge is embraced as a platform for automating a broad range of IoT applications such as elderly care, personal assistant, robotic control, auto pilot, and augmented reality. To materialize the vision, it is proposed in this article that FoMo fine-tuning is provided as a 6G service (§ 3). Wireless communication in 6G is shifting from generic rate-centric designs towards goal-oriented communications designed using a communication-computing integrated approach. Aligned with this paradigm shift, we further design a novel framework called device-edge fine tuning (DEFT) to provide goal-oriented wireless techniques for supporting efficient, privacy-preserving device-edge cooperation in fine-tuning FoMos. The framework comprises a set of novel DEFT techniques summarized as follows. ! The control of fine-tuning parameter size in different transformer blocks of a FoMo to cope with devices' resources constraints in DEFT (§ 4.1); ! Over-the-air computation for realizing neural connections in DEFT with a split FoMo (§ 4.2); ! Federated DEFT in a multi-device system via downloading of either a FoMo emulator or gradients (§ 5.1); ! On-the-fly prompt-ensemble tuning based on a boosting approach (§ 5.2); ! Device-to-Device (D2D) prompt transfer among devices with similar tasks (§ 5.3). We have also conducted experiments of fining-tuning pre-trained FoMos, one called Megatron-11B, which consists of around 11 billion parameters, and another one is T5-Base with 200 million parameters, on a Nvidia A100 server to demonstrate the effectiveness of DEFT (§ 6). 2. State-of-the-Art Techniques for PEFT The prevalent Transformer architecture adopted for FoMo pre-training is shown in Fig. 1, which can be fine-tuned using one or multiple of the following PEFT techniques. 3 Fig. 1. State-of-the-Art PEFT techniques. 2.1 Soft Prompts A prompt refers to input information that can help a FoMo understand the current context or task so as to interact with or assist a user more effectively [1]. In contrast with a handcrafted prompt, a soft prompt is a variable matrix representing the tunable parameters. It is either input into a pre-trained FoMo together with input embeddings or appended to attention matrices in each Transformer block of the FoMo, corresponding to two techniques, P-tuning [see Fig. 1(a)] and Prefix-tuning [see Fig. 1(b)], respectively. 1) P-tuning: A soft prompt is generated using a lightweight encoder, typically a bidirectional longshort memory network (LSTM) with two successive multilayer perceptron (MLP), with a chosen initial prompt as the input [5]. LSTM is used because it can associate different elements ('tokens') in the soft prompt. Then, the fine-tuning process is to train the encoder for a specific task via backpropagated gradient descent. Its completion yields the desired soft prompt that is attached to every instance of input embeddings in subsequent task execution while the encoder can be discarded. 2) Prefix-tuning: Both the key and value attention matrices in each FoMo transformer block are attached with prefix matrices (i.e., soft prompts) [6]. The prefixes are then tuned using an MLP for a given task to overcome the instability faced in direct optimization. Prefix-tuning is demonstrated to achieve comparable text-generating performance as full-model fine-tuning by optimizing only around 1% parameters. Its performance is highly dependent on the prefix length. 2.2 Reparameterization Methods Fine-tuning a large language model towards a downstream NLP task is demonstrated to be performed only in an extremely low-dimensional parametric subspace [7]. Inspired by this fact, the essential idea of the reparameterization-based techniques is to approximate the weight matrix of each block of a fine-tuned FoMo by the superposition of the original counterpart with a tunable extremely low-rank !! !' !# Add & Layer norm Feed forward ×' Input text Embedding matrix Hidden states Attention Input embeddings Add & Layer norm ' # (a) $ (b) (c) (d) ⊕ ⊕ $ !! !# Hidden states # &! &# : (a) Prompt tuning Prompt encoder Virtual prompt Input embeddings (b) Prefix tuning (c) Reparameterization (d) Adapter '' ' '$ # !%& Act !'()* Feed forward Feed forward : FoMo Architecture Insertion of Fine-tuning Parameters 4 matrix, thereby reducing the computation overhead of fine-tuning. The principle can be implemented by adding an additional data path with low-rank weight matrices to each FoMo block as illustrated in Fig. 1(c). There exist three common methods for constructing a low-rank matrix, called a fine-tuning matrix: 1) Fastfood transformation: a fine-tuning matrix is the product of several pre-defined, randomly generated matrices and a low-dimensional tunable matrix. 2) Low-rank approximation: a fine-tuning matrix is given by the product of two low-rank matrices with preset ranks. 3) Kronecker product: a fine-tuning matrix is given by the Kronecker product of several low-dimensional components. 2.3 Adapters and Selective Parameters An adapter refers to a lightweight, tunable neural network consisting of only several layers. Then, an adapter-based PEFT technique inserts an adapter into the original FoMo architecture and tunes the adapter to a specific task by applying the back-propagated gradient descent to the modified architecture. The originally proposed adapter comprises two feedforward projection layers sandwiching a nonlinearity activation layer [8]. The initial design has been substantially generalized to have a variable number of layers of different types and a flexible location in the backbone neural network. Instead of inserting an adapter, a selective-parameter technique is simpler as it fine-tunes a small subset of layers (or a small subset of parameters) of the original FoMo while freezing the rest. For example, the fine-tuned parameters could be the first few layers of a FoMo, the model's bias parameters, or specific rows of model weights [9]. Alternatively, a selective-parameter technique can make an unstructured selection of parameters in FoMos for sparse model updating. There exist a number of selection criteria such as the top-K rule or one based on optimizing a sparse binary mask for weight selection. 2.4 Hybrid Methods The PEFT techniques discussed are complementary to each other. Hence, combining multiple techniques (e.g., sparse adapter [9]) can further improve the fine-tuning performance. 3. PEFT as a 6G Service The advancement in general-purpose FoMo is believed to reaching its peak. The next phase of FoMo aims at developing task-oriented FoMos via fine-tuning or highly compact mobile FoMos. For instance, OpenAI offers an interface with ChatGPT to allow customers to fine-tune the FoMo according to their needs; Qualcomm demonstrates a sub-10 billion parameter FoMo for mobile devices; Google is developing a multi-modal FoMo targeting robotics. In view of this above trend and the broad range of IoT applications 6G aims to support, we propose that PEFT be provided as a service to mobile users to automate relevant tasks. As mentioned, FoMos can also benefit from their integration with mobile networks to overcome the data bottleneck and attain continued improvements. The PEFT services can be implemented in different ways. One is to store a large library of task-specific and thus highly compact FoMos in the central and edge clouds for the user to download according to their real-time demands [10]. This approach is limited by the availability of task-specific data to the cloud as a large portion is private data. The alternative implementation is the DEFT framework. 5 The basic principle of DEFT is to coordinate on-device and on-server computing to optimize finetuning parameters using local data and a FoMo operated by a server (or servers) based on stochastic gradient descent, as shown in Fig. 2. One approach is split learning that splits a FoMo with fine-tuning Fig. 2. Device-edge cooperative fine-tuning in 6G networks and relevant applications. parameters into device and server sub-models, which are conducted by air interface. This not only allows fine-tuning to leverage mobile computation resources but also helps to preserve user privacy by avoiding raw data uploading. Ideally, fine-tuning parameters should be kept on-device and updated using local data. For instance, considering the selective-layer PEFT techniques, the fine-tuned layers can be executed on a device with the remaining FoMo network executed by the server. Other DEFT approaches include uploading data embeddings and downloading a low-dimensional FoMo emulator. Several key challenges faced by PEFT implementation are described below and related to ! Mobile resource constraint: The performance of fine-tuned FoMo improves as the number of finetuning parameters and/or computation time and/or the amount of local data grows. However, the improvement is limited by the mobile constraints on computation resources and storage space. To overcome the limitation benefits from increasingly powerful mobile AI processors. The issue can also be addressed from the perspective of algorithmic design (e.g., see § 4.1). ! User privacy preservation: A naive approach is to employ an existing PEFT technique at an edge server to fine-tune a FoMo. However, the required uploading of users 'personal data would compromise their ownership. Therefore, data privacy preservation is an important factor to consider in designing DEFT (e.g., see § 5.1). Edge Server Autonomous driving Navigation Parking Communication Repair Intelligent guidance for autonomous driving. E-healthcare Heartbeat Blood oxygen Temperature Diagnosis Medical report and timely monitoring the patient. Remote robotic control Action Self-renewal Conversation Perception Environment understanding and commands response. Wireless link Wireless link Wireless link Mobile Devices and Applications Light-weight additive parameters at devices Frozen foundation models at servers/clouds Device-Edge Fine-tuning 6 ! Goal-oriented air interface: Each iteration in the DEFT process involves the device and server exchanging high-dimensional data, i.e., data embeddings, prompt, gradient, or parameters. The resultant communication bottleneck is exacerbated in the scenario of multi-device cooperation. This calls for the design of a goal-oriented air interface for DEFT, which integrates computing and communication to achieve high communication efficiency. We propose relevant techniques in §§ 4.2, 5.2, and 5.3. 4. Single-User DEFT 4.1 Fine-tuning Parameter Allocation for DEFT A PEFT technique (e.g., reparameterization methods) typically adds fine-tuning parameters to individual (transformer) blocks of a FoMo. In the context of DEFT, the communication overhead tends to increase rapidly as the parameter size grows. On the other hand, a larger size leads to a more significant improvement in downstream task performance. To boost the performance while reining in communication overhead, we propose to allocate fine-tuning parameters to individual blocks according to their importance levels under a constraint on the total parameter size. In contrast, in current techniques, the number of such parameters is kept uniform across blocks. The design is inspired by the observation that the model weights at different locations in a FoMo show different importance levels for fine-tuning. Considering the low-rank reparameterzation method, the importance of a fine-tuning matrix in a transformer can be measured by the magnitude of singular values of the matrix. Alternatively, the ablation method can be applied to evaluate the important of a fine-tuning component by measuring the training loss increment due to the removal of the component, which can be approximated by the gradient-weight product [11]. Considering parameterrepresentation based PEFT, the parameter allocation is reflected in controlling the ranks of low-rank reparameterization in different blocks. In the case of Prefix-tuning, the allocation involves controlling the prefix sizes of individual blocks. 4.2 Over-the-Air Computing for DEFT For DEFT implementing adapter-based fine-tuning, the adapter, which is a lightweight neural network, can be kept and updated on the device while the remaining FoMo executed at the server. The high-dimensional message exchange between the two network components places a heavy burden on the air interface. We propose to lift the burden by applying AirComp to realize neural connections over the air. Traditional AirComp techniques exploit the radio waves 'superposition principle to realize over-the-air signal aggregation in a distributed computing system, thereby solving the scalability problem in multi-access [12]. The novelty of the proposed design lies in the over-theair realization of matrix-vector multiplication, a common operation in neural connections. Thereby, turning interference into a computation mechanism avoids the need to transmit neuron outputs using many orthogonal channels. The specific design leverages precoding and post-equalization to convert a multi-input-multi-output (MIMO) channel into a desired weight matrix. Thereby, the transmission of an analog modulated signal vector results in the receiver directly receiving the matrix-vector product result. It is also possible for MIMO AirComp to support over-the-air non-linear activation components in MLP layers such as ReLU and Sigmoid. One approach is to approximate the activation functions into a set of linear AirComputable sub-functions. An alternative one can use a non-linear 7 detector at the receiver, whose parameters can be calibrated according to the characteristics of activation layers. Fig. 3. Federated device-edge fine-tuning. 5. Multi-User DEFT Involving multiple users to cooperate in fine-tuning has several advantages. First, leveraging training data distributed at users helps to meet the need for more data as required by fast few-shot fine-tuning. Second, the cooperation makes DEFT more efficient as a FoMo can be simultaneously fine-tuned to multiple downstream tasks. Last, the DEFT process can exploit distributed computation resources. These motivate us to propose three relevant techniques, namely federated DEFT, distributed prompt ensemble, and prompt transfer, in the following sub-sections. 5.1 Federated DEFT The federated DEFT coordinates multiple users with similar tasks to fine-tune a FoMo that serves all users simultaneously. The technique exploits distributed data while preserving their ownership by avoiding raw data uploading. This is realized by applying the well-known Federated Edge Learning (FEEL) that iteratively trains a global model at the server by efficiently utilizing personalized data at devices. Specifically, in each iteration, the edge devices compute gradient (or sub-models) based on their datasets and the server aggregates these immediate results to update (or assemble) a global model. This process is repeated until the global model converges. Building on FEEL, two federated DEFT paradigms are presented as follows. 1) Emulating FoMo for Local Fine-tuning: A FoMo is needed to compute gradients for updating finetuning parameters (e.g., adapter or prompt encoder) locally at a device but downloading the FoMo is infeasible as mentioned. A natural approach is to download a compressed version to act as an emulator of the original model. Researchers have developed sophisticated techniques for emulator compression that adopt knowledge distillation to drop transformer layers while making an attempt to maximize the approximation accuracy [13]. The federated learning framework of emulating FoMo is depicted in Fig. 3. Provisioned with an emulator, a device is then able to compute a stochastic gradient using its local dataset. However, the gradients deviate from the optimal one due to limited Edge server Edge devices Topology broadcast Emulator broadcast Local training Local Adapters upload Adapters aggregation Aggregated adaptors broadcast ×' Convergence … Protocol Adapter Aggregation Model Data Emulator Compress Emulator Generation Local data On-device local training Local data On-device local training Adaptors Local data On-device local training … 8 local data. This makes it necessary to aggregate local gradients across devices using the FEEL technique. Specifically, in each iteration of federated DEFT, all devices upload local gradients to a server for aggregation, which ten broadcasts the aggregated gradient to devices for updating finetuning parameters. The avoidance of uploading data embeddings gives the emulator-based technique the advantages of privacy preservation and relatively low communication overhead. FoMo emulation is still at its nascent stage and the state-of-the-art techniques can achieve a reduced model size to a fraction, say, 20%. This limits the application of emulator-based federated DEFT to small-scale FoMo (e.g., no more than 1 billion parameters) and high-end smartphones. This calls for the development of more efficient FoMo compression techniques. 2) Server FoMo Assisted Local Fine-tuning: This federated DEFT technique targets two PEFT techniques, P-tuning, and adapters (which are resided in the first few layers of a fine-tuned FoMo). Its essential idea is for the FoMo at a server to compute and provide a gradient to all devices for local updating of fine-tuning parameters, i.e., prompt encoder in the case of P-tuning or adapters. Consider P-tuning and a single iteration of federated DEFT has the following procedure. Specifically, a tunable prompt encoder is employed at devices to generate intermediate soft prompts that are appended to data embeddings [5]. The results are then efficiently uploaded (using e.g., AirComp) to the server for aggregation before input into the FoMo for aggregated gradient computation via back-propagation. In the case of the adaptor-based tuning, local adapter outputs, which result from input of local data embeddings, are uploaded and then aggregated instead. For other PEFT techniques, the implementation of the above federated DEFT techniques is possible in principle but more tedious as the fine-tuning parameters are distributed over different blocks of the FoMo [see prefix-tuning or parameter representation in Fig. 1(b)]. This calls for new intelligent designs. 5.2 On-the-Fly Prompt Ensemble Boosting Recent research findings show a prompt ensemble, which comprises a set of lightweight prompts, outperforms a single prompt [14]. This inspires us to propose the technique of on-the-fly boosting (FlyBoosting) to progressively broaden the range of solvable problems of a prompt ensemble, thereby realizing communication-and-computation efficient multi-device DEFT. Starting with an initial ensemble (prompt set) at the server, each iteration of FlyBoosting, which is based on a well-known approach called chain of thought, consists of three steps. First, each device tests the effectiveness of the current ensemble for the local task by asking the server to use it together with the FoMo to fill in the blanks in a set of reasoning paths and compares the answers with their group truth known locally. A simple example of a reasoning path is “Neymar plays for team ___; The team plays in city ___.” This allows each device to generate a local-effectiveness score for the ensemble. Second, a server selects those devices with low scores to generate new prompts, which better match their local task, for modifying the ensemble (element addition or replacement). As a result, the problem-solving range of the current ensemble is enhanced. FlyBoosting is communication efficient as it involves only a subset of devices to update the ensemble in each round. The above steps are repeated until all devices are satisfied with the ensemble with, e.g., their local-effectiveness scores exceeding a given threshold. Among others, designing goal-oriented communication techniques for FlyBoosting is a promising direction. One particular opportunity is importance-aware scheduling and resource allocation where the importance of a device is inversely proportional to its local-effectiveness score. The rationale is that a device with a lower score contributes more significantly to enhancing the ensemble’s capability. 9 The scheduler should balance the score and channel condition by using a metric combining the score with channel gain/rate. Fig. 4. D2D prompt transfer-and-fusion 5.3 Device-to-Device (D2D) Prompt Transfer-and-Fusion Consider a cluster of devices with pre-trained prompts (or prompt encoder models) and the admission into the cluster a new device, called target device with task relevance or similarity to some existing devices, called source devices. We propose D2D prompt transfer, a form of knowledge transfer, from source devices to the target device to reduce fine-tuning overhead. The system operations are illustrated in Fig. 4 and explained as follows. The underpinning method is interpolation of source prompts using a trainable attention modular to learn on measuring the contribution of each source model [15]. Interpolation has demonstrated effective even via simple model fusion, namely that the desired prediction in the target task can be performed by a combination of outputs of different prompt-encoder models at source devices. Prompt transfer leveraging peers’ assistance is faster and more efficient than fine-tuning from scratch. It can be implemented either in a distributed network with D2D links or a network with server coordination. From the perspective of air interface, the prompt interpolation and model fusion are essentially linear operations (e.g., weighted combination of source prompt matrices) and thus can be efficiently realized over the air using AirComp. 6. Experiments 6.1 Experiment settings We consider the employment of P-tuning techniques (§ 2.1) to fine-tune two prevalent pre-trained FoMos: 1) Megatron-11B with 11 billion parameters and 2) T5-base with 220 million parameters. To this end, a server with Nvidia A100 GPUs is employed. The wireless link connecting the device-server pair is modeled as a single-input-single-output Gaussian channel with a bandwidth of 20MHz. We consider both digital transmission and (analog) AirComp. Consider the digital-transmission case. Each transmitted coefficient is quantized into 8 bits and the communication rate set as the channel capacity that depends on the signal-to-noise-ratio (SNR). Moreover, multi-access is based on frequency division orthogonal access. For the case of AirComp, the symbol rate is 1/20MHz with each symbol modulated with one real coefficient using uncoded linear analog modulation. The settings for singledevice and multi-device DEFT are as follows. Server Source Devices Target device Local data … Attention module … Weights Pre-trained FoMo Backward propagation Prompt aggregation ! ! ' # ! $ $ Upload Gradients Weights broadcast 10 1) Single-device DEFT: The target downstream task is knowledge probing on the popular dataset of LAMA-29k, where the fine-tuned Megatron-11B is required to perform a cloze test that involves filling Fig. 5. Latency performance comparison between DEFT and centralized fine-tuning. a portion of masked text (see [14] for more details). The number of epochs is set as 5 by default, each of which comprises 91 iterations. The fine-tuning parameters for Megatron-11B are those of a ondevice prompt encoder that comprises around 132 million weights. The prompt encoder and dataset are owned by the edge device whose computation capability is set as 1/50 of the server's. 2) Multi-device DEFT: We consider D2D prompt transfer (§5.3) to obtain a target soft prompt consisting of about 2 million parameters. The target prompt is appended to a pre-trained T5-base to perform the well-known downstream task of GLUE (General Language Understanding Evaluation) (see [15] for more details). Each epoch comprises 115 iterations. The prompt fusion module (i.e., attention module) is operated by the server. In each iteration, for the purpose of prompt transfer, each source device uploads the element-wise product of their fixed prompts and the attention matrix broadcasted by the server. 6.2. Performance of Single-device DEFT We compare the performance of the proposed single-user DEFT with the centralized (server) finetuning. The former requires the device uploads its prompt embeddings in each iteration. On the contrary, the latter, a benchmarking scheme, requires the device to upload the raw data, i.e., LAMA29K. Reliable digital transmission is used for both schemes and thus the task-precision performance of their corresponding fine-tuned FoMos is identical. However, DEFT outperforms centralized tuning in terms of (end-to-end) latency. The latency consists of both computation and communication delays accumulated over the whole fine-tuning process. In Fig. 5, the curves of latency required versus SNR are plotted for both DEFT and centralized fine-tuning. From Fig. 5, DEFT is shown to substantially reduce the latency as opposed to the centralized tuning as prompt embeddings are much smaller in size than high-dimensional raw data. In particular, 5-time latency reduction is observed at SNR of 10 dB. The performance gap narrows as the SNR increases. The resultant higher communication rate helps centralized tuning significantly as communication latency dominates its computation counterpart due to a very powerful A100 server. In contrast, DEFT is less sensitive to the rate changes 2 4 6 8 10 12 14 16 18 20 10 20 30 40 50 60 11 since local computation for fine-tuning constitutes a bottleneck especially in the high SNR region. This is reflected in the saturation of the latency curve in the region. Fig. 6. Comparison of latency and task precision between DEFT using AirComp and digital transmission. 6.3. Performance of Multi-device DEFT Multi-device DEFT employs AirComp to realize ultra-fast over-the-air fusion of attention weighted prompts from source devices. It is benchmarked against the traditional scheme of frequency division orthogonal access for digital transmission. The curves of task precision (i.e., averaged task execution accuracy on the evaluation set) and communication latency (accumulated over the iterative finetuning process) and versus the number of fine-tuning epochs are plotted in Fig. 6 in solid and dashed lines, respectively. The SNR is set as 20dB. 1) Task precision of fine-tuned FoMo: For both schemes in comparison, the task precision is observed from Fig. 6 to rapidly increase in the first five epochs and then exhibit a plateau in the following epochs. For AirComp without coding, the exposure of transmitted prompts to channel noise degrades the precision with respect to its digital counterpart. The performance gap, however, narrows as the training progresses. Specifically, the precision of both schemes simultaneously reachs 90% after 20 epochs. This shows the iterative fine-tuning process together with multi-device averaging helps to rein in the effect of channel distortion caused by AirComp without coding. 2) Communication latency performance: For both schemes, the communication latency is incurred by the transmission of outputs of prompt encoder, which is the component being fine-tuned, and summed over the number of epochs. One can see in Fig. 6 that AirComp-assisted DEFT is much faster than DEFT based on orthogonal access, e.g., achieving around 10-time latency reduction at 15 epochs. The precision-and-latency results advocate AirComp as an ultra-fast air interface for DEFT based on D2D prompt transfer-and-fusion. 7. Concluding Remarks With their human-like capabilities, FoMos (or generative AI models) are expected to revolutionize fields in engineering and science. In the context of 6G, we will see wide development of FoMos as a platform to automate next-generation mobile tasks. Researchers believe that scaling up of AI is 0 5 10 15 20 102 103 104 30 40 50 60 70 80 90 12 reaching the peak and the next phase shall focus on fine-tuning FoMos to downstream tasks. Then the proposed area of DEFT presents a goldmine of research opportunities in communicationcomputation-integrated designs. Let us conclude this article by describing several promising opportunities. 1) Communication and computation balancing: While keeping fine-tuning components on the device, DEFT can download some frozen layers of FoMos to reduce communication overhead and to improve privacy at the cost of higher local computation load. This necessitates the optimization of FoMo partitioning and downloading of FoMos balance communication and computation. 2) Distributed computing for collective intelligence: FoMos can solve a complicated task by breaking down higher-level prompts into lower-level sub-tasks for distribution to connected devices. The task splitting and distribution need to be optimized based on the principle of matching the heterogeneous devices 'capabilities, i.e., computation power, communication rate, sensors and actuators, to subtasks' requirements. 3) Resource management for dynamic DEFT: The on-device FoMo components need to be updated periodically for adaptation time tasks. This calls for developing online DEFT including queue management, predictive device clustering, and dynamic resource allocation in the process of cooperative tuning. 4) DEFT for hierarchical FoMos: In mobile networks, FoMos are envisioned to be implemented hierarchically with components offloaded to edge devices, servers, and the cloud. Then, DEFT needs to be extended to support the architecture where fine-tuning of FoMos can occur flexibly at an arbitrary networking layer for layer-specified goals. For instance, on-device fine-tuning targets personalized services, while on-server tuning adapts to common requirements of users under its coverage based on behavior analysis.",
		"summary": "Foundation models (FoMos), referring to large-scale AI models, possess human-like capabilities and are able to perform competitively in the domain of human intelligence. The breakthrough in FoMos has inspired researchers to deploy such models in the sixth-generation (6G) mobile networks for automating a broad range of tasks in next-generation mobile applications. While the sizes of FoMos are reaching their peaks, their next phase is expected to focus on fine-tuning the models to specific downstream tasks. This inspires us to propose the vision of FoMo fine-tuning as a 6G service. Its key feature is the exploitation of existing parameter-efficient fine-tuning (PEFT) techniques to tweak only a small fraction of model weights for a FoMo to become customized for a specific task. To materialize the said vision, we survey the state-of-the-art PEFT and then present a novel device-edge fine-tuning (DEFT) framework for providing efficient and privacy-preserving fine-tuning services at the 6G network edge. The framework consists of the following comprehensive set of techniques: 1) Control of fine-tuning parameter sizes in different transformer blocks of a FoMo; 2) Over-the-air computation for realizing neural connections in DEFT; 3) Federated DEFT in a multi-device system by downloading a FoMo emulator or gradients; 4) On-the-fly prompt-ensemble tuning; 5) Device-to-device prompt transfer among devices. Experiments are conducted using pre-trained FoMos with up to 11 billion parameters to demonstrate the effectiveness of DEFT techniques. The article is concluded by presenting future research opportunities.",
		"id": "UUID22"
	},
	{
		"document": "I. INTRODUCTION In recent years, public landline mobile networks (PLMNs) have transformed from monolithic structures to multistakeholder networks, ushering in an era where concepts such as OpenRAN, network slicing, mobile edge computing, and infrastructure sharing will become reality and dominate the architecture discussions about the next mobile network generation 6G [1]. Network entities engaging in cross-domain interactions even within PLMNs will increasingly become the standard rather than the exception [2]. As the evolution unfold, the role of access control within the access and core networks This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. will shift from being optional to being indispensable for secure and trustful networking. Access control in the Service-Based Architecture (SBA), the control plane of the 5G core network, can use TLS with X.509 certificates for the authentication complemented by OAuth2.0 for the token-based authorization of network functions (NF). An X.509 certificate is used primarily to verify the identity of an NF holding the private key that corresponds to the certificate’s public key. It is issued by a trusted certificate authority (CA) as part of a public key infrastructure (PKI). Due to a lack of a globally trustworthy CA for PLMNs, every mobile network operator (MNO) is assumed to operate its own PKI with a centralized CA and apply cross-certification among CAs of different PLMNs to enable the cross-domain authentication of NFs in a roaming scenario [3]. This approach, while effective to a certain degree, brings about significant vulnerabilities. The primary concern is that each CA becomes a single point of failure, jeopardizing potentially the security and reliability of the entire ecosystem [4]. Furthermore, as the number of stakeholders grows, the intricacy of managing interconnected PKIs surges, posing scalability, complexity, and as a result efficiency challenges. Decentralized Identifiers (DIDs) [5], in contrast, are an attempt to standardize the way identifiers and verification material associated with a subject that is being referred to by the identifier are managed in a decentralized fashion. An entity equipped with a DID becomes its own identity provider. This eliminates the need for a CA-issued X.509 certificate, which typically encompasses an identifier that is determined and controlled by the CA. Verifiable Credentials (VC) [6] are then tamper-resistant claims that are cryptographically bound to a DID, issued by a trustful 3rd-party, and hold by the identity subject itself. A VC enables an entity to prove identity and arXiv:2310.19366v1 [cs.NI] 30 Oct 2023 arbitrary sorts of claims towards others in a cryptographically verifiable way without involving the VC’s issuer. Since DIDs are self-issued and VCs are self-managed there is no need to operate full-fledged PKIs with potentially vulnerable CAs, nor to interconnect them for cross-certification purposes. This paper is an initial attempt to conceptualize the use of DIDs and VCs for access control purposes within the SBA. As a first proof of concept, it presents an SBA which assigns DIDs to NFs and encode identity claims and permissions grants of NFs as DID-anchored VCs. The objective is to transition away from complex X.509 certificate management for access control purposes, which currently involves at least a single CA per stakeholder of a 5G ecosystem. Instead, this work aims to make access control in the SBA multi-stakeholder-ready by the introduction of decentralized identity and permission management for NFs, charting a path towards more secure, trustful, and efficient highly-collaborative PLMNs in 6G. II. FUNDAMENTALS A. Service-Based Architecture In contrast to former mobile network generations, the control plane of the 5G core network has been designed as a Service-based Architecture (SBA), to incorporate state-of-theart Internet technologies and service design patterns. The 5G SBA consists of different independent modules or services that are denoted as Network Functions (NF) as described in TS 23.501. Each NF must expose a Service-based Interface (SBI) which offers a RESTful API so that other NFs (in the role of consuming NFs) can interact with and thus consume services offered by the NF (in the role of a producing NF). Access to the SBI is conducted with HTTP/2 and JSON on the application layer, as defined in TS 29.500. According to TS 33.501, TLS on the transport layer can optionally be supported by NFs for security protection if network protection is not provided by other means such as with IPSec on the network layer. NFs are equipped with X.509 certificates so that they are able mutually authenticate themselves during a TLS handshake. Neither the management of X.509 certificates for NFs with PKIs nor the management of trust relationships among issuers and verifiers of X.509 certificates is in the scope of the 5G specification. TS 33.501 also specifies authorizations to be conducted optionally with OAuth2.0 on the application layer. The network repository function (NRF) takes over the role of the authorization server besides being a service discovery server, provisioning consuming NFs with access tokens before they access a service of a producing NF. The Client Credentials Flow grant type, specified in clause 4.4 of IETF RFC 6749, is applied here whereby a client ID and a client secret is required by a consuming NF for an access token to be granted by the NRF. B. Decentralized Identity Management A digital identity is a set of electronically stored attributes, such as a name or type, that uniquely represents an identity subject, such as a human being or a thing. It serves as a digital counterpart to a subject’s real-world identity and provides the Trust Trust Issuer Holder Verifier Verifiable Credential (VC) Verifiable Presentation (VP) DID Document (Issuer) DID Document (Holder) DID Document (Verifier) VDR View & Edit View & Edit View & Edit DID DID DID Fig. 1. Issuance and presentation of DID-bound verifiable claims. basis to establish trust among different actors in the digital domain. In centralized and federated Identity Management (IDM) systems, identity data is owned and controlled by identity providers, not the identity subjects. With decentralized IDM, the latter have ownership and full control over their identity data. However, this demands a standardized and tamper-resistant method to encode identity claims. The Verifiable Credentials (VC) model is a first attempt to conceptualize and standardize a tamper-proof container that stores the identity attributes in the form of claims. A VC is not issued by the identity subject itself, but by an issuer who verifies the claims about the identity subject and confirms their integrity by generating a proof, such as a digital signature, which becomes part of the VC. Once the VC is created, the issuer hands it over to the identity subject, who is now in control of the VC and can decide where and how it is stored and with whom to share it. A verifier, with whom a VC is shared by an identity subject in form of a Verifiable Presentation (VP), e.g., for authentication purposes, can verify its authenticity and integrity. But VCs and VPs require unique references to the parties involved in its creation, including the issuer and the identity subject the claims are about. This is accomplished with DIDs. A DID is a unique identifier that refers to an entity and resolves to a DID document. The latter contains public key material so that the entity that is referred to by the DID can proof ownership to others by means of the underlying private key material. DID documents are accessed through a Verifiable Data Registry (VDR), which is a tamper-proof, preferably decentralized infrastructure, e.g., a distributed ledger (DL). Only the DID owner can edit its DID document in the VDR but view all others. DIDs are intended to be created, owned, and controlled by the entity they refer to. An issuer, an identity subject holding a VC (denoted as holder), and a verifier each owns a unique DID. During creation, VCs are cryptographically bound by the issuer to the DID of the identity subject the claims are about. A VP then contains the VC and is, in addition, digitally signed by the identity subject. As a result, a VP empowers the identity subject to proof ownership over its DID and to present tamperresistant and 3rd-party issued verifiable claims about the entity the DID refers to, the identity subject itself. Figure 1 illustrates the relations among the issuer, holder, and verifier. Traditional X.509 certificates mix the concepts of identity and key management; they are containers that bind an identity to its key material, secured by the CA’s signature. DIDs and VCs, in contrast, clearly separate identity attributes in VCs from key material in the DID documents and link them by the DID. This facilitates not only the realization of fundamental security features such as authentication, integrity, and confidentiality, but also advanced functions such as authorization and assertion, which X.509 does not cover. VC formats even provide advanced mechanisms to protect the VP’s attributes that are not supported in X.509, including selective disclosure and zero-knowledge proofs. III. RELATED WORK The inherent characteristic of a DID is that once an entity owns the underlying key pair it can prove ownership of it to others. With DID-bound VCs, an entity can even clearly identify itself towards others. Since DIDs and VCs were originally made to represent human beings and their claims there exist numerous approaches about how the authentication procedure can be conducted with a human in the loop [7], e.g., by typically involving a human to scan QR codes [7]–[9]. In the context of the SBA, however, entities in form of software or hardware service instances need to mutually authenticate themselves without the intervention of a human. It reassembles an Internet of Things (IoT) environment, where things do not only mutually authenticate themselves autonomously but also need to handle permissions for authorization purposes. An access control model based on DIDs and VCs that gets along without human intervention was developed within the QualiChain project [10]. In the proposed model, which is capable to implement role- as well as attribute-based access control, agents act autonomously on behalf of humans and present VPs in order to access resources. For constraint IoT devices, Lagutin et al. present a method to use DIDs for authentication purposes and VCs for authorization purposes within the ACE-OAuth flow, an OAuth2.0 framework for constraint environments [11]. In [12], DIDs are used to authenticate an entity towards another entity while access policies are encoded within the DID document of the resource provider. However, the latter extensions violate the DID specification and contradicts the primary intention of a DID document to contain only pseudo-anonymous verification material of an entity. Kim et al. proposes DIDs and VCs for identification while the authorizations are given through local policies by a resource provider. In [13], DIDs and VCs are used for authentication and the role-based permission policies are stored in a smart contract, which decentralizes the policy decision point. In the realm of mobile networks, the concepts and ideas that DIDs and VCs are based upon are primarily applied for the subscriber authentication. You et al. discusses the potential and benefits of sharing verification material for subscribers in 5G via a DL in order to efficiently authenticate visiting subscribers [14]. Although not using DIDs and VCs, Xu et al. describe an approach to enable subscriber authentication with the help of locally stored verifiable claims (similar to VCs) and verification material shared via a DL (similar to DL-anchored DID documents) [15]. Haddad et al. follows a similar path and introduces a new authentication and key agreement protocol for 5G that authenticates visitors in 5G networks without the need to consult the home PLMN authentication center [16]. From a standardization point of view, the 5G Public-PrivatePartnership (5GPPP) started to emphasize the general challenge of secure and trustful cross-domain access control and describes DIDs and VCs as potential key concepts to conduct trustful cross-domain identity and permission management in 5G and beyond [17]. Rodriguez Garzon et al. then adds access control of network entities within the core networks to the range of application areas in 6G that can benefit from the usage of DIDs and VCs [18]. Within the 5GZORRO project, Valero et al. introduce a security and trust framework for 5G networks to let entities of different operators create secure VPN tunnels among them by means of the verification material found in each other’s DID documents on a DL [19]. Despite mentioning the use of VCs for the identification and authorization of stakeholders across administrative domains, it remains unclear what role VCs play in the setup of a VPN channel. But besides sketching the idea of a DID-based decentralized PKI for the SBA [18] and using VCs for the authorization of NFs within the SBA [20], so far, there exists no technical concept to use DIDs and VCs for access control purposes in the SBA nor a proof of concept. This work is a first attempt to conceptualize and implement an SBA that uses DIDs and VCs for access control instead of X.509 certificates in the transport layer and OAuth2.0 on the application layer. IV. CONCEPT In an SBA where access management utilizes DIDs instead of X.509 certificates, every NF must possess at least a DID. An NF owns multiple DIDs if it intends to use different digital identities to interact with different NFs. It can either create the DID itself by generating a key pair or letting an entity of the management plane provision it with a DID including the underlying key pair. An NF’s DID can be of public or private nature. With a public DID, the DID document is anchored respectively stored in a VDR. With a private DID, also referred to as a peer DID, the DID document can be extracted from the DID itself. The type of DID to use for NFs depends primarily upon the need to subsequently change the DID document. The latter of a private DID can’t be changed while its public counterpart can subsequently be adapted in a cryptographically verifiable in the VDR way by the DID owner. Private DIDs are therefore better suited for short-lived NFs while their counterparts are a better fit for long-living NFs, e.g., that rotate keys on a regular basis for security reasons. An NF equipped with a DID, regardless of being public or private, can authenticate itself only in a pseudo-anonymous manner towards other NFs because the DID document does not reveal the identity of the owner. To clearly identify itself towards others, the NF needs, in addition, verifiable identity claims in the form of VCs that are bound to the NF’s DID. For example, the latter might state that an NF is of a specific type, belongs to a network slice, or is owned by a specific MNO. These type of VCs form the basis of an NF’s digital identity and are in the following, when presented during authentication, denoted as AuthN VPs. They are used by NFs during access control to identify themselves towards others in addition to the pseudo-anonymous authentication with a DID. For the authorization, the authorizer can either give permission to access its resources based on the verifiable identity claims in the AuthN VPs of the requesting NF (attributed-based access control), the role of the requesting NF as part of the same AuthN VPs (role-based access control), or in the spirit of an access token through a VP encoding the permission grant. The former two options can be realized with local policies at the requested NF or by a dedicated and trustful policy decision function as proposed in [10] or [13]. With the latter option, the requesting NF is granted permission based on a presented VP, here denoted as an AuthZ VP, which contains the access permission as key/value pairs. While DIDs are self-issued, regardless of being self-created or handed over by the management plane during NF instantiation, verifiable identity attributes for identification (AuthN VCs) and verifiable permissions grants (AuthZ VCs) for authorization must be issued by a trusted 3rd-party. Consequently, there is a need for trustful DID-enabled entities in the SBA that act in the role of identity providers, issuing AuthN VCs, and authorization servers, issuing AuthZ VCs. Although being logically different type of entities, they are subsumed here under the term identity and permission management function (IPMF). Besides the issuance of VCs, IPMFs are also responsible to revoke VCs, e.g., once an NF is compromised or the access permission has been withdrawn. The revocation mechanism can thereby be implemented via a DL, so that a verifying entity is able to check the revocation status of a VP by inquiring the DL. Alternatively, an IPMF can introduce a validity period for a VC and thus, if the security policies permit it, eliminate the need for a revocation mechanism. Once NFs are equipped with DIDs and DID-bound AuthN and AuthZ VCs, they can (mutually) authenticate and authorize themselves. Authentication subsumes a) the pseudoanonymous authentication with DIDs and b) the identification with AuthN VPs. While a) can alternatively be conducted on the transport layer as part of a modified TLS handshake [21] or with a VPN tunnel [19], or on the application layer as part of an application layer transport protocol such as DIDComm [22], b) happens only on the application layer with a VP exchange protocol. Given that AuthZ and AuthN VPs are technically alike and only differ in the intended purpose, the authorization is conducted on the application layer with the same VP exchange protocols as in b). Depending on the scenario, DID and VC-based access control can be conducted in a two-way or one-way manner. During access control, a verifying NF can can consider the VPs presented by another NF as technically valid as long as they can be verified by means of the IPMF’s and other NF’s verification material and - if a revocation mechanism exists - are not revoked by the IPMF. The verifying NF has access to the verification material either via a VDR, if the other NF and IPMF own public DIDs, or from the DIDs itself, if private DIDs are used. The extent Root IPMF Issuance of Del VCs Issuance of AuthN VCs Issuance of AuthZ VCs Inherent Trust IPMF NF Entity Relation Administrative Domain A Administrative Domain B Fig. 2. Exemplary relationships among IPMFs and NFs. to which the identity attributes or permissions are trustworthy, however, depends upon the verifying NF’s trust in the IPMF. Multiple IPMFs within the same administrative domain can be structured hierarchically by enabling IPMFs (denoted as parent IPMFs) to delegate subsets of their rights to other IPMFs (denoted as child IPMFs). These delegable set of rights must always include the right to issue AuthN and/or AuthZ VCs to NFs and might also include the right to subdelegate the former rights to another IPMF. Delegations are encoded as VCs, denoted as Del VCs, and are issued by each parent IPMF to its child IPMFs. Del VCs received from a parent IPMF also contain the Del VCs the parent IPMF received from its parent as so forth. This empowers an IPMF to cryptographically prove to others that the delegations it received are valid all the way up to the first Del VC issued by the root IPMF. Since an NF receives the chain of Del VCs of its IPMF as part of the AuthN/AuthZ VCs, a verifying NF can securely trace back the AuthN/AuthZ VPs to an IPMF which is well-known and trusted by it. Del VCs allow organizations to align a hierarchy of IPMFs to their organizational layout, to allocate the operational issuance tasks among different entities for optimal load sharing, and to distinctly protect the critical upper IPMFs, e.g., the root IPMF, by letting them only delegate and not participate in the operational issuance process for potentially harmful external NFs. Figure 2 illustrates exemplarily how NFs and IPMFs can be related to each other. In highly dynamic cloud environments, virtualized NFs are numerous and might come and go while IPMFs are fewer in number and are supposed to exist for a longer period, in particular the root IPMF as the major trust anchor. This makes an NF a good candidate to own private DIDs especially if edit operations of a DID document in a VDR are costly, e.g., in a DL, and as long as a key rotation can be omitted. IPMFs, on the other hand, are critical long living entities where at least a periodical change of verification material in the DID document is mandatory from a security point of view. IPMFs are therefore predestined to own public DIDs, especially a root IPMF, because their DID documents can not only be adapted subsequently but be anchored in a DL that is commonly operated and equally governed by multiple stakeholders and NF NF TCP HTTP/2 TCP HTTP/1.1 HTTP/2 TCP HTTP/1.1 HTTP/2 TCP HTTP/2 Database Controller Database Controller K8s Pod K8s Pod K8s Cluster Hyperledger Indy Distributed Ledger Sidecar Sidecar TCP ZeroMQ 3GPP 3GPP free5GC free5GC IDunion ACA-Py (Agent) Ledger Node ACA-Py (Agent) Ledger Node HTTP/1.1 DIDComm v1 Present Proof v1|HTTP/2 TCP Fig. 3. NF-sidecar architecture with DIDComm-tunneled HTTP/2 communication link between NFs of the SBA. is therefore well-suited to become a global common source of truth for verification material of major trust anchors. V. IMPLEMENTATION For the prototypical DID- and VC-enabled SBA, several cloud-native solutions were integrated to closely follow a state-of-the-art core network deployment model. As a foundation, the open-source framework free5GC1 was used; a fully operational 5G core network in alignment with 3GPP Release 15. Both the User Equipment (UE) and the Radio Access Network are simulated with UERANSIM2 . All the SBA’s NFs, an IPMF, a gNodeB, and a UE are containerized and orchestrated in a Kubernetes (K8s) cluster. An Aries Cloud Agent3 (ACA-Py) at each NF manages DIDs and VCs, integrates a digital wallet, interacts with the VDR, and enables message-based and transport-agnostic communication among agents of different NFs with DIDComm [22] (v1). DIDcomm is an application layer transport protocol which allows to establish secure and connectionless end-to-end communication channels with DIDs, ensuring confidentiality and integrity. Each NF, including the IPMF, is isolated in a separate K8s pod and owns a public DID that is anchored in a DL. The DL of the IDunion project4 serves as the VDR and is powered by Hyperledger Indy5 . All communication logic, incl. access control, is encapsulated in a sidecar, encompassing three entities. The controller, an interface tailored for the ACAPy agent’s API, is the sidecar’s central hub. The agent is the interface towards other NF sidecars and the VDR while the database keeps state of the associations among NF sidecars. The request-response HTTP/2 traffic among NFs is tunneled through simplex DIDComm messages. The free5GC NFs are kept unaltered but are configured in such way so that all inand outgoing communication comes and goes through their sidecars. So once an NF initiates a request to another NF, it will be intercepted by the sidecar’s controller. The latter determines the targets NF’s DID from the information gained from 1https://free5gc.org/ 2https://github.com/aligungr/UERANSIM 3https://github.com/hyperledger/aries-cloudagent-python 4https://idunion.org 5https://www.hyperledger.org/projects/hyperledger-indy the request, which contains at this point, because of redirecting it to the controller, localhost as the target NF’s IP address. The controller mimics hereby partly the functionality of the NRF which could itself, due to a resulting incompatibility, not be extended to provision DIDs instead of fully qualified domain names as part of a service discovery response. The controller will then forward the request encapsulated in an HTTP/1.1 message and enriched with the target DID to the sidecar’s agent. The agent resolves the IP address of the target DID by querying the associated DID document in the VDR (IDunion ledger), containing what is looked for in the service end point field. It then encapsulates the original HTTP/2 request into a DIDComm message, encrypted by the target’s DID public key from its DID document, and forwards it to the target’s agent. At the target NF’s sidecar, the request goes through a reverse process and is then forwarded in a decapsulated form to the target NF. In the reverse direction, the response goes through similar processing and forwarding stages. However, neither the DID nor the IP address of the requesting NF needs to be resolved because they become known by the target’s sidecar through the request. Figure 3 illustrates the architecture and protocol stacks. The decision to go with a lesser performant tunnel solution was driven by the requirement to be fully compatible with the current 5G SBA implementations of the SBI. However, this allows the sidecar solution to be future proof for upcoming iterations of the free5GC or similar 5G core implementations as longs as they adhere to the 5G specs. With DIDComm, message authenticity can be guaranteed. The pseudo-anonymous authentication with DIDs happens hereby in a passive way by setting up a secure end-toend DIDComm channel in which the proof of ownership of a DID is accomplished by signing each message with the private key corresponding to the key material contained in the sender’s DID document. VCs/VPs for identification and authorization purposes are encoded as AnonCreds6 , issued by an IPMF to NFs using the Aries Issue Credential Protocol7 , and are presented as VPs by an NF for access control on 6https://hyperledger.github.io/anoncreds-spec/ 7https://github.com/hyperledger/aries-rfcs/blob/main/features/0036-issuecredential/README.md top of DIDComm with the Aries Present Proof Protocol8 . A mutual authentication aka the exchange of AuthN VPs for identification purposes happens between all NFs, including the NRF. For authorization, a consuming NF needs to present AuthZ VPs to the producing NF but not vice versa. Access control with AuthN and AuthZ VPs is conducted only once at the start of a communication link between NFs. A UE registration as defined in TS 23.502, encompassing 58 REST calls among six NFs, was conducted in an experimental setup on a virtual machine hosted on a server powered by an AMD EPYC 7262 CPU, with 4 vCPUs, 16GB of memory, and Ubuntu 22.04.2 LTS with kernel version 5.15.0-73-generic. A complete UE registration in the original and unaltered SBA takes 5.129 sec. without TLS and 5.130 sec. with TLS; after 30 iterations. With a DID- and VC-enabled SBA, the duration extends to 5.838 sec, about a 13.8% increase. VI. CONCLUSION This work presented a first concept and a prototype of an SBA where NFs utilize DIDs and VCs as an alternative to X.509 certificates and OAuth2.0 access tokens to authenticate and authorize each other. As a proof of concept, it demonstrates the technical feasibility to eliminate the need for centralized and vulnerable CAs within full-fledged PKIs for the SBA without having to sacrifice core security features related to them such as verifiable digital identities and permissions grants or secure communication channels. In fact, the proposed approach to decentralize identity and permission management for NFs in the SBA comes along with benefits, such as simplified and trustful cross-domain key management, unified access control as well as improved security against centralized breaches. The introduction of IPMFs for the issuance and, optionally, the revocation of verifiable identity and permission claims empowers NFs during operation to adapt on-demand to evolving access control requirements in highly dynamic multi-stakeholder environments without requiring full CI/CD pipeline runs. Furthermore, it relieves the NRF from acting as an (cross-domain) authorization server for NFs by logically outsourcing permission management to a dedicated network entity. Currently, however, it lacks standardized technical means to setup highly efficient and secure connection-oriented channels with the help of DIDs on the transport layer, which is assumed to have a significant impact on the performance of transported HTTP/2 REST calls. With respect to a commonly operated and most probably permissioned DL to share DID documents of major trust anchors for access control purposes in the SBA, it remains still unclear how it will be governed by stakeholders of a global 6G ecosystem in which geopolitical, market, and regulatory interests collide.",
		"summary": "—In 6G, mobile networks are poised to transition from monolithic structures owned and operated by single mobile network operators into multi-stakeholder networks where various parties contribute with infrastructure, resources, and services. This shift brings forth a critical challenge: Ensuring secure and trustful cross-domain access control. This paper introduces a novel technical concept and a prototype, outlining and implementing a 5G Service-based Architecture that utilizes Decentralized Identifiers and Verifiable Credentials to authenticate and authorize network functions among each other rather than relying on traditional X.509 certificates or OAuth2.0 access tokens. This decentralized approach to identity and permission management for network functions in 6G reduces the risk of a single point of failure associated with centralized public key infrastructures, unifies access control mechanisms, and paves the way for lesser complex and more trustful cross-domain key management for highly collaborative network functions of a future Service-based Architecture in 6G.",
		"id": "UUID23"
	},
	{
		"document": "I. INTRODUCTION Network telemetry is important for various network management applications [1], such as fault location [2], congestion control [3], path verification [4], and more [5]. Real-time and reliable network information is critical to comprehending the network status. However, designing efficient network telemetry systems remains challenging due to two key difficulties. First, the network telemetry system needs to have flexibility to meet various telemetry requirements. Second, the network telemetry system should be adaptable to the dynamic network environment. These two difficulties are summarized as follows. • Flexibility. With the development of network function virtualization (NFV) [6] and service function chaining Penghui Zhang, Hua Zhang, and Zijian Cao are with the National Mobile Communications Research Laboratory, Southeast University, Nanjing 211111, China. (email: phzhang@seu.edu.cn, huazhang@seu.edu.cn, caozijian@seu.edu.cn) Yibo Pi is with the UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, 200240, China. (e-mail: yibo.pi@sjtu.edu.cn) Jingyu Wang and Jianxin Liao are with the State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China. (email: wangjingyu@bupt.edu.cn, liaojx@bupt.edu.cn) This work was supported by the National Key Research and Development Program of China under Grant(2020YFB1807803) . (SFC) [7], there is a growing diversity of network applications with unique telemetry requirements [8], e.g., high accuracy, low control overhead, low latency, and full network coverage. If a network telemetry system fails to meet the requirements of these applications, it is hard to provide personalized services for users, negatively impacting its effectiveness. Therefore, flexibility is a crucial characteristic of a network telemetry system. • Adaptability. Dynamic network environments [9] are characterized by frequent fluctuations in network load [10] and the potential for link disconnections or congestion [11]. To maintain stability and reliability in dynamic network environments, the telemetry systems should have the adaptability to adjust the system parameters in time according to the changes in the network environment and optimize the quality and efficiency of the network telemetry [12]. Despite their advantages in terms of network visibility, scalability, and accuracy [13], existing telemetry systems, including both passive network telemetry (PNT) systems [14]– [17] and active network telemetry (ANT) systems [18]–[20], face significant challenges in overcoming the difficulties (discussing in Section II). One major issue is they are typically designed based on fixed telemetry requirements and cannot flexibly meet new requirements, which can limit their ability to respond to changing conditions or evolving demands. Moreover, existing telemetry systems are often deployed for specific network loads and topologies, making them less adaptable to unexpected scenarios such as node failures or link disconnections. When such faults occur, the performance of these systems may suffer significant degradation, leading to potential disruptions and outages. To address these challenges, researchers need to develop innovative approaches that can improve flexibility and adaptability. This paper proposes AdapINT, a novel in-band network telemetry framework that leverages dual-timescale probes to provide flexible and adaptable telemetry services. AdapINT comprises two types of probes: long-period auxiliary probes (APs) and short-period dynamic probes (DPs). The APs collect basic network status information, while the DPs gather specific network information required by applications. To improve adaptability, AdapINT uses the basic network status collected by the APs to periodically update the DPs’ forwarding paths. Specifically, during each long period, AdapINT injects APs arXiv:2310.19331v1 [cs.NI] 30 Oct 2023 2 into the network using source routing technology [21] to collect basic network status such as the topology and load of the entire network. Based on the latest APs’ telemetry reports, AdapINT updates the DPs’ forwarding paths to adapt to the dynamic network environment and uses DPs to collect specific network information required by network applications during each short period. The probe path planning algorithms of APs and DPs are crucial for ensuring system performance. Since the APs’ forwarding paths should cover the entire network to collect comprehensive basic network status information. We propose an auxiliary probes path deployment (APPD) algorithm based on the Depth-First-Search (DFS) algorithm to achieve full network coverage. To improve flexibility, AdapINT should be capable of meeting different telemetry requirements through the DPs’ telemetry process. To achieve this, we propose a dynamic probes path deployment (DPPD) algorithm based on deep reinforcement learning (DRL). By adjusting the reward function, the DPPD algorithm can meet various telemetry requirements, such as shortest path, minimum bandwidth consumption, and minimum latency. Inspired by [22], our proposed DRL model consists of a recursive neural network (RNN) decoder and an attention mechanism. To reduce the complexity of the proposed DRL model, we require the RNN model to keep the input sequence unchanged, so that changing the order of any two inputs does not affect the network. Additionally, we set a mask to shield infeasible solutions, which not only avoids the routing loop problem but also facilitates faster model training. To adapt the DPPD algorithm to different telemetry requirements and environments, we introduce transfer learning for various reward functions and transfer learning for different training sets [23], which can reduce the training time of the DRL model. The contributions of this paper can be summarized as follows: • We propose a flexible and adaptable in-band network telemetry framework, called AdapINT. It uses long-period APs to collect basic network status information and short-period DPs to collect various network information required by network applications, thus achieving ondemand, network-wide, stable network telemetry. • The DFS-based path planning algorithm is proposed for APs, called the APPD algorithm. The proposed algorithm can achieve full network coverage with low complexity. • The DPPD algorithm is proposed for DPs based on a selflearning DRL model. The proposed algorithm can meet various telemetry tasks by adjusting the reward and can constantly update the forwarding path according to realtime network information. Moreover, a transfer learning method is introduced to reduce the training time of the DRL model. • We develop a self-learning DRL model based on the attention mechanism. Considering the characteristics of the routing path planning problem, we improve the input queue of the RNN model and set a mask function to reduce model complexity. • We demonstrate the flexibility and adaptability of AdapINT under a variety of different network environments and telemetry requirements. Compared with traditional network telemetry systems, AdapINT can reduce telemetry latency by 75% in online games and video conferencing scenarios and can reduce control overheads by 34% in cloud computing services. The rest of this paper is organized as follows. In Section II, we discuss the related work. Later, we propose an architecture of AdapINT and describe the critical designs for AdapINT in Section III and we propose the APPD algorithm in Section IV. In Section V, we propose a DRL model and apply it to the design of the DPPD algorithm. In Section VI, simulation results are presented to demonstrate the performance of AdapINT. Finally, we conclude the paper in Section VII. II. RELATED WORK This section discusses the related work, including PNT, ANT, and DRL. A. Passive Network Telemetry In-band network telemetry (INT) is an emerging representative of PNT technology due to the development of softwaredefined networks (SDN) [24] and programmable data-plane technology [25]. INT enables network devices to add deviceinternal information to packets, such as port numbers, link utilization, or queuing latency, which is then sent to the centralized controller for further analysis at the last hop. INT increases the packet size, which incurs additional overhead, and any methods have been proposed to reduce the telemetry overhead, such as PRoML-INT, PINT, and INT-label [15]– [17]. However, due to the uncontrolled probe paths, PNT makes it difficult to achieve full network coverage and can result in the switches being repeatedly probed. The redundant information caused by repeated collection leads to a variety of problems, including increased difficulty of data analysis and higher computational overhead [26]. B. Active Network Telemetry The basic idea of ANT [27] is to generate a telemetry probe from an INT agent to probe the user-specified telemetry path actively [21] through source routing technology. However, existing ANT systems only use flexible probe path planning algorithms to meet fixed telemetry requirements in different network scenarios, including INT-path, NetView, and IntOpt [18]–[20]. INT-path is designed to cover the entire network and reduce telemetry overhead by using the Depth-FirstSearch (DFS) algorithm and Euler Trail/Circuit-based path planning policy [18]. NetView is designed to support multiple telemetry applications at different frequencies. It requires only one vantage server for the entire telemetry process and can cover the entire network topology [19]. IntOpt is specifically designed for VNF service chain network monitoring and uses a simulated annealing-based random greedy metaheuristic (SARG) to minimize overhead during active probing and collection [20]. These ANT systems tend to optimize specific performance metrics, such as latency or overhead, with fixed control strategies in limited types of scenarios. They lack the flexibility to reconfigure and redeploy probe paths for new telemetry requirements [28]. Furthermore, existing network telemetry systems are typically deployed based on 3 Path:5,4,3,2 Auxiliary Probe 1 2 3 4 5 SR Stack INFO Stack 2 3 4 5 1 Push Pop Controller Dynamic probe path Auxiliary probe path Generate APs Collect APs Collect DPs Generate DPs Auxiliary probe path deployment system Dynamic probe path deployment system Telemetry analyzer Peport basic network information Provide network status report Report network information in DPs Telemetry task Provide the telemetry requirements Dynamic Probes 2 1 1 3 4 5 1 Path:5,1,2 Fig. 1. Architecture of AdapINT. fixed network environments and are hard to self-adjust when the network environment, such as network topology and network load changes. These environmental shifts are relatively common in real-world networks and can significantly impact the effectiveness of telemetry systems, requiring path planning strategies to be continuously adjusted to match the current network state. C. Deep Reinforcement Learning and Transfer Learning DRL provides new opportunities for solving multi-objective optimization and dynamic environment adaptation problems [29]–[33]. For example, DRL has significant potential in dealing with custom optimization objective problems [34]. It can continually adjust model parameters to adapt to various optimization objectives and efficiently meet new telemetry requirements without any human assistance [35]. Therefore, we use it to enable network telemetry systems to have the ability to meet different telemetry requirements. Furthermore, DRL has excellent scalability and can solve problems with similar structures [36], making it ideal for updating path planning based on real-time network information [37]. By leveraging transfer learning techniques, we can further reduce the training time of the DRL model by transferring knowledge from existing telemetry tasks to new ones [23]. This approach is crucial for enabling telemetry systems to adapt to evolving network environments and meet diverse telemetry requirements effectively. III. SYSTEM DESIGN In this section, we first present the architecture of AdapINT. Then, we describe the critical designs for AdapINT, including probe format, switch behaviors, metadata, and queries. At last, we introduce how the controller handles the telemetry results. A. Architecture of AdapINT AdapINT is committed to designing an on-demand network telemetry system in a dynamic network environment. To improve the adaptability, AdapINT designs a network telemetry architecture comprised of dual-timescale probes [18]. Dynamic Probes Networks 2. Inject APs into networks. 3. Collect basic network information. 4. Report the basic telemetry results. 5. Specify dynamic probes paths according to basic telemetry results. 1. Specify auxiliary probes paths Controller Auxiliary Probes 8. Report final telemetry results. 7. Collect network information required by network management applications 6. Inject DPs into networks. Fig. 2. Workflow of AdapINT. Fig. 1 illustrates that AdapINT comprises four main components: the auxiliary probe path deployment system, the dynamic probe path deployment system, the telemetry analyzer, and the telemetry task. Users can specify telemetry tasks through network applications to provide telemetry requirements to the telemetry analyzer. The auxiliary probe path deployment system designs auxiliary probe paths for APs to acquire basic network status, which is used by the dynamic probe path deployment system to deploy dynamic probe paths [27]. After receiving APs each time, the dynamic probe paths are updated based on the real-time network information. Besides, DPs are responsible for collecting network information required for network applications based on telemetry tasks at a higher frequency. Compared to APs, DPs take up more network resources and incur most of the telemetry overhead due to the higher frequency and more network information collected. As shown in Fig. 2, the workflow of AdapINT can be described as follows: First, the controller plans the auxiliary probe paths according to the network topology structure. Second, the network device generates APs and injects them into the network at a low frequency. Third, APs are forwarded along auxiliary probe paths and collect basic network status. Next, after receiving APs, the controller obtains the basic telemetry results. Then, the controller designs dynamic probe paths based on real-time network information. Moreover, DPs are injected into the network at a high frequency. Furthermore, the DPs collect all required network information. At last, DPs report the collected network information to the controller for further analysis and processing. B. Probe Format APs are responsible for collecting basic network status information, while DPs need to collect specific network information required by applications. Therefore, AdapINT designs the auxiliary probe format and dynamic probe format based on the characteristics of different tasks. 1) Auxiliary probe format: As shown in Fig. 3, the AP’s header is similar to a normal packet header. It consists of an Ethernet header, an IP header, and a TCP/UDP header. The destination port is set to “Auxiliary probe” to ensure that 4 Value 1 Value 2 …… Value 2 …… ETH IP TCP/UDP SR stack INFO stack Destination Port: Auxiliary_probe Source Port …… Port 1 Port 2 …… Port n INFO 1 INFO 2 …… INFO n ETH IP TCP/UDP SR stack INFO stack Source Port …… Port 1 Port 2 …… Port m INFO 1 Switch ID 1 INFO 2 …… INFO m Value 1 Auxiliary Probe Format Switch ID 1 Bitmap Legend Source routing (SR) stack Information (INFO) stack Fields in packet headers that have been changed New field in dynamic probes Dynamic Probe Format Length Destination Port: Dynamic_probe Fig. 3. Auxiliary probe format and dynamic probe format. programmable switches can parse APs. In addition, the AP’s destination address is set as the controller’s IP to forward APs to the controller. Moreover, the AP consists of a source routing stack (SR stack) and an information stack (INFO stack). The SR stack is a fixed-length stack consisting of a series of Port labels. Each Port label records one hop on the auxiliary probe path, and the switches forward APs based on the Port labels. The INFO stack is used to store network information. Each INFO label contains the switch ID and the value of each network metadata. 2) Dynamic probe format: The dynamic probe format is similar to the auxiliary probe format. However, as the length of dynamic probe paths varies more widely, a fixed-length SR stack can result in unnecessary waste. Furthermore, given the diverse types of network metadata collected by DPs, switches need to identify which kind of metadata is to be collected. Therefore, the dynamic probe format introduces two new fields: (i) Length and (ii) Bitmap. As shown in Fig. 3, the SR stack is a variable length stack in the dynamic probe format, and the “Length” determines the length of the SR stack. And the “Bitmap” determines what kinds of metadata should be collected by the INFO label. It is worth mentioning that the destination port in the dynamic probe is set to “Dynamic probe” so that the switches can parse DPs. C. Switch Behaviours The tasks of programmable switches [25] are recognizing, processing, and forwarding probes. After receiving the packet, the switch behaviors are shown in Fig. 4. Firstly, the switch needs to check if the packet is a probe. If the destination port is “Auxiliary probe” or “Dynamic probe”, the switch considers this packet to be a probe. If it is not, the packet is processed and forwarded normally. Next, the switch should process the probe according to the probe type. For an AP, the switch should record the basic network information with the INFO label and add the INFO label to the INFO stack. For a DP, the switch should add network information to the INFO label according to the “Bitmap”. Finally, the switch at the last hop in the probe path sends the telemetry information to the controller for further analysis. Otherwise, the switch forwards the probe to the next hop according to the probe path. D. Metadata and Queries Metadata represents the various network information values collected by AdapINT from switches. To simplify user Receive a packet Normal packets Auxiliary probe Dynamic probe Process packets normally Check if the packet is a probe Forwarding probes to the next hop based on the probe path Adding basic network information Check the “Bitmap” Add network information according to “Bitmap” Check if the current node is the last hop Send probes to the controller Forward packets normally No Yes No Yes Fig. 4. Switch behaviours. operations, AdapINT employs queries to specify the types of metadata required for various telemetry tasks. AdapINT can not only obtain metadata directly from switches, ingresses, egresses, and buffers, but also obtain high-level metadata through the calculation of the underlying data. In summary, AdapINT supports two types of metadata, namely node-level metadata and path-level metadata. Nodelevel metadata includes switch ID, switch workload, queue length, congestion status, queue packet loss rate, port packet loss rate, port timestamp, port packet count, and so on. Pathlevel metadata includes path latency, path utilization, path throughput, etc. The user can write queries to represent one or multiple telemetry tasks. We use the following telemetry tasks as examples: (a) Congestion Control. Users can let AdapINT periodically measure the ratio of the current queue length to the configured maximum queue limit to obtain the congestion status. (b) Network fault diagnosis. After obtaining the link latency, users can quickly know the location of the network fault by determining whether the latency is abnormal or not. Note that APs have a fixed telemetry task to obtain the network information needed for the DPPD algorithm. Therefore, APs have a unique query. In this paper, APs provide link latency 5 1 4 5 6 3 2 1 4 5 6 3 2 Legend: Available Link Failed Link Unknown Link (a) (b) Fig. 5. Telemetry results analysis. and port status information for the DPPD algorithm. E. Telemetry Results Analysis When probes are collected by the controller, the telemetry analyzer parses each probe into a series of dictionaries or tuples. For APs, the analyzer provides basic network status to the dynamic probe path deployment system. For DPs, the analyzer stores the network information in a database, which other network applications can access. To ensure structured storage, the storage format is designed according to the switch ID and metadata types. In order to save storage space, whenever new probes are received, the telemetry analyzer updates the contents of the existing dictionaries or tuples with the latest network information. Finally, the telemetry analyzer calculates high-level metadata based on the queries. Network applications can periodically obtain network telemetry reports based on their telemetry requirements. Another task of the telemetry analyzer is to locate network faults via APs. When a link fails, all APs that pass through the faulty link cannot be collected by the controller. Therefore, it is necessary to check all the links that these APs pass through to locate the fault. To minimize the number of APs that need to be checked, we must guarantee that only one AP passes through every link. By utilizing non-overlapping auxiliary probe paths, we can quickly find the path where the fault is located and reduce the workload of the fault location. As illustrated in Fig. 5, if link (4, 5) is faulty, the APs with the paths [2, 4, 5, 3] will not be received by the telemetry analyzer. Consequently, when locating the faulty link, the controller can narrow down the potential location to links (2, 4), (4, 5), and (5, 3). IV. AUXILIARY PROBE PATH DEPLOYMENT SYSTEM In this section, we first analyze the problem of auxiliary probe path deployment. Then, we design the APPD algorithm based on DFS. A. Problem Analysis Considering a network containing n routers, we define the network topology as an undirected physical graph, denoted as G = (V, E). V is the set of physical nodes denoted as V = {i|i = 1, · · · , n} and E is the set of physical links denoted as E = {(i, j)|i, j ∈ V }. The index of the physical node is denoted by i ∈ V , and the physical links between node i and node j are denoted by (i, j) or (j, i). E is the set of unordered binary groups consisting of the elements in V . We denote the p-th auxiliary probe path as ap = vp,1, vp,2, · · · , vp,Np  , p = 1, 2, · · · , P, where Np is the number of nodes in path ap passes and vp,i denotes the ith node that path ap passes through. Since the SR stack of the auxiliary probe is a fixed-length stack, the length of the port information cannot exceed the length of the SR stack, which can be represented as |ap| ≤ lth, (1) where |ap| denotes the length of the p-th path and lth denotes the length threshold of auxiliary probe paths. Comprehensive network status information is crucial for the path planning of DPs. As a result, it is crucial to ensure that APs can traverse all physical links within the network. This requirement can be described as follows: E = [ P p=1 Lp, (2) where Lp is the set of links which the p-th path passes through. Moreover, to reduce the number of links that require verification by the controller during fault localization (as explained in Section III-E), it is necessary to ensure that each link is probed by only one probe. The above requirement can be described as Lp1 \ Lp2 = ∅, ∀p1, p2 ∈ {1, 2, · · · , P} , p1 ̸= p2, (3) where p1, p2 are the index of any two auxiliary probe paths. Since APs have lower frequency and less data carryover compared to DPs, their impact on network performance is minor. Thus, we consider the auxiliary probe path planning problem as finding feasible solutions that meet the following constraints: First, the length of each auxiliary probe path must not exceed the SR stack capacity (Constraint (1)). Second, the auxiliary probe path should cover the entire network topology (Constraint (2)). Finally, the auxiliary probe paths should be designed to avoid overlap with one another (Constraint (3)). B. Auxiliary Probes Path Deployment (APPD) Algorithm In this subsection, based on the analysis presented in Section IV-A, we propose an APPD algorithm to plan auxiliary probe paths and describe its operation steps in detail. Once an auxiliary probe path is created, we should select and add suitable nodes to the path. How to choose a suitable node is very important to improve the efficiency of path planning. Therefore, different rules for adding nodes will directly affect the performance of the APPD algorithm. Due to the Constraint (2) and Constraint (3), we can propose a simple DFS-based algorithm to select the next node, which can generate non-overlapping paths to cover the whole network. However, because the basic idea of the DFS algorithm is to explore the graph along each branch as much as possible, it may generate excessively long probe paths. To ensure that the length of the auxiliary probe path does not exceed the capacity of the SR stack, we must limit the depth of each exploration. Let us assume that we are planning the p-th auxiliary probe path ap, p = 1, 2, · · · , P. After adding the node vp,k−1, k = 2, · · · , lth, the p-th path can be temporarily denoted as ap = [vp,1, vp,2, · · · , vp,k−1]. Next, we should look for the appropriate node vp,k, which is the node connected to 6 node vp,k−1. The set of nodes connected to node vp,k−1 can be denoted as V (vp,k−1) = {i| ∃ (vp,k−1, i) ∈ E} , (4) where the symbol ∃ indicates that the link is in the set E. We should choose node vp,k in such a way that the link (vp,k−1, vp,k) is not yet covered by existing auxiliary probe paths, which can be denoted as (vp,i−1, vp,i) ∈/ L, ¯ (5) where L¯ = Sp i=1 Li is the set of links covered by existing auxiliary probe paths. Moreover, considering the constraint (1), we also need to ensure that the length of path ap does not exceed Lth after adding node vp,k, which can be denoted as |ap| + 1 ≤ lth. (6) Therefore, based on the DFS algorithm, the node selection rule of our proposed long-time-scale algorithm can be denoted as vp,k = min i i, i ∈ V (vp,k−1). (7) Note that if we are unable to append node vp,k to path ap due to Eq. (6), it is necessary to create a new path ap+1 from node vp,k−1 and start planning from that point onwards. In the event that no nodes satisfying Eq. (5) are found, we should go back to the previous node, which has the remaining unvisited neighbors, before creating a new path ap+1. To enhance clarity and facilitate description, we present the proposed APPD algorithm in Algorithm 1, followed by a detailed description of its specific steps: Step 1) Initialization:The algorithm parameters are initialized as follows: lth represents the length threshold of auxiliary probe paths, v represents the node index, n represents the length of paths, and flag is used to control the creation of a new path. Step 2) Traverse: First, select a node in the network and create a path. Following selection rule (7), find the first uncovered link of the just-visited node and mark it as covered. Then, select the other node connected to the link and add it to the path. This newly added node becomes the current node, and the process repeats until the current node has no uncovered connected links. Note that node 1 is chosen as the initial node. Step 3) Restriction: When there are no uncovered connected links, return to the previously visited node that still has uncovered connected links. Create a new path and add the node to it. Then, repeat step 2. If the path length exceeds the threshold lth, complete the path planning of the current path and add the node to the new path. Step 4) Termination test: When the termination condition is satisfied, stop and complete all path planning tasks. The termination condition can also be when the network telemetry system covers the entire network topology or the maximum running time is achieved. Algorithm 1 Auxiliary Probes Path Deployment Algorithm Require: Graph G and starting node v0 Ensure: A set of auxiliary probe paths 1: Function: APPD(node v , bool flag) 2: if flag then 3: Create a new path as the current path. 4: Add v to the current path, n ← 1. 5: end if 6: flag ← 0. 7: for all i ∈ V (v) do 8: if (v, i) ̸= None then 9: (v, i) = (i, v) = None. 10: if flag then 11: Create a new path as the current path. 12: Add v and i to the current path, n ← 2. 13: else 14: if n < lth then 15: Add i to the current path, n ← n + 1. 16: else 17: flag ← 1. 18: end if 19: end if 20: flag ←APPD(v, flag). 21: end if 22: return true. 23: end for V. DYNAMIC PROBE PATH DEPLOYMENT SYSTEM In this section, we first analyze the problem of the dynamic probe path deployment. Then, we propose an end-to-end DRL framework and apply it to design the DPPD algorithm. Finally, a transfer learning method is employed to reduce the training time of the DRL model. A. Problem Analysis This subsection analyzes the problem of the dynamic probe path deployment based on section IV-A. For a specific query, we assume that the number of dynamic probe paths in the network telemetry system is Q. We denote the q-th path as dq = h v ′ q,1 , v′ q,2 , · · · , v′ q,Nq i , where v ′ q,i denotes the i-th node that path dq passes through and N′ q is the number of nodes in path dq. The set of links that the q-th path passes through is denoted by L ′ q . The network application not only determines the types of metadata required for the query, but also puts forward requirements for which links should be probed. We refer to the subnet that supports the network application as a service network, which is composed of switches and links that support the network application. The set of links in the service network is denoted by S, S ⊆ E. DPs obtain the metadata required for the query from the switches they pass through. To obtain comprehensive service network information for the controller, the telemetry targets of DPs must include the entire service network, which can be represented as S ⊆ [ Q q=1 L ′ q . (8) 7 Furthermore, since DPs undertake the main task of network telemetry, the deployment of dynamic probe paths has a significant impact on the performance of the telemetry system. During the path planning process, we need to meet users’ various performance requirements as much as possible, including control overhead, latency, coverage, computing resources, storage resources, bandwidth utilization, etc. We represent all performance indicators of a telemetry task as the set F = {f1, f2, · · · , fi , · · · , fm}, where fi ∈ F represents the numerical value of a performance indicator. Because the importance of each performance indicator is different, we set the weight of the performance indicator fi to wi , wi ∈ W, where W = {w1, w2, · · · , wi , · · · , wm}. We add the products of each performance indicator and its weight as the optimization objective for dynamic probe path planning, which can be denoted as C = Xm i=1 wifi , (9) where C is the weighted telemetry revenue which is related to F and W. Considering covering the entire service network, the dynamic probe path planning problem can be formulated as min dq,q=1,··· ,Q C = Xm i=1 wifi (10) s.t. S ⊆ [ Q q=1 L ′ q . (10a) We influence each performance indicator of the telemetry system by adjusting the paths of the DPs. The constraint (10a) ensures that the path of DPs can cover the entire service network, which means that the network information collected by the telemetry system can meet the requirements of the telemetry task. Problem (10) represents a challenging multi-objective optimization problem, particularly in the context of multi-path planning for network telemetry systems. Given the complex and dynamic nature of modern networks, several factors make it difficult to solve this problem directly, including: • Multiple optimization objectives: Network telemetry systems must balance multiple optimization objectives simultaneously. These objectives often conflict with one another, making it challenging to find optimal solutions that meet all requirements. • Diversity of telemetry tasks: Each telemetry task has unique requirements and constraints, requiring algorithms to be flexible to different optimization objectives and scenarios. • Dynamic network environment: Network topologies and traffic can change rapidly, requiring telemetry systems to adapt quickly to changing conditions while minimizing downtime and other performance issues. To address these challenges, the proposed algorithm based on DRL offers several significant advantages over classical heuristic algorithms. Firstly, it automatically adjusts the deployment plan according to diverse users’ requirements, making it more robust than traditional heuristic algorithms. Secondly, it can quickly solve the same type of combinatorial Network Topology Input Greedy Decoder Embeddings Attention Layer 1 2 3 ... ... ... ... n... 1 2 3 ... n... Output probabilities 1-dimensional Convolution Layers P y( t+1 ) The Proposed Model 1 ˆ M i i t t t m c a x = =  ˆ ˆ ( , ) i t t t t a = a x h 1 The selected node 1 t x 2 t x 3 t x n t x 1 t x2 t x3 t x n t x t hFig. 6. The proposed model for the DPPD algorithm. At each time step t, the model contains an embedding layer and an attention layer. optimization problem and adapt to dynamic changes in the network environment. B. Deep Reinforcement Learning Model In this subsection, we introduce the DRL model [22], which is shown in Fig. 6. 1) Model Structure Review: Problem (10) is a combinatorial optimization problem, and we assume that the input set is X .=  x i , i = 0, 1, · · · , n	 . Each input x i is a sequence of network information tuples, containing the connection of the node i with other nodes, the latency information of each port, and so on. We begin by selecting an arbitrary input value y0 ∈ X0. At each decoding step t, t ∈ [0, T], we select yt+1 from the set of available inputs Xt and continue until constraint (10a) is met, which means that no more demand need to be satisfied. The sequence generated by this process can be denoted as Y = {yt, t = 0, · · · , T′}, where T ′ is the sequeue length. We use Yt = {y0, · · · , yt} to denote the decoded sequence at time t. We aim to make the generated stochastic policy as close as possible to the optimal ones. Similar to [38], we adopt the probability chain rule to break down the probability P (Y| X0) of generating the sequence Y, which are expressed as follows: P (Y| X0) = Y T t=0 P (yt+1| Yt, Xt), (11) where P (yt+1| Yt, Xt) is computed by the attention mechanism, which will be described in detail in the next subsection. We denote the affine function that outputs an input-sized vector as g and the state of the RNN decoder as ht, which summarizes the information of previously decoded steps y0, · · · , yt. P (yt+1| Yt, Xt) can be expressed as P (yt+1| Yt, Xt) = softmax (g (ht, Xt)). (12) In addition, according to [38], we can express the recursive update of the problem representation as Xt+1 = f (yt+1, Xt), (13) 8 where the role of the state transition function f is to recursively update the problem representation. The RNN encoder’s complexity is high as it attends to the order of the input set, which is crucial for tasks like text translation where word combination and position impact translation accuracy. However, Problem (10), where the input is the information of a set of nodes, and we do not need to pay attention to their order. Therefore, we simply remove the RNN encoder and use input embedding directly to reduce model complexity. Fig. 6 shows the proposed model. We map the input into a vector space, which may have multiple embeddings corresponding to different input elements that are shared. We utilize 1-dimensional convolutional layers To perform the embedding. 2) Attention Mechanism: The attention layer in Fig. 6 shows the proposed model’s attention mechanism, which is a commonly used structure for processing input. Similar to [39], to extract relevant information from the inputs at decoder step i, we utilize a content-based attention mechanism with a glimpse. The variable-length alignment vector aˆt is used to compute this mechanism. x¯ i t represents the embedded input x i t . Furthermore, ht ∈ R D denotes the memory state of the RNN cell at decoding step t. The variable-length alignment vector aˆt can be computed as aˆt = ˆat  x¯ i t , ht  = softmax (ut), (14) where u i t = v T aˆ tanh Waˆ  x¯ i t ; ht  , (15) aˆt determines the relevance of each input data point for the upcoming decoding step t, and the symbol “;” denotes the concatenation of two vectors. The variables vaˆ and Waˆ are trainable variables. To compute the conditional probabilities, we first calculate the context vector ct with the embedded inputs, defined as ct = X M m=1 aˆ i tx¯ i t , (16) Then, we normalize the values with the softmax function to obtain the conditional probability as follows: P (yt+1| Yt, Xt) = softmax u˜ i t  , (17) where u˜ i t = v T c tanh Wc  x¯ i t ; ct  , (18) and the variables vc and Wc are also trainable. 3) Training Method: We use the classical policy gradient approach to train the network. These methods use the estimated gradient of the expected return concerning the policy parameters to iteratively improve the policy, which is a standard training method in reinforcement learning. The policy gradient algorithm consists of two networks: (i) an actor network that predicts the probability distribution over the next action at any given decision step; (ii) a critic network that estimates the reward for any problem instance from a given state. The critic network consists of a dense layer with ReLU Algorithm 2 Reinforcement Learning Algorithm 1: Initialization: Initialize the actor network and critic network with random weights θ and δ. 2: for i = 1, 2, · · · , epoch do 3: Reset gradients. dθ ← 0, dδ ← 0 4: Sample instances from set M. 5: for all instances m = 1, 2, · · · , batch do 6: t ← 0. 7: while termination condition is not reached, do 8: Choose the next node according to the output probabilities P (yt+1| Yt, Xt). 9: Get the new state Xt+1 . 10: t ← t + 1. 11: end while 12: Compute the reward Rm based on the generated policy. 13: end for 14: Compute dθ and dδ according to the rewards. 15: dθ ← 1 batch batch P m=1 (Rm − V (Xm 0 ; δ))∇θ log P (Y m| Xm 0 ) 16: dδ ← 1 batch batch P m=1 ∇δ (Rm − V (Xm 0 ; δ)) 2 17: Update θ and δ according to dθ and dδ. 18: end for activation and a linear layer with a single output. In the ActorCritic network, the critic network computes a weighted sum of the embedded inputs with the output probabilities of the actor network. To optimize the selected action in the current state, the actor network updates its network parameters through backpropagation. Similar to [40], the details of our training method are shown in Algorithm 2. We consider a class of problems whose set of instances is represented by M. Random weights θ and δ are respectively used to initialize the actor network and critic network. At the beginning of the training, we first obtain the problem instances from the set M, where the variable batch represents the number of instances. For each instance m, we generate feasible sequences based on the output probabilities as the current policy. After the termination condition is reached, we use the generated policy to compute the reward Rm. Then, we compute the policy gradient to update the actor network and critic network, where V (Xm 0 ; δ) is the reward approximation calculated by the critic network. In general, the reinforcement learning algorithm provides an appropriate paradigm for training neural networks for combinatorial optimization problems with similar structures. C. Dynamic Probe Path Deployment (DPPD) Algorithm In this subsection, we design the DPPD algorithm, a key component of AdapINT that is designed based on our proposed DRL model. We first create instances for training. To make our model adaptable to a variety of networks, we assume that whether a switch is directly connected to another switch in the network is completely random. The content of the input X can be adjusted to suit telemetry requirements. In this paper, for convenience, x i ∈ X contains the connection status of the node i to other nodes and the latency information of each p 9 1) State: The state of the DRL network st at the end of time slot t is defined as st = {n˜t, Et} , n˜t ∈ V [ {0} , t = 0, · · · , T, (19) where Et represents the set of links that still have telemetry requirements at time slot t. n˜t ∈ V represents the current node index. n˜t = 0 represents that the algorithm is in the state of creating a new path. 2) Action: At state st, our actions include adding a node to the path and creating a new path. To represent the action of creating a new path, we introduce a virtual variable “0”. When the decoder selects the virtual variable “0” as the next action, it means that we should create a new dynamic probe path. Thus, the actions at state st can be denoted as at = {i| ∃ (˜nt, i) ∈ Et} [ {0} , t = 0, · · · , T. (20) As shown in Fig. 6, we use a greedy decoder to select actions, which can effectively improve the quality of the solution. Therefore, the action with the highest probability is selected at each decoding step. Then, the element n˜t+1 is defined as the action with the highest probability of the current time slot, and Et+1 also needs to update the state based on this action. Moreover, since too many actions can be selected, we need to mask infeasible actions to train faster. We use a masking scheme to force some infeasible solutions to be masked and set their log probability to −∞. Specifically, we enforce masking on the following nodes: (i) nodes not connected to the current node. (ii) nodes that have completed their telemetry requirements. Indeed, using a masking scheme can significantly reduce the solution space, resulting in faster attainment of better solutions. In addition to improving training efficiency, the masking scheme enhances solution reliability. However, this scheme may also result in the exclusion of optimal solutions. Therefore, it is essential to strike a balance between achieving optimality and ensuring reliability. 3) Reward: We set the reward function of the model according to Problem (10), which can be expressed as r = min dq Xm i=1 wifi , q = 1, · · · , Q. (21) When users’ requirements change, users only need to modify the reward function according to their requirements. They can retrain a model capable of meeting new requirements without any manual calculation. D. Transfer Learning for DPPD Algorithm When telemetry requirements change, the DPPD algorithm must retrain the DRL model using a new reward function, which incurs additional costs. Furthermore, if the network environment significantly varies, such as in terms of the network scale, creating multiple training sets to train the network also results in extra expenses. To address these issues, we implemented a transfer learning approach [23] to assist in training the model. This allowed the DPPD algorithm to solve different problems more quickly and be applied to various network environments. Fig. 7 presents the transfer learning process, which deals with different reward functions and training sets. Algorithm Embedding Layer Actor Network Critic Network Input Output Different Reward Functions Different Training Sets Embedding Layer Actor Network Critic Network Input Output The Pre-trained Network Fig. 7. Transfer learning model. 3 details the pseudocode for the transfer learning approach, which shares a similar training method shown in Fig. 6. To accommodate various reward functions, we need to load the pre-trained parameters of the embedding layer and retrain the actor and critic networks. Similarly, when facing different training sets, we should load the pre-trained parameters of the actor and critic networks and retrain the embedding layer. In cases where both the reward function and training set are different, we can utilize the transfer learning approach to retrain the entire model. This is because even though all networks require retraining, some parameters from specific networks can still be reused in the new model, thus reducing the training time. Once the entire network converges, we end the model training. Algorithm 3 Transfer Learning Training Algorithm Initialization: Load the pre-trained model parameters. Function: Training for different reward functions 1: Input: The training set with the different reward functions. 2: Get the model parameters from the pre-trained model. 3: Retrain the actor network and the critic network. 4: while Termination condition is not met do 5: Train the deep reinforcement model. 6: end while 7: Output: The re-trained network. Function: Training for different training sets 8: Input: The different training set. 9: Get the model parameters from the pre-trained model. 10: Retrain the embedding layers. 11: while Termination condition is not met do 12: Train the deep reinforcement model. 13: end while 14: Output: The re-trained network. VI. PERFORMANCE EVALUATION In this section, we simulate the AdapINT in Python 3 on the platform with an Intel (R) Core (TM) i7-7700k CPU @ 4.20ghz 4.20GHz machine equipped with 8GB RAM. The evaluation of AdapINT consists of two parts: evaluating the APPD algorithm and the DPPD algorithm. For the APPD algorithm, we evaluated the coverage of the algorithm and simulated a fault localization scenario on auxiliary probe paths. For the proposed DPPD algorithm, we compared the telemetry performance of our proposed 10 0 5 10 15 20 25 Switch ID 0 5 10 15 20 25 Switch ID Low High Unconnected Fig. 8. Visibility of basic network information. DPPD algorithm with three traditional algorithms, namely the DFS algorithm, Euler Trail/Circuit, and Latency-Constrained algorithm [18], under various telemetry requirements. Next, we briefly introduce various traditional algorithms. • DFS algorithm is a graph traversal technique that starts at an arbitrary node and explores as far as possible along each branch before backtracking. This algorithm can be implemented using a stack or recursion to keep track of the nodes to visit and the order in which to visit them. The advantage of the DFS algorithm is that it has extremely low time complexity. • The Euler Trail/Circuit algorithm is a graph theory algorithm whose purpose is to make the number of paths reach a theoretical minimum. Specifically, each path extracted from the graph should begin at an odd vertex and end at another odd vertex. Removing one such path from the graph will eliminate a pair of odd vertices. Euler Trail/Circuit is based on the idea of extracting paths iteratively between pairs of odd vertices until all edges or vertices are removed from the original graph. This algorithm has the advantage of minimizing the number of generated paths, which makes it an efficient option for reducing telemetry overhead. • The Latency-Constrained algorithm controls telemetry latency by setting limits on the maximum latency threshold Tmax during path planning, such as IntOpt and NetView [19], [20]. Since the setting of the maximum latency threshold Tmax is subjective, for convenience, we set the threshold as a linear function related to the number of switches, and apply it to the DFS algorithm. We analyze the performance of AdapINT on random topologies without isolated nodes. Considering the clogged drain effect and the straggling herd effect [41], we refer to the prototype on Mininet and adopt the latency setting scheme proposed in [42]. It is difficult for traditional algorithms to support complex network topologies with multiple services. To simplify the evaluation, we assumed that all switches in the network provided the same service and determined the metrics that users care about according to different scenarios. These metrics were not assigned specific weights since their relative importance may vary among different scenarios. A. Evaluation of APPD Algorithm In this subsection, we aim to demonstrate the reliability of our APPD algorithm. To achieve this, we simulate a 0 5 10 15 20 25 Switch ID 0 5 10 15 20 25 Switch ID Failed Unknow Low High Unconnected Fig. 9. Network fault location information. network consisting of 30 switches. The basic information collected by the auxiliary probe is the link latency, which is vital for assessing the performance of our algorithm. By analyzing the results obtained from this experiment, we can determine whether the algorithm can deploy auxiliary probe paths efficiently in a real-world network. Fig. 8 is a grid diagram composed of basic network information collected by APs, which can intuitively visualize networkwide traffic loads. The black grid indicates no link connection, while the green grid indicates the load according to the depth of the colour. Through APs, we can efficiently gather networkwide critical information. As demonstrated in Fig. 8, we can quickly identify areas experiencing heavy congestion through the darker colouration. Fig. 9 shows the network information grid diagram for random link failures. The red grids signify faulty links, while the yellow grids represent unknown links. We can observe a significant reduction in the range of faults that the controller needs to troubleshoot, thereby enhancing the efficiency of fault diagnosis. These results demonstrate the effectiveness of our proposed APPD algorithm, which is instrumental in facilitating telemetry analysis. B. Evaluation of DPPD Algorithm In this subsection, we analyze the performance of the DPPD algorithm in various scenarios. During training, we created 64000 instances of the network topology whose number of nodes is 30. Our model was trained for 50 epochs with a batch size of 1280. Both the Actor network and the critic network have a learning rate of 0.0001. To demonstrate that AdapINT can meet various telemetry requirements, we evaluated the network telemetry performance in the following three scenarios: (i) Scenario 1. Consider scenarios such as video conferencing and online gaming. Weighted telemetry revenue comprises control overhead and telemetry latency, and users are more concerned about telemetry latency. We refer to telemetry tasks in this scenario as Latency and Control Overhead-Prioritized Latency-Aware telemetry tasks (LO-L telemetry tasks). (ii) Scenario 2. Consider scenarios such as cloud computing services. Weighted telemetry revenue is also composed of control overhead and telemetry latency, but users pay more attention to control overhead. We refer to telemetry tasks in this scenario as Latency and Control Overhead-Prioritized Control Overhead-Aware telemetry tasks (LO-O telemetry tasks). 11 5 10 15 20 25 30 Number of Switches 10 1 10 2 Weighted Telemetry Revenue AdapINT Latency-Constrained Depth-First-Search Euler Trail/Circuit Fig. 10. The weighted telemetry revenue in LO-L telemetry task. 5 10 15 20 25 30 Number of Switches 10 1 10 2 Telemetry Latency(ms) AdapINT Latency-Constrained Depth-First-Search Euler Trail/Circuit Fig. 11. Telemetry latency in LO-L telemetry task. 5 10 15 20 25 30 Number of Switches 10 0 10 1 10 2 Weighted Telemetry Revenue AdapINT Latency-Constrained Depth-First-Search Euler Trail/Circuit Fig. 12. The weighted telemetry revenue in LO-O telemetry task. 5 10 15 20 25 30 Number of Switches 10 0 10 1 10 2 Number of Probes AdapINT Latency-Constrained Depth-First-Search Euler Trail/Circuit Fig. 13. Telemetry overhead in LO-O telemetry task. 0 20 40 60 80 100 Number of Probes 0 50 100 150 200 250 Telemetry Latency (ms) Latency-Constrained Depth-First-Search Euler Trail/Circuit AdapINT (Scenario 1) AdapINT (Scenario 2) Fig. 14. Telemetry latency and the number of probes in different algorithms. 5 10 15 20 25 30 Number of Switches 10 1 10 2 Weighted Telemetry Revenue AdapINT Latency-Constrained Depth-First-Search Euler Trail/Circuit Fig. 15. The weighted telemetry revenue in LOBLO telemetry task. (iii) Scenario 3. Consider scenarios such as intelligent transportation systems and the Internet of Things. Weighted telemetry revenue consists of three metrics: control overhead, telemetry latency, and bandwidth utilization. Users are more concerned with control overhead and telemetry latency. We refer to telemetry tasks in this scenario as Latency, Control Overhead, and Bandwidth Utilization-Prioritized Latency and Control Overhead-Aware telemetry tasks (LOB-LO telemetry tasks). Next, we analyze the performance of AdapINT in three scenarios to illustrate the advantages of the DPPD algorithm in various optimization objectives. For convenience, we denote the control overhead weight as w1, the latency weight as w2, and the bandwidth utilization weight as w3. To evaluate control overhead, we focused on the controller and identified probe collection as the main cause of such overhead. Specifically, as the number of probes that need to be processed per unit of time increases, telemetry overhead also increases. Therefore, we estimated telemetry overhead by measuring the number of probes that need to be processed. As for telemetry latency, we considered the time it takes for the controller to collect all available probes before performing telemetry analysis. In particular, we determined the telemetry latency of AdapINT by identifying the return time of the probe from the longest path. Then, bandwidth utilization is related to the total amount of network information collected by all probes. 1) Scenario 1: Since telemetry latency is important, we set the latency weight to 0.9, w2 = 0.9, and the overhead weight to 0.1, w1 = 0.1. Fig. 10 shows the weighted telemetry revenue of various algorithms in the LO-L telemetry tasks. The weighted telemetry revenue of all algorithms increases with the size of the network topology, as larger topologies require more probes and longer paths. Additional probes lead to heightened network overhead, while longer paths result in longer telemetry latency. We observe that AdapINT can achieve the weighted telemetry revenue at least 30% lower than traditional algorithms. Furthermore, due to the telemetry latency constraints, the weighted telemetry revenue of the Latency-Constrained algorithm has been shown to perform exceptionally well, second only to AdapINT. However, Euler Trail/Circuit, which uses the fewest number of probes and tends to generate ultra-long paths, may not be suitable for LO- 12 20 40 60 80 100 Instance Number 0 50 100 150 200 Weighted Telemetry Revenue AdapINT Latency-Constrained Depth-First-Search Euler Trail/Circuit Fig. 16. The weighted telemetry revenue in Scenario 1. L telemetry tasks. As such, there may be better choices than Euler Trail/Circuit in Scenario 1. Fig. 11 shows the telemetry latency of various algorithms in detail. We can observe that even in large-scale networks, the telemetry latency of Dynamic INT remains low. Although there are fluctuations in telemetry latency, it is still the best among the four algorithms. Although the latency-constrained algorithm limits the telemetry latency, the fixed constraint setting by subjective judgment has defects in adapting to different network topologies. Determining the optimal constraint for different network scenarios remains a challenging task. 2) Scenario 2: We set the overhead weight to 0.9, w1 = 0.9, and the latency weight to 0.1, w2 = 0.1 in LO-O telemetry tasks. In Fig. 12, we show the weighted telemetry revenue of different algorithms. Similarly, AdapINT is also the best after retraining the model according to telemetry requirements. We can find that Euler Trail/Circuit and Depth-First Search excel in various network sizes, showcasing their strengths. Fig. 13 shows the relationship between the number of probes and the size of the topologies. While Euler Trail/Circuit generates minor probes, its weighted telemetry revenue is not always the best and can sometimes be worse than the DFS algorithm. This is because Euler Trail/Circuit only considers the control overhead and does not consider telemetry latency. Although telemetry latency may not be critical in LO-O telemetry tasks, the significant telemetry latency associated with Euler Trail/Circuit ultimately affects its weighted telemetry revenue. Thus, compared to traditional algorithms, AdapINT achieves a better balance of multiple objectives. Fig. 14 displays a scatterplot of telemetry latency and the number of probes for various algorithms. It can be observed that the LatencyConstrained algorithm effectively controls telemetry latency within the threshold, while Euler Trail/Circuit can minimize the number of probes. Additionally, AdapINT can update path planning according to telemetry requirements by retraining the model, which is impossible with traditional algorithms. This demonstrates that AdapINT can solve multi-objective optimization problems with diverse telemetry requirements. 3) Scenario 3: Different from the above scenario, weighted telemetry revenue is composed of control overhead, telemetry latency and bandwidth utilization in LOB-LO telemetry tasks. Since both control overhead and telemetry latency are significant, we set the overhead weight and latency weight to 0.4, w1 = 0.4 and w2 = 0.4. Then, we set the bandwidth utilization weight to 0.2, w3 = 0.2. Fig. 15 shows the weighted telemetry revenue of various algorithms in LOB-LO 20 40 60 80 100 Instance Number 0 20 40 60 80 Weighted Telemetry Revenue AdapINT Latency-Constrained Depth-First-Search Euler Trail/Circuit Fig. 17. The weighted telemetry revenue in Scenario 2. 5 10 15 20 Epoch 0 500 1000 1500 2000 2500 Loss Training without TL Training with TL (different reward functions) Training with TL (different training set) Fig. 18. Loss values of transfer learning aided DPPD algorithm. telemetry tasks. We can find that AdapINT is still the best. Compared with the traditional algorithm, it can be seen that AdapINT can meet the user’s requirements no matter whether the type or weight of the metric is changed. AdapINT is more suitable for multi-user or network environments where telemetry requirements change frequently. Comprehensively analyzing the above three scenarios, we can find that AdapINT can meet users’ requirements in various scenarios. Moreover, AdapINT also has the adaptability to dynamically update policies based on the current environment and experience. Taking Scenario 1 and Scenario 2 as examples, we make the network topology and link latency change frequently to verify its adaptability. Fig. 16 shows the weighted telemetry revenue in Scenario 1. Compared with other algorithms, we can find that AdapINT’s weighted telemetry revenue is the most stable. The telemetry performance changes caused by dynamic network environments are relatively small. As shown in Fig. 17, we can also reach the same conclusion in Scenario 2. It is worth noting that AdapINT has the smallest variance in both scenarios, indicating that our proposed DPPD algorithm can adapt to changes in the network environment. This is attributed to the robustness of the DRL algorithm, which enables it to maintain outstanding performance even in the presence of noise and interference. C. Evaluation of Transfer Learning In this subsection, we use Scenario 2 as an example to present the performance of transfer learning using a smallscale network. For different reward functions, we use the pre- 13 6 8 10 12 14 Switch Number 0 1 2 3 4 5 6 7 8 The weighted telemetry revenue Training without TL Training with TL (different reward functions) Training with TL (different training set) Fig. 19. The weighted telemetry revenue of transfer learning aided DPPD algorithm. trained model from Scenario 1. For different training sets, we use the pre-trained model that was trained on the larger networks’ training set. Fig. 18 shows the training efficiency of transfer learning. We observe that for both different reward functions and different training sets, transfer learning effectively reduces the training epochs needed to achieve convergence from approximately 9 to 4. Fig. 19 shows that the performance of the path deployment using the training with transfer learning is similar to that without transfer learning. This indicates that transfer learning significantly reduces the complexity of training the DRL model and requires fewer epochs for model training without compromising the performance of the DPPD algorithm. This makes the DPPD algorithm superior in solving the challenge of telemetry requirements changes. VII. CONCLUSIONS In this paper, we propose AdapINT, a dual-timescale network telemetry system. We design a network telemetry architecture consisting of APs and DPs, where APs are forwarded along auxiliary probe paths and DPs are forwarded along dynamic probe paths. We have developed two path deployment algorithms to make AdapINT adaptable to the dynamic network environment: a low-complexity APPD algorithm and a DRL-based DPPD algorithm. In addition, the DPPD algorithm also uses transfer learning to reduce the training time. Simulation results show that AdapINT is capable of automatically identifying telemetry solutions that meet diverse telemetry requirements without the need for a manual calculation. Furthermore, our system demonstrates good robustness in dynamic network environments. In future work, we aim to address the challenge of reducing the complexity of model training for ultra-large-scale data center networks.",
		"summary": "—In-band Network Telemetry (INT) has emerged as a promising network measurement technology. However, existing network telemetry systems lack the flexibility to meet diverse telemetry requirements and are also difficult to adapt to dynamic network environments. In this paper, we propose AdapINT, a versatile and adaptive in-band network telemetry framework assisted by dual-timescale probes, including long-period auxiliary probes (APs) and short-period dynamic probes (DPs). Technically, the APs collect basic network status information, which is used for the path planning of DPs. To achieve full network coverage, we propose an auxiliary probes path deployment (APPD) algorithm based on the Depth-First-Search (DFS). The DPs collect specific network information for telemetry tasks. To ensure that the DPs can meet diverse telemetry requirements and adapt to dynamic network environments, we apply the deep reinforcement learning (DRL) technique and transfer learning method to design the dynamic probes path deployment (DPPD) algorithm. The evaluation results show that AdapINT can redesign the telemetry system according to telemetry requirements and network environments. AdapINT can reduce telemetry latency by 75% in online games and video conferencing scenarios. For overhead-aware networks, AdapINT can reduce control overheads by 34% in cloud computing services. ",
		"id": "UUID24"
	},
	{
		"document": "Introduction The cellular technology forms the foundation of mobile phone networks, earning them the common name 'cell phones.' Instead of relying on a single large transmitter, cellular 1 arXiv:2310.19195v1 [cs.NI] 29 Oct 2023 technology utilizes multiple small transmitters linked together. These cellular networks serve as high-speed voice and data communication networks, capable of accommodating cellular devices with advanced multimedia features and seamless roaming capabilities [1]. Over time, cellular networks have become the cornerstone of the communications industry. As illustrated in [1] and depicted in Figure 1, subscribers gain access to a cellular network through radio signals facilitated by a radio access network. This component of the mobile telecommunication system employs radio access techniques to connect specific devices to other network elements, particularly the Core Network. The Core Network undertakes essential operations such as traffic routing and handling subscriber requests. Additionally, it serves as the link between the cellular network and other networks like the Public Switched Telephone Network (PSTN) and the Internet. The PSTN allows users to make landline calls, encompassing various telephone networks worldwide. Furthermore, Internet connectivity empowers the cellular network to provide cutting-edge multimedia services. Figure 1: A cellular network’s architecture In summary, a cellular network consists of discrete areas known as cells or cell sites, strategically dispersed across a wide geographic area. Each cell site is equipped with one or more transceivers that provide radio coverage for the respective region, typically housed within a base station. To prevent interference and ensure satisfactory service quality, each cell site operates on different frequencies than nearby cells. By positioning cells in close proximity to one another, the network can offer radio coverage over a large area and facilitate seamless communication for mobile devices transitioning between cells, such as a cell phone in a moving vehicle. The range of a cell can vary significantly depending on factors such as configuration, size, and environmental conditions, spanning from approximately 10 meters to over 25 miles. In larger areas, cell repeaters can be deployed to enhance coverage. As each new generation of cellular networks emerges, bringing with it evolving requirements, technologies, and solutions, significant advancements have occurred. Cellular networks have evolved into the fundamental infrastructure for various applications, including the Internet of Things (IoT) devices and robotics. Presently, with the emergence of 6G and 7G on the horizon, new possibilities and opportunities are beginning to unfold. 2 2 The Cellular network generations 2.1 First Generation (1G) The advent of analog telecommunications standards marked the era of 1G, which primarily focused on delivering foundational voice services. Japan played a pioneering role in the late 1970s by establishing the first mobile network in Tokyo. This was followed by the deployment of NMTs (Nordic Mobile Telephones) in Europe and the introduction of AMPS (Advanced Mobile Phone Service) technology. However, these technologies faced certain limitations. AMPS, for instance, had inherent restrictions in terms of available channels and the associated devices were relatively expensive. Moreover, users encountered challenges such as suboptimal voice quality, capacity constraints, limited security measures, and slow data speeds. One of the major issues with 1G technology was its reliance on analog impulses for information transmission. Analog signals were less effective compared to digital signals, leading to reduced efficiency in communication. This limitation became increasingly apparent as technology advanced and the demand for more sophisticated services grew. Despite these challenges, the introduction of 1G networks laid the foundation for the remarkable advancements that followed. It set the stage for the subsequent generations of mobile networks, which addressed the shortcomings of their predecessors. The transition from analog to digital technology brought about significant improvements in voice quality, data capacity, security features, and transmission speeds. 2.2 Second Generation (2G) The GSM (Global System for Mobile Communication) technology forms the foundation for 2G mobile networks. One of the key features of GSM is terminal mobility, which allows mobile users to move between different locations while maintaining continuous connectivity. This mobility is facilitated by a subscriber identity module (SIM) inserted into the GSM network, which stores the unique number assigned to the mobile user [2]. The SIM card plays a crucial role in enabling personal mobility within the GSM network. The GSM network comprises four main components that work together to provide seamless operation: the mobile device itself, the Base Station Subsystem (BSS), the Network Switching Subsystem (NSS), and the Operation and Support Subsystem (OSS). The BSS consists of the Base Transceiver Station (BTS) and the Base Station Controller (BSC). The NSS includes essential elements such as the Mobile Switching Centre (MSC), Visitor Location Register (VLR), Home Location Register (HLR), Authentication Centre (AC), and Equipment Identity Register (EIR). These components collaborate to ensure the smooth functioning of the GSM network. Unlike the analog radio signals used in 1G networks, 2G networks introduced the use of digital radio signals. This digitalization allowed for multiple users to be accommodated on a single channel through a process called multiplexing. As a result, 2G networks enabled cell phones to be used for both voice and data communication. The second generation (2G) of wireless mobile communication systems, driven by groundbreaking technology and the services it offered, was a resounding success. Figure 2 provides an overview of the GSM architecture, illustrating how 2G networks operate. 3 Figure 2: GSM architecture 2.3 Third Generation (3G) The third generation of cellular networks brought significant advancements in data speed and network capabilities, enabling a wide range of services such as video calling, video streaming, gaming, and fast internet browsing. The Universal Mobile Telecommunications System (UMTS) was established as a result, combining and enhancing the capabilities of second-generation Global System Mobile (GSM) networks to enable international roaming [3, 4]. UMTS, being a part of the 3G family, offers higher data speeds compared to its predecessors. Under the International Telecommunication Union (ITU) IMT-2000 standard, 3G systems are classified as capable of supporting high-speed data ranging from 144 kbps to over 2 Mbps [3]. IMT-2000 serves as a global standard for 3G systems. Furthermore, 3G systems employ five key radio technologies, including Time Division Multiple Access (TDMA), Code Division Multiple Access (CDMA), and Frequency Division Multiple Access (FDMA), which facilitate efficient communication [4]. TDMA systems divide the channel time into frames, with each frame further divided into time slots, allowing only one user to send or receive data in each slot. CDMA, on the other hand, multiplies the narrowband message signal by a spreading signal (code) with a wide bandwidth before modulation and transmission, allowing for multiple users to share the same frequency band through spreading. FDMA assigns a specific frequency band or channel to each user, ensuring that no other user can utilize the same band during a call. In conclusion, 3G systems have the capability to coexist with 2G technologies, enabling seamless transition and compatibility. These systems are designed to accommodate future growth and expansion with minimal costs, aiming to provide broader coverage and enhanced services [5]. 4 2.4 Fourth Generation (4G) The advancement of next-generation systems, commonly referred to as 4G, became imperative as the existing IMT-2000 standard failed to address the challenges related to higher data rates and capacity. A 4G system offers users significantly higher data rates for voice, data, and streaming multimedia compared to 3G or 2G systems. To achieve faster user data rates and lower latency, 4G leverages research conducted in the fields of GSM, EGPRS, WCDMA, and HSPA. With transmission rates exceeding 20 Mbps, 4G technology delivers impressive data transfer capabilities. Additionally, 4G networks exhibit significantly lower latency, which is crucial for real-time interactions like video conferencing, where minimal delay is essential. The transmission and reception capabilities of 4G are enabled by two key technologies: Orthogonal Frequency Division Multiplexing (OFDM) and Multiple Input Multiple Output (MIMO). OFDM serves as the foundation for 4G systems by dividing the available spectrum into several sub-channels. Each of these narrowband sub-channels experiences nearly flat fading, simplifying the equalization process. The frequency responses of these sub-channels are orthogonal and overlapping, providing high spectral efficiency and justifying the name 'Orthogonal Frequency Division Multiplexing' [6]. OFDM offers faster speeds compared to the primary 3G technologies. On the other hand, MIMO is a 4G LTE antenna technology that utilizes multiple antenna components at both the transmitter and receiver ends to enhance data rates and improve signal quality. The implementation of MIMO technology in 4G systems contributes to the overall performance improvement and efficiency in wireless communications. These technological advancements in 4G networks have revolutionized the way users experience mobile communication, enabling high-speed data transfer, low-latency applications, and enhanced multimedia services. 2.5 Fifth Generation (5G) The emergence of 5G cellular networks stems from the limitations of previous generations, such as 4G, in meeting the increasing demands for high-speed and reliable connectivity. While 4G networks initially brought about a significant leap in data rates, the International Telecommunication Union (ITU) recognized the need for a new standard to address the evolving requirements of mobile communication. This led to the definition of the fifth generation (5G) as 'International Mobile Telecommunication 2020' (IMT-2020) [8]. The COVID-19 pandemic has further underscored the significance of 5G networks. With the widespread adoption of remote work, online education, and video conferencing, there has been a massive surge in mobile data traffic. The need for seamless video streaming, high-quality video conferencing, and real-time communication has become more critical than ever. 5G networks are designed to handle this increased demand by offering higher data rates, accommodating a larger number of wireless connections, reducing communication latency, and improving energy efficiency [7]. At the architectural level, the 5G core network introduces several key advancements. One notable aspect is the emphasis on control-plane/user-plane separation. By separating these two planes, the network can optimize resource allocation, improve scalability, and enhance flexibility in managing network services. This architectural approach enables efficient handling of control signaling and data traffic, leading to improved overall network performance. The 5G core network is designed based on a service-based architecture, which allows 5 for the creation, deployment, and orchestration of network services in a more modular and flexible manner. This service-based approach enables efficient management of network functions, easier integration of new services, and enhanced customization according to specific application requirements. Figure 3 provides a high-level visualization of the 5G core network, illustrating the service-based architecture and the various network components involved [9]. In summary, the transition to 5G networks is driven by the need for advanced connectivity capabilities that can meet the growing demands of modern applications and services. The COVID-19 pandemic has further accelerated the adoption of 5G due to the increased reliance on remote communication and digital connectivity. With its higher data rates, increased wireless capacity, reduced latency, and improved energy efficiency, 5G represents a significant leap forward in cellular network technology, enabling a wide range of innovative applications across industries and sectors. Figure 3: 5G’s core network’s architecture The User-Plane Function (UPF) in 5G plays a crucial role in connecting the actual data flowing through the Radio Area Network (RAN) to the Internet. Its primary task is to handle the data traffic, ensuring its seamless transmission and reception. On the other hand, the control-plane functions comprise various components, with one of them being the Session Management Function (SMF). The SMF takes charge of managing user sessions, from their establishment to modification and termination, while also allocating IP addresses for IP PDU (Protocol Data Unit) sessions. Another essential component is the Access and Mobility Management Function (AMF), which handles connection and mobility management activities. Although it gathers all connection- and session-related data from the User Equipment (UE), it is responsible for managing the connections and movements of UEs throughout the network. The core network also includes other functions such as the Network Exposure Function (NEF), Network Repository Function (NRF), and Policy Control Function (PCF), each serving specific roles in the overall network architecture. As we look at the evolution of cellular networks from 1G to 5G, it becomes evident why it is essential to keep advancing. The increasing demands for faster data rates, higher capacities, low latency, and improved connectivity have been driving the continuous development of cellular technologies. This evolution has led researchers to explore and investigate the possibilities of the next generation, 6G. As we move towards the future, research and development efforts are already underway to envision 6 and shape the cellular networks of 6G, which are expected to push the boundaries of wireless communication even further. 2.6 Sixth Generation (6G) The objective of this chapter is to conduct a comprehensive examination of the characteristics of the upcoming generation of mobile networks, drawing on existing research documented in the bibliography. 6G wireless networks are poised to deliver even greater bandwidth compared to 5G, along with significantly lower latency. It is projected that 6G networks will gain widespread usage around 2030, and prominent telecommunications companies like Ericsson, Samsung, and Nokia are already spearheading research efforts in this area. The exponential growth of big data in everyday life highlights the imperative to transition to a new generation of networks. The handling of today’s data, characterized by its large volume and often real-time processing requirements, poses challenges. Moreover, this data is frequently semi-structured or unstructured. By 2030, it is expected that an advanced digital society will rely on ubiquitous wireless access. These factors serve as driving forces behind the initiation of 6G research [10]. The 6G mobile network is anticipated to utilize air fiber technology, employing masts and transceivers installed on tall buildings to establish local networks that can provide remarkable speeds. This approach differs from relying solely on average performance figures. The combination of air fiber technology and local networks will enable the secure transmission of data from transmitters to destinations [12]. In its initial implementation, the 6G network will leverage the existing design and advantages of 5G, including wider frequency bands and an improved decentralized network architecture [11]. Therefore, it is reasonable to assume that millimeter waves, which are extremely high frequencies in the radio spectrum, will be employed, as in 5G. The availability of ample spectrum resources justifies the utilization of such frequencies in both 5G and 6G, with numerous unlicensed bands being available. The fifth generation of cellular networks is characterized by its cloud-based nature. Looking ahead, during the design, deployment, and operation of 6G networks, machine learning (ML) and artificial intelligence (AI) are expected to play pivotal roles. Intelligent, self-organizing, and cost-effective 6G networks are necessary, and ML offers pragmatic solutions that can reshape the future of wireless network technologies. The integration of AI and ML aims to generate new revenue streams and enhance network performance. Deep learning AI approaches have already started replacing conventional algorithms, leading to significant reductions in power consumption and improved system performance [13]. To further illustrate the potential of AI in 6G, consider the example of self-driving cars. These autonomous vehicles have faced challenges in applying AI and ML algorithms to real-world situations, hindering their ability to learn effectively. In addition to addressing this problem, autonomous vehicles need to be aware of their location, surroundings, and other road users to compute routes and arrival times. This necessitates the rapid construction of on-the-fly networks while also being part of larger networks. Processing the immense volume of real-time data involved requires extremely low latency, even lower than what the 5G network can provide. It is evident that 6G will revolutionize daily life, particularly in applications such as self-driving cars, addressing significant challenges from their troubled past. 7 2.7 Seventh Generation (7G) and onwards Looking ahead, the question arises: what lies beyond 6G? Despite 6G not yet being fully deployed, research on the subsequent generation has already commenced. Given the rapid transitions from 1G to 5G, it appears inevitable that a more advanced network will follow 6G. So, how might 6G be further improved? The forthcoming intelligent cellular technology, known as 7G, is set to succeed 5G and 6G, promising significantly enhanced capacity, even higher frequencies, and vastly lower latency. Anticipated to meet the requirements of ultra-high bandwidth, nearly zero latency, and seamless integration, 7G will enable key applications such as data analysis, imaging, and artificial general intelligence. Speculations suggest that quantum computing could underpin 7G, but the probability of this happening is considered low. While quantum computing may be more powerful, it remains slower, and network evolution primarily relies on speed, making quantum computing less relevant in achieving the desired advancements in cellular technology. In the quest for ever-evolving mobile network technologies, the emergence of 7G raises intriguing possibilities. With its envisioned capabilities, 7G could potentially revolutionize various industries, including healthcare, transportation, and entertainment. Seamless connectivity, real-time data processing, and advanced communication systems are expected to reshape the way we live, work, and interact with technology. As the demand for faster, more reliable, and intelligent networks continues to grow, the development of 7G holds the potential to unlock new opportunities and propel innovation to unprecedented levels, bringing us closer to a truly interconnected and digitally empowered society. 3 Future references The evolution from 1G to 7G wireless networks has been a remarkable journey, marked by significant advancements in capacity, latency reduction, and technological innovation. Each generation of mobile networks has pushed the boundaries of communication, transforming the way we connect, communicate, and access information. From the introduction of 1G, which revolutionized mobile voice communication, to the lightningfast speeds and seamless connectivity promised by 7G, the progression has been driven by a constant quest for improved performance and enhanced user experiences. The emergence of 7G as the next frontier in mobile network technology offers tremendous potential to address the ever-increasing demands of the digital era. With its dramatic increase in capacity and near-zero latency, 7G brings us closer to achieving our objectives of seamless, real-time communication and data processing. It promises to empower industries with transformative applications such as data analysis, imaging, and artificial general intelligence, revolutionizing sectors ranging from healthcare to transportation and entertainment. However, the realization of the full potential of 7G comes with significant challenges. The high costs associated with deploying such advanced technology pose financial considerations that need to be carefully addressed. While 7G represents a major milestone, it is important to recognize that it is not the endpoint of the evolutionary journey. The pursuit of cost efficiency will be a key driver for the next stage of network evolution, spurring further advancements in standards, technologies, and business models. As we reflect on the trajectory of mobile network evolution, it is clear that each generation has built upon the achievements of its predecessor, propelling us towards 8 increasingly sophisticated and connected digital landscapes. The transition from 1G to 7G has witnessed transformative shifts in capacity, latency, and network architecture, enabling unprecedented levels of communication and data exchange. The journey has been fueled by collaboration among industry stakeholders, visionary research, and a commitment to meet the evolving needs of a rapidly changing world. In summary, the transition from 1G to 7G represents a remarkable continuum of advancements in mobile network technology. While 7G holds the promise of dramatically increased capacity, reduced latency, and revolutionary applications, we are reminded that the path to fully realizing its potential is still unfolding. Balancing technological advancements with cost efficiency will be crucial as we navigate the challenges and opportunities that lie ahead. The future of mobile networks, driven by the pursuit of seamless connectivity, real-time processing, and affordability, holds immense potential to transform industries and empower individuals in an interconnected and digitally empowered society. 4 Conclusion The wireless mobile communication industry is experiencing rapid growth and continuous development. Over the years, cellular networks have evolved at an astonishing pace. This paper has aimed to comprehensively evaluate and trace the evolution of mobile networks, starting from the first generation (1G) and progressing to the fifth generation (5G). Additionally, we have discussed the future generations, including the anticipated advancements of 6G and the potential prospects of 7G. Throughout this study, we have delved into the transformative changes brought about by each generation, from the introduction of voice communication in 1G to the revolutionary advancements in data rates, connectivity, and latency reduction in 5G. Moreover, we have explored the potential capabilities and applications expected in the upcoming generations, such as the increased bandwidth and the integration of intelligent technologies in 6G, as well as the remarkable improvements in capacity, latency, and transformative applications in 7G. While this research provides valuable insights into the evolutionary path of cellular networks, it is important to acknowledge that the journey does not end with 7G. The wireless communication landscape will continue to evolve, fueled by advancements in technology, industry collaboration, and the ever-increasing demands of a digitally interconnected society. Looking ahead, further research and exploration are necessary to fully understand and harness the potential of future generations of cellular networks. We encourage researchers and industry professionals to delve deeper into the subject matter, explore emerging technologies, and engage in interdisciplinary collaborations to unlock new possibilities and drive innovation in the field. In conclusion, this paper has provided a comprehensive overview of the evolution of mobile networks, ranging from 1G to the forthcoming 7G. It highlights the transformative changes, technological advancements, and future prospects of wireless communication. As we continue to push the boundaries of connectivity and performance, the cellular network landscape will remain a dynamic and exciting domain, propelling us towards a more connected and digitally empowered future.",
		"summary": "The evolution of cellular networks has played a pivotal role in shaping the modern telecommunications landscape. This paper explores the journey of cellular network generations, beginning with the introduction of Japan’s first commercial 1G network by Nippon Telegraph and Telephone (NTT) Corporation in 1979. This analog wireless network quickly expanded to become the country’s first national 1G network within a remarkably short period. The transition from analog to digital networks marked a significant turning point in the wireless industry, enabled by advancements in MOSFET (Metal-Oxide-Semiconductor Field Effect Transistor) technology. MOSFET, originally developed at Bell Labs in 1959, underwent modifications to suit cellular networks in the early 1990s, facilitating the shift to digital wireless mobile networks. The advent of the 2G generation brought forth the first commercial digital cellular network in 1991, sparking recognition among manufacturers and mobile network operators of the importance of robust networks and efficient architecture. As the wireless industry continued to experience exponential growth, the significance of effective network infrastructure became increasingly evident. In this research, our aim is to provide a comprehensive overview of the entire spectrum of cellular network generations, ranging from 1G to the potential future of 7G. By tracing the evolution of these networks, we aim to shed light on the transformative developments that have shaped the telecommunications landscape and explore the possibilities that lie ahead in the realm of cellular technology",
		"id": "UUID25"
	},
	{
		"document": "INTRODUCTION Video streaming services have evolved dramatically, transitioning from simple on-demand platforms to sophisticated and real-time interactive systems. A statistics report shows that the global video streaming market size was valued at $89 billion in 2022, and is expected to grow at a compound annual growth rate of 21.5% until 2030 [1]. Emerging video streaming services, including intelligent short video streaming, extreme immersive virtual reality (VR), and holographic video streaming, demand tailored network management to satisfy users’ personalized requirements [2]. For instance, by adding multiple video branches and view angles, intelligent short video streaming emphasizes the interaction with users, which is triggered by the users’ swipe and rotation behaviors. To satisfy smooth video playback while reducing bandwidth consumption, communication networks should accurately mine the preferences and behavior characteristics of users for better intelligent video caching. Furthermore, extreme immersive VR and holographic video streaming aim at providing high-fidelity three-dimension (3D) object display and immersive experience, which demand efficient coordination between sensing, video tile transmission, video rendering, and specialized video codecs. To satisfy these evolving requirements, efficient network management through advanced communication technologies becomes an imperative endeavor. Xinyu Huang, Haojun Yang, Shisheng Hu, and Xuemin (Sherman) Shen are with the Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, N2L 3G1, Canada (E-mail: {x357huan, haojun.yang, s97hu, sshen}@uwaterloo.ca). Advanced communication technologies, such as enhanced mobile broadband (eMBB)-Plus, native artificial intelligence (AI), sensing, network slicing, and digital twin (DT), are expected to satisfy the above requirements [3], [4]. For instance, eMBB-Plus can provide gigabit-level data rates and seamless connections, while native AI enables intelligent data processing and decision-making. Moreover, sensing-related techniques facilitate real-time 3D object modeling, and network slicing is used to isolate network resources for prescribed service requirements. To seamlessly integrate these technologies for video streaming services, a holistic network management architecture is essential. As a promising approach, DT can realize the holistic network virtualization for video streaming services by exploiting its real-time monitoring, analytics, and emulation capabilities [5]. Specifically, DTs can characterize users’ real-time status, quality of service (QoS), and quality of experience (QoE) through native AI and sensing, and provide an emulation environment for network management by implementing tailored network slicing and resource allocation policies on eMBB-Plus. By leveraging the capabilities of DTs, an efficient holistic network management architecture for video streaming services can be realized. However, developing an efficient DT-driven network architecture for video streaming services faces many challenges, such as • Lack of Efficient Data Abstraction Mechanism: An efficient data abstraction mechanism should be developed to facilitate the real-time and intricate interplay among DTs, slice domain, and physical domain, which includes the determination of data types, granularities, and features. • Lack of Comprehensive Performance Evaluation Framework: Since the DT performance consists of two-fold aspects, i.e., the accuracy and cost of itself, and its impact on network performance, it is crucial to develop a new and comprehensive performance evaluation framework to evaluate the DT performance. • Lack of Adaptive DT Model Update: Due to the distinct spatiotemporal dynamics that exist in network conditions and user behaviors, as well as diversified service requirements, it is essential to fine-tune tailored DT models to adapt to the dynamics and diversity. In this article, we propose a DT-driven network architecture for video streaming (DTN4VS) to enable network virtualization and tailored network management. Three kinds of DTs, i.e., user DT (UDT), infrastructure DT (IDT), and slice DT (SDT), are built to characterize the network from the userarXiv:2310.19079v1 [cs.NI] 29 Oct 2023 2 level, operation-level, and service-level perspectives, respectively. DTs can provide distilled user information, emulated environment, and tailored network management strategies to realize efficient network management. To tackle the mentioned challenges, we first propose an efficient data collection, fusion, and abstraction mechanism. Secondly, we propose a comprehensive performance evaluation framework, which integrates DT data freshness, QoS/QoE gain, and DT operation cost. Thirdly, we propose an adaptive DT model update method that integrates distributed and transfer learning algorithms to realize computing load balance and computing overhead reduction. A case study pertaining to DT-assisted network slicing for short video streaming is presented, followed by a discussion on potential research issues. The remainder of this article is organized as follows. Firstly, emerging video streaming services and the corresponding communication techniques are discussed, followed by the proposed DTN4VS. Then, we discuss the challenges for DTN4VS and some potential solutions. Next, a case study about DTassisted network slicing for short video streaming is presented. Finally, the open research issues are identified, followed by the conclusion. II. DT-DRIVEN NETWORK ARCHITECTURE FOR VIDEO STREAMING In this section, we first introduce emerging video streaming services and advanced communication technologies, and then the DTN4VS architecture is proposed. A. Emerging Video Streaming Video streaming has become an integral part of our daily lives, revolutionizing how we access the latest information. Various innovative video streaming is emerging, including intelligent short video streaming, extreme immersive VR streaming, and holographic video streaming, which demand enhanced requirements on communication networks, as shown in Table I. 1) Intelligent Short Video Streaming: It has two main characteristics, i.e., rotation-based swipe and multi-branch. The front indicates that the short video streaming will be extended from the current two-dimension (2D) format to 3D format, thus users can watch different angles by rotation-based swipe behaviors. Based on the analysis of user swipe behaviors, video tiles are selectively transmitted to user terminals to reduce network traffic load. The latter means that more video branches will be added to the main video storyline, which boosts users’ interaction. Through intelligent recommendation and buffering, part of critical video branches are preferentially cached in users’ buffers to save bandwidth consumption. 2) Extreme Immersive VR Streaming: It provides immersive and interactive experience by transmitting real-time 3D videos and audio to specialized headsets or mobile devices. Since future VR streaming is expected to provide a panoramic view and ultra-high-definition resolution, field of view (FoV) transmission is an effective method to reduce transmission burden by only transmitting tiles in users’ view. Furthermore, the latency requirements of extreme immersive VR are very stringent, reaching just tens of milliseconds. It is essential to develop advanced sensing technologies to quickly capture users’ macro and micro motions, and optimize the computing process for video rendering. 3) Holographic Video Streaming: It refers to the realtime transmission of 3D holographic content, which usually requires the sensing-assisted communication technology to perceive users’ behavior dynamics and conduct 3D object modeling. Due to the characteristics of ultra-low delay and strong interaction, high-performance computing nodes need to be deployed to quickly compress and render the holographic video. For instance, to quickly respond to users’ movements in the six degrees of freedom (6-DoF), the end-to-end delay needs to be constrained to 5 ms [6]. B. Advanced Communication Techniques for Video Streaming In the rapidly evolving landscape of emerging video streaming, advanced communication techniques can offer groundbreaking solutions to satisfy enhanced requirements, including personalized video buffering, ultra-low-latency interaction, and smooth 3D video transmission. Utilizing advanced technologies, such as eMBB-Plus, native AI, sensing, network slicing, and DT, we can achieve intelligent and efficient network management to improve service quality and watching experience. 1) eMBB-Plus: As a cornerstone technology, eMBB-Plus is designed to provide higher bandwidth capacity, wider access coverage, and smarter caching and computing, which can effectively improve throughput and reduce rebuffering. • For intelligent short video streaming, eMBB-Plus can provide more efficient distributed video caching and collaborative transcoding mechanisms to ensure seamless delivery of high-definition (HD) and ultra-HD (UHD) video segments with minimal buffering. • For extreme immersive VR streaming, eMBB-Plus can provide the increased frequency band and exploit more advanced modulation and coding techniques to ensure the smooth transmission for high-bitrate 3D videos. • In holographic video streaming, since 3D modeling occupies plenty of computing time due to users’ motions and micro-expression dynamics, eMBB-Plus will provide more advanced data offloading and collaborative computing mechanisms to help reduce computing delay. By providing advanced communication, caching, and computing technologies, eMBB-Plus can significantly empower emerging video streaming services to enhance users’ watching experience. 2) Native AI: As a built-in component in next-generation communication networks, native AI is promised to provide more intelligent and efficient data processing methods and resource management strategies [8]. • For intelligent short video streaming, graph neural networks (GNNs) can model the complex relationship between users and contents, which can enable more personalized and context-aware video recommendations and buffering. • For extreme immersive VR streaming, deep reinforcement learning (DRL) algorithms can dynamically and 3 TABLE I EMERGING VIDEO STREAMING SERVICES Video Type Characteristics Video Codec Bandwidth Requirements Latency Requirements Component Intelligent Short Video • Swipe • Multi-Branch H. 264, MPEG 4K: 45 Mbps Several Seconds Segments, 3D Tiles Extreme Immersive VR Video • Interactive • Viewpoint • Rendering X3D, MPEG-I [7] • 8K: 80 ∼ 100 Mbps • 30K: 800 ∼ 1000 Mbps • Strong Interaction Mode: 5 ∼ 10 ms • Weak Interaction Mode: 10 ∼ 20 ms 2D/3D Tiles Holographic Video • Interactive • 3D Modeling • Rendering HEVC, AV1, VP9 4K: 100 Mbps • 6-DoF Movement: 5 ms • 3-DoF Movement: 20 ms 3D Tiles accurately allocate network resources and video tiles to adapt to users’ dynamic behaviors and ensure smooth video playback. • In holographic video streaming, convolutional neural networks (CNNs) can be employed for efficient data compression and semantic extraction, significantly reducing bandwidth consumption while maintaining high fidelity. To further enhance the network management performance, novel AI algorithms should be designed that fully consider the characteristics of communication networks and emerging video streaming services. 3) Sensing: As a crucial technology in next-generation communication networks, sensing can capture users’ real-time macro and micro behaviors, which can help tailor network management for individual users. • For the intelligent short video, advanced facial recognition sensors can capture users’ micro-expressions, which are further analyzed by machine learning algorithms for intelligent video recommendations and buffer control. • In extreme immersive VR streaming, communication and sensing signals can be multiplexed in the time domain, frequency domain, and spatial domain to improve spectrum utilization. For instance, IEEE 802.11 working group proposed the Wi-Fi sensing technology to exploit the features of the physical layer and medium access control, which can measure users’ motion in real time [9]. • For holographic video streaming, the joint communication and sensing technology can effectively improve both data transmission reliability and holographic video resolution via efficient radio frequency spectrum allocation and advanced beamforming technology [10]. The diverse sensing technologies can complement each other to compensate for their respective deficiencies, thereby achieving tailored network management. 4) Network Slicing: Empowered by technologies such as software-defined networking (SDN) and network function virtualization (NFV), network slicing can provide isolated resources for diversified emerging video streaming services to satisfy the differentiated requirements [8]. • For intelligent short video streaming, network resources can be sliced to support real-time analysis of video content and user behaviors, thereby enabling intelligent video recommendation and adaptive video delivery. • In the realm of extreme immersive VR, specialized slices can be allocated to high-performance sensors and computing nodes to realize ultra-low latency and high data rates, which can ensure a truly immersive experience. • For holographic video streaming, dedicated slices can guarantee high bandwidth and computational requirements, facilitating real-time and high-fidelity 3D interactions. Through differentiated resource isolation, network slicing can provide satisfied service quality for emerging video streaming services. 5) Digital Twin: It is defined as a full digital representation of a physical object or a process and real-time synchronization between the physical object or process and its corresponding digital replica [11]. DT is usually classified into three types, i.e., UDT, IDT, and SDT, deployed at the network edges and core. • UDT corresponds to an end user and reflects fine-grained user information, such as real-time network conditions, playback status, and interaction behaviors, etc. UDTs can emulate user status to help the network controller (NC) make tailored resource management strategies. • IDT is a digital mirror of network infrastructure, such as base station (BS) and edge server, which reflects its operation status, traffic load, resource utilization, etc. IDTs can separate the resource operation function from the NC and empower the function with emulated data and tailored strategies. • SDT is constructed by aggregating UDTs and IDTs to obtain coarse-grained distilled information, such as spatiotemporal service demand distribution, resource utilization, etc. By separating the resource planning function from the NC, SDTs can strengthen the function’s capability with emulated data and tailored strategies. Based on the fine- and coarse-grained information and tailored 4 network management strategies provided by DTs, the NC can realize efficient and accurate network management to further improve users’ watching experience. C. DTN4VS To seamlessly integrate these emerging technologies, as shown in Fig. 1, we develop a DTN4VS framework to enhance video streaming service performance. The physical domain includes real-world video streaming infrastructures, while the slice domain leverages network slicing for prescribed QoS. The DT domain provides real-time digital replicas for data analytics, emulation, and network management decisionmaking, where an orchestration among UDTs, IDTs, and SDTs is intelligently coordinated to facilitate efficient network management. 1) Physical Domain: In the physical domain, the proposed DTN4VS meticulously integrates user terminals, radio access networks (RANs), edge networks, and cloud networks to build a holistic video streaming ecosystem. User terminals are not merely endpoints for video delivery but are also equipped with advanced codec technologies and adaptive bitrate (ABR) algorithms for efficient data exchange with RANs. The RAN layer consists of small BSs (SBSs) and macro BSs (MBSs) that employ advanced communication technologies, such as eMBB-Plus and sensing, to support high-throughput and highfidelity video transmission. Edge networks provide localized computing and caching capabilities for latency-sensitive video streaming, while cloud networks handle large-scale video processing and storage. These elements collectively form the backbone of our framework, and through real-time resource scheduling and optimization in the DT domain, we can achieve holistic network management. 2) Slice Domain: Network slicing emerges as a pivotal technology for guaranteeing QoS requirements. A single physical network can be partitioned into multiple isolated slices to satisfy differentiated service requirements. For instance, one slice could be optimized for low-latency and interactive VR video streaming, while another might target high-throughput holographic video streaming. Each slice consists of a unique set of network resources, policies, and protocols, which can enable tailored control over network resources. Through intelligent orchestration in the DT domain, these slices can be dynamically adjusted to meet varying resource demands, thereby achieving a harmonious balance between resource utilization and service quality. 3) DT Domain: In the DT domain, each kind of DTs consists of a finite database and a model pool. For instance, the database of UDTs includes users’ playback-related, networkrelated, and behavior-related data. These data can reflect users’ actual watching process, and be analyzed to fine-tune native AI models, such as long short-term memory (LSTM), recurrent neural network (RNN), and CNN, etc., to emulate and predict user status, and abstract distilled features. As a crucial hub connecting UDTs and SDTs, IDTs can further aggregate UDTs’ data to obtain some global information that can be provided to SDTs for slice adjustment. Furthermore, IDTs is responsible for interacting with the physical domain and slice domain, such as adaptive data collection frequency and resource management strategies, etc. D. A Processing Procedure Example for Short Video Take the short video slice as an example, the processing procedure is shown in Fig. 2. UDTs store users’ historical status information, including channel conditions, locations, swipe timestamps, and preferences, etc. The data stored in UDTs are analyzed by embedded models to emulate user status, such as swipe behaviors, and abstract some essential user features, such as swipe probability distribution. In the small timescale, the emulated user status and abstracted user features are transferred to IDTs, integrated with network topology and performance metrics to emulate network operation status and design tailored resource allocation algorithms. The resource allocation policy will be delivered to the NC and then implemented on the access points to facilitate realtime video transmission. In the large timescale, the emulated network operation status and system performance are further transferred to SDTs to abstract global network information for the slicing policy adjustment. The slicing policy will be delivered to the NC and then implemented on the access points to reserve resources. III. RESEARCH CHALLENGES AND SOLUTIONS To realize the proposed DTN4VS, some research challenges need to be addressed. A. Efficient Data Abstraction from Physical Domain 1) Challenge: Intuitively, addressing the complex interplay among the physical domain, slice domain, and DT domain requires an efficient data abstraction mechanism. Since DTs need to be updated to guarantee accuracy, how to autonomously identify the types and granularities of collected data is crucial. To enhance the network management level, it is imperative to clarify what specific insights and optimizations that DTs can provide. Furthermore, the development of efficient algorithms for real-time data collection is essential, especially in ultra-low latency extreme immersive VR scenarios. Taking user behavior data as an example, we encounter several challenges. Initially, user behavior data, such as swipe, likes, subscribe, favorites, comments, etc., constitutes a complex user profile. It is challenging to determine the importance of different user behavior data and the data update frequency for maintaining UDTs. Additionally, since user behavior data is multi-dimensional, it is challenging to discern which data dimensions can be effectively fused, and which user behavior patterns can be extracted to assist network management. Finally, user movement in VR scenarios spans from macro gestures like head and hand motions to micro shifts in viewpoint, which requires a well-designed data abstraction algorithm to realize data synchronization with low communication overhead. 5 Extreme immersive VR users Intelligent short video users Holographic video users User terminals Radio access network Edge network Cloud network Intelligent short video slice Holographic video slice UDTs ·Networking Behaviors Playback Raw data Sanitized data SDTs Emulation Feature abstraction · Status prediction · Network topology Performance · Emulation Resource scheduling IDTs Raw data Sanitized data · Slicing configuration Demand Performance Raw data Sanitized data · Emulation Slicing policy Data fusion Data Data collection frequency, strategies, etc. Status, features, etc. Strategies Status, strategies, etc. Features, strategies, etc. Extreme immersive VR users Intelligent short video users Holographic video users User terminals Radio access network Edge network Cloud network Intelligent short video slice Holographic video slice UDTs ·Networking Behaviors Playback Raw data Sanitized data SDTs Emulation Feature abstraction · Status prediction · Network topology Performance · Emulation Resource scheduling IDTs Raw data Sanitized data · Slicing configuration Demand Performance Raw data Sanitized data · Emulation Slicing policy Data fusion Data Data collection frequency, strategies, etc. Status, features, etc. Strategies Status, strategies, etc. Features, strategies, etc. Physical domain Slice domain DT domain Extreme immersive VR users Intelligent short video users Holographic video users User terminals Radio access network Edge network Cloud network Intelligent short video slice Holographic video slice UDTs ·Networking Behaviors Playback Raw data Sanitized data SDTs Emulation Feature abstraction · Status prediction · Network topology Performance · Emulation Resource scheduling IDTs Raw data Sanitized data · Slicing configuration Demand Performance Raw data Sanitized data · Emulation Slicing policy Data fusion Data Data collection frequency, strategies, etc. Status, features, etc. Strategies Status, strategies, etc. Features, strategies, etc. Physical domain Slice domain DT domain Fig. 1. DTN4VS framework. End users Access points UDTs IDTs SDTs 1. Status information 2. Data forward 3. Emulation and data abstraction 6. Emulation and resource allocation decision-making 4. Emulated status and data features 5. Network status 7. Emulated status and system performance 11. Emulation and slicing decision-making NC 12. Slicing policy 8. Resource allocation policy 13. Policy implement 9. Policy implement 10. Video transmission Fig. 2. DT-assisted short video streaming processing procedure. 2) Solution: To address the complex interplay among different domains, an efficient data collection mechanism should first be developed, which relies on data importance and distribution to adjust the collected data type and granularity. Then, since network performance is usually closely related to a part of users’ status dimension, a meticulously designed data fusion mechanism should be developed to abstract some distilled features from user status to facilitate network management. Finally, a lightweight semantic abstraction algorithm can be developed to capture users’ real-time behaviors and network conditions, which can help reduce transmission overhead and guarantee real-time responsiveness. Similarly, taking user behavior data abstraction as an example, we can first employ the principal component analysis technique to analyze which data types have a strong relationship with network performance, and make an adaptive data collection granularity based on data distribution variation. Then, meticulously designed machine learning algorithms can be leveraged to abstract and predict user behavior patterns, such as swipe probability distributions in short video streaming and region of interest (RoI) in VR streaming. Finally, some lightweight feature abstraction algorithms, such as visual geometry group (VGG) and vision transformers (ViT), can be embedded into Internet of Things (IoT) sensors to abstract user behavior information in real time. B. Comprehensive Performance Evaluation Framework 1) Challenge: The development of a comprehensive performance evaluation framework is crucial for effectively assessing DT performance in network management. Such a framework should integrate a diverse and adaptable set of key performance indicators (KPIs) to analyze DT performance. Traditional KPIs such as latency, throughput, and buffering time may not be sufficient to capture the multi-faceted nature of DT performance, which includes not only network metrics but also user behavior and QoE. The integration of machine learning algorithms for predictive analytics, real-time data synchronization between the physical network and its DTs, and edge computing for low-latency data processing adds layers of complexity to performance evaluation. Additionally, the dynamic nature of video streaming, characterized by fluctuating bit rates, stochastic video requests, and user 6 interactivity, requires KPIs to be robust and sensitive to these fluctuations. Furthermore, the KPIs must be adaptable to different network architectures and technologies, ranging from traditional content delivery network (CDN) to next-generation communication infrastructures. Therefore, the challenge lies in developing a comprehensive performance evaluation framework that can holistically evaluate DT performance, taking into account the intricate interplay among the physical domain, slice domain, and DT domain. 2) Solution: To address this gap, a novel KPI, termed holistic DT value, has been introduced, denoted by V . From the perspective of DT itself, data freshness [12] and model operation cost are very important metrics to optimize the synchronization between DTs and physical entities. From the perspective of DT impact, resource management gain such as QoS and QoE, directly reflects the impact of DTs on service quality and watching experience. The new KPI should integrate the key metrics to provide a holistic view of network performance, which is expressed as: V = α ·  f F  + β · Q(C, A, P, S,L) − γ · R(L), (1) where α, β, γ are weighting factors. Here, f and F represent the data collection frequency and data freshness, respectively. Function, Q, represents the traditional KPIs, such as QoS and QoE, which is related to communication resource scheduling matrix C, caching resource scheduling matrix A, computing resource scheduling matrix P, sensing resource scheduling matrix S, and data abstraction level L. In the actual network optimization, we can select a part of resource scheduling decisions as the joint optimization object to avoid dimension curse. Function R reflects the DT model operation cost related to its data abstraction level L. C. Adaptive DT Model Update 1) Challenge: Unlike static models, DTs must continuously evolve to reflect actual changes in both network conditions and user behaviors. This requires sophisticated machine learning algorithms capable of processing large data volumes in real time, possibly leveraging collaborative cloud-edge computing mechanism embedded with distributed learning for the lowlatency DT model update. Moreover, the dynamics of video streaming, characterized by fluctuated bit rates, diverse content types, and user interactivity, also add the complexity of the DT model update. Techniques such as DRL or generative adversarial networks (GAN) can facilitate efficient decisionmaking and network emulation [13], but come with their own challenges, such as the requirement of extensive training data and computing resources, and the risk of model overfitting. Furthermore, since users’ network conditions, behaviors, and preferences own certain similarities, DTs can exchange part of data and learn models from each other to evolve together. This demands an efficient learning algorithm design with low computational overhead. Therefore, the challenge lies in developing adaptive DTs that can adapt to highly dynamic and complicated video streaming services. 2) Solution: To effectively tackle the intricate challenge of developing adaptive DT models for emerging video streaming services, a multi-layered solution that synergistically integrates distributed learning and transfer learning is put forth. In a hybrid cloud-edge computing architecture, distributed learning algorithms are employed to facilitate model split and parallel computing. The architecture allows DTs to efficiently process seamless collected data, including network-related data and user-specific behavior data. In parallel, transfer learning can be particularly effective when exploiting similarities between different DTs. For instance, a model trained on one DT that has successfully been adapted to certain network conditions and user behaviors can be fine-tuned for another DT with similar conditions. This approach capitalizes on the inherent similarities between different DTs to quickly adapt to new scenarios, eliminating the requirement for extensive retraining and thereby reducing computing overhead. IV. CASE STUDY: DT-ASSISTED NETWORK SLICING FOR SHORT VIDEO STREAMING In this section, a case study is provided on DT-assisted network slicing, aimed at improving the system utility consisting of user satisfaction and resource consumption. A. Considered Scenario We consider a DT-assisted multicast short video streaming (MSVS) network, which consists of two BSs, an edge server (ES), and sixty UDTs. Bandwidth and computing resources are sliced (or reserved) for each multicast group to guarantee QoS requirement. Each UDT corresponds to an individual user consisting of a finite data pool and a data analysis function. Specifically, in each UDT data pool, we first simulate the user’s trajectory within the University of Waterloo (UW) campus with differentiated speed, and the user’s real-time channel condition is generated based on propagationModel at Matlab. Then, we employ the real-world dataset1 to simulate the user’s swipe timestamps and preference on the sampled YouTube 8M dataset2 . The data analysis function investigates the user’s swipe timestamps to obtain a swipe probability distribution for each video type. After the construction of UDTs, a DRL-based user clustering algorithm is implemented to cluster UDTs into different multicast groups. UDTs’ swipe timestamps and preferences are used to abstract the swipe probability distribution and recommended video list for accurate bandwidth and computing resource demand prediction in each multicast group. Based on the predicted information, the NC can make appropriate bandwidth and computing resource reservation strategy for each multicast group to enhance the system utility. We propose a hybrid data-model-driven solution, where UDTs’ data are analyzed by the DRL-based user clustering algorithm to update multicast groups, and the resource reservation problem is transformed into a convex problem to obtain the optimal solution [14]. For performance comparison, we 1ACM MM Grand Challenges: https://github.com/AItransCompetition/ShortVideo-Streaming-Challenge/tree/main/data 2YouTube 8M dataset: https://research.google.com/youtube8m/index.html 7 0 5 10 15 20 25 30 Resource Reservation Window Index 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 User Satisfaction Proposed Heuristic Optimization Fig. 3. User satisfaction vs. resource reservation window. Proposed Heuristic Optimization 0.5 1 1.5 2 2.5 Resource Consumption Fig. 4. Resource consumption comparison. adopt two schemes, i.e., 1) heuristic solution, where multicast groups are updated based on users’ preferences and locations, and bandwidth and computing resource reservation is based on historical video traffic distribution; 2) optimization-based solution, where multicast groups are updated based on the density-based spatial clustering of applications with noise (DBSCAN) algorithm, and resource reservation is based on the branch- and bound-based scheduling algorithm. B. Simulation Results 1) Simulation Settings: We emulate two BSs at the UW campus and users’ initial positions are randomly and uniformly generated around two BSs. Each user moves along a prescribed path within the UW campus at a speed of 2∼5 km/h. The transmission power and noise power are set to 27 dBm and -174 dBm, respectively. We sample 1000 short videos from the YouTube 8M dataset, which includes 8 video types, i.e., Entertainment, Games, Food, Sports, Science, Dance, Travel, and News. Each video has a duration of 15 sec and is encoded into four versions by the H. 265 encoder. The detailed simulation setting can be found at [14]. 2) Performance Analysis: As shown in Fig. 3, we present user satisfaction within 30 resource reservation windows. The time length of each resource reservation window is set to 5 min. It can be observed that the proposed solution can achieve the highest user satisfaction with smaller fluctuations because the DT-based user clustering algorithm can well mine users’ intrinsic correlation to accurately update multicast groups, while the convex optimization algorithm can make the optimal resource reservation strategy based on the updated multicast groups to enhance user satisfaction. Finally, we present the resource consumption comparison in Fig. 4. It can be observed that the proposed solution can achieve lower median, third-quartile, and maximum values compared with other schemes. Our proposed scheme demonstrates superior performance with relatively minimal variations, while the heuristic scheme exhibits a larger fluctuation. V. OPEN RESEARCH ISSUES A. Efficient Coordination of DT Modules As an efficient data management platform for video streaming services, the coordination of DT modules directly influences network performance. Specifically, the development of real-time synchronization mechanisms among different DT modules is crucial for ensuring that all components are operated based on the latest data, thereby enhancing network responsiveness and DT accuracy. Furthermore, the establishment of standardized communication protocols is essential for seamless interoperability among different modules, facilitating a unified and effective system architecture. Lastly, the design of adaptive algorithms that can dynamically allocate resources among modules based on real-time performance analysis and emulation is vital for optimizing resource utilization. Therefore, it is crucial to develop an efficient coordination mechanism of DT modules to improve network performance. B. Closed-Loop Network Management To realize a sustainable and continuously evolving network for video streaming services, it is essential to construct an internal and external closed-loop network management system. Internally, UDTs, IDTs, and SDTs are mainly responsible for user status analysis, network emulation, and network slicing, respectively. For instance, UDTs analyzing users’ highfrequency interaction behaviors could prompt the SDTs to reserve more resources, validated by the IDTs, which can create a self-regulating loop for optimal performance. Externally, DTs continuously monitor the physical network’s status and provide useful information for network management. The physical network feeds back its actual performance data to update DT data and models. Hence, constructing a closed-loop network management system requires efficient data exchange, network emulation, data synchronization, and model update. C. Security and Privacy of DT While much of the current research primarily emphasizes the constructions of UDTs, IDTs, and SDTs to enhance 8 video streaming services, there exists a notable oversight in addressing the issues of DT security and privacy protection. From the perspective of users, the urgency of data privacy escalates within the DTN4VS framework. Particularly, not only content service providers but also NCs need to gather sensitive user data, such as video preferences and locations, and the unique data collection model intensifies the complexity of effective data privacy regulation. Furthermore, the creation of DTs mandates collaboration amongst various stakeholders, which requires them to contribute their own data and analytic models [15]. Thus, establishing trust in such a distributed environment and protecting both data and AI model security poses significant challenges. Although current privacy-preserving techniques such as differential privacy, secure multi-party computation, and homomorphic encryption, offer potential solutions, they mandate further exploration for efficiency enhancements and tailored strategies. VI. CONCLUSIONS We have proposed the DTN4VS to realize holistic network virtualization for emerging video streaming services. Specifically, DTN4VS aims to seamlessly integrate eMBBPlus, native AI, sensing, and network slicing through DTs to achieve efficient network management. It can further separate the resource management functions from NCs and empower the functions with emulated data and tailored strategies, which can reduce the centralized computation burden and enhance network robustness. To supplement the DTN4VS’s functionality, we have proposed a data importance-based abstraction mechanism, a holistic DT performance evaluation metric, and a distributed transfer learning algorithm, respectively. A case study has been presented, and some open research issues have been provided for accelerating the pace of DTN4VS development. ",
		"summary": "Digital twin (DT) is revolutionizing the emerging video streaming services through tailored network management. By integrating diverse advanced communication technologies, DTs are promised to construct a holistic virtualized network for better network management performance. To this end, we develop a DT-driven network architecture for video streaming (DTN4VS) to enable network virtualization and tailored network management. With the architecture, various types of DTs can characterize physical entities’ status, separate the network management functions from the network controller, and empower the functions with emulated data and tailored strategies. To further enhance network management performance, three potential approaches are proposed, i.e., domain data exploitation, performance evaluation, and adaptive DT model update. We present a case study pertaining to DT-assisted network slicing for short video streaming, followed by some open research issues for DTN4VS.",
		"id": "UUID26"
	},
	{
		"document": "I. INTRODUCTION Fueled by recent advances in wireless communication and computation technologies, cyber-physical network applications have evolved to intelligently connect the physical and cyber worlds, enabled by fully utilizing computation resources scattered over communication networks. These applications, including autonomous driving, remote healthcare, and realtime monitoring, rely on collecting raw sensing data about the time-varying physical environment, extracting valuable status information through computation, and generating control demands based on the status information. However, since the environment changes constantly, control quality degrades until a new status update is made. Hence, the performance of these applications heavily depends on the freshness of This work was sponsored in part by the Natural Science Foundation of China (No. 62341108, No. 62022049, No. 62111530197, No. 62301024, No. 62221001), Hitachi Ltd, the Beijing Natural Science Foundation under grant L222044, the Fundamental Research Funds for the Central Universities under grant 2022JBXT001, and the Talent Fund of Beijing Jiaotong University under grant 2023XKRC030. (Corresponding author: Sheng Zhou). J. Sun, L. Wang, Z. Nan S. Zhou, and Z. Niu are with Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China. (Emails: sunjz14@tsinghua.org.cn, {wanglh19@mails.,nzj660624@mail.,sheng.zhou@, niuzhs@}tsinghua.edu.cn. ) Y. Sun is with the School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing 100044, China. (Email: yxsun@bjtu.edu.cn) status information provided by the network system, which necessitates a shift of focus from solely conveying information bits to providing timely information for certain tasks under service, named as task-oriented communications [1]. One key challenge in this shift is how to effectively orchestrate communication and computation resources in the system, taking account of task-specific timeliness requirements. Over the past decade, edge computing has received much attention due to its potential in providing timely information processing service [2]. This trend motivates design of schemes that can adaptively offload the computation burden to the edge side or execute it locally, according to the capabilities of both communication and computation resources [3]–[5]. In this work, we study a system consisting of multiple energy-constrained devices, where each device observes a time-varying process and generates tasks that require computation to extract status information about the underlying process. These computation-intensive tasks can be executed on-device or offloaded to an edge server for assistance. Based on the latest status information, devices can decide control actions. And the control quality depends on the freshness of status information. For example, in Augmented Reality (AR) applications, a headset needs to continuously capture the position of certain objects and render virtual elements [6]. If the position information becomes stale, the virtual overlay may not align with the physical objects. In this case, the headset can update position status by running object detection algorithms, which can be offloaded to an edge server when the network is in good condition, or done locally at the cost of higher latency and energy consumption. Local computing usually takes longer time than that on the edge server side [7], and thus the problem of where to compute seems trivial for single-device system. However, in a multidevice scenario where devices share the wireless channel, some devices might be more suitable than others to offload, probably due to factors such as better channel conditions. Because of limited communication resources, a scheduling policy is needed to determine when devices should generate computation tasks and where computation should be executed to extract status information. A. Related Work In recent years, Age of Information (AoI) has provoked great interest as a metric to quantify the freshness of information [8]. In a status update system, AoI measures the elapsed time since the generation of the freshest received information and arXiv:2310.18895v1 [cs.NI] 29 Oct 2023 2 characterizes the freshness of the status information used for decision-making. Unlike metrics such as delay and throughput, which focus on packet-level performance, AoI provides a system-level view. There has been a growing body of research on designing device scheduling policies based on AoI. Among them, weighted average AoI is widely adopted as the optimization target. Periodic status sampling is investigated in [9], and stochastic sampling is considered in [10], where Whittle’s index has been shown to enjoy a close-to-optimal performance. Energy harvesting system is studied in [11]–[13]. Besides weighted average AoI, nonlinear functions of AoI have also gained attention. In networked control systems with estimation error as the control performance, it has been found that the error can be expressed as a nonlinear function of AoI [14], [15] for linear time-invariant system, if the sampling time is independent of the underlying status. In the single device case with a general monotonic AoI penalty function, the optimal sampling problem is studied in [16], [17]. For the multi-device case, a scheduling policy using Whittle’s index is proposed in [18]. A threshold-type policy is derived in [19] based on the steady distribution of AoI. A more recent work [20] shows that AoI functions can be applied to timeseries prediction problem. However, most of these studies have ignored the role of computation in providing fresh information. As for communication and computation co-design for AoI under the framework of edge computing, tandem queuing model is widely adopted to describe the interplay between communication and computation [21]–[24]. With Poisson sampling process, the average AoI is derived as a function of the sampling rate, transmission rate, and computation rate. Soft update is proposed in [25] to characterize of process of computation. Optimal sampling policies are derived for exponentially and linearly decaying age cases. In [26], constrained Markov decision process is adopted to decide when to offload computation-intensive status update to the edge server. In [27], a finite horizon problem is formulated to optimization linear AoI target. Multi-device scheduling problem is studied in [28] which only considers local computing. B. Contributions For multi-device scheduling in the context of information freshness, an important problem is how to orchestrate communication and computation resources. Most previous papers on this problem only consider single-device case. The one most closely related to our work is [28], but we extend the choices of computing to include the edge side. Our work aims to address the problem of scheduling energy-constrained devices with nonlinear AoI penalty functions and explore ways to provide up-to-date status information by switching between edge computing and local computing. Our contributions can be summarized as follows, • We develop a general framework to jointly consider the communication and computation aspects of real-time status update applications. Computation tasks can be done on-device or on the edge server side. Taking transmission and computation time into account, control performance Device 1 Sensing . . . Device 2 Offloading Edge Computing Local Computing Device N Fig. 1: System model. is modeled as general monotonic AoI penalty functions. Given system parameter, a nontrivial lower bound of the time average AoI penalty is derived. By inspecting the property of the lower bound, we propose indices that represent the priority of local computing or edge computing at different AoI values. • A low-complexity scheduling policy is proposed by combining the indices introduced above with the virtual queue technique from Lyapunov optimization [29]. We show that this policy satisfies the energy constraints of each device. For penalty functions of the form f(x) = x p , p > 0, we derive the performance gap between the proposed policy and the lower bound, when communication and computation stages take single time slot. • Extensive simulations are carried out to evaluate the performance of the proposed policy for different forms of penalty functions and latency distributions. Simulation results demonstrate that the average AoI penalty under the proposed policy is close to the lower bound. Moreover, we apply the proposed policy to object tracking applications which can be naturally cast as status update processes. The proposed policy is examined on a large video dataset ILSVRC17-VID [30]. Our results show that the proposed policy improves object tracking accuracy by 27% to that of the video content matching-based scheduling. Furthermore, the proposed policy also outperforms content-based scheduling that has access to the ground truth information. The rest of this paper is organized as follows. In Section II, we present the system model and the problem formulation. In Section III, we formulate a convex optimization problem to compute a nontrivial lower bound of the average AoI penalty. In Section IV, a low-complexity scheduling policy is provided based on the lower bound problem. In Section V, numerical results are presented along with the object tracking application. We conclude the paper in Section VI. II. SYSTEM MODEL We consider the status update system shown in Fig. 1. This system consists of a set of energy-constrained devices, denoted as N , with a total number of N. Each device performs a sensing-control task by collecting sensing data, 3 extracting status information from it, and determining control actions based on the status. As the status information becomes stale, the control quality decreases. This simplified model is well-suited for many real-time applications and abstracts away irrelevant details. For energy-constrained mobile devices, however, frequent status updates can quickly drain the battery. Therefore, a scheduling policy is required for each device to decide 1) when to generate a computation task for status update and 2) where to execute the computation. Slotted time system is considered. At the beginning of a time slot, each device collects sensing data if scheduled, which is assumed to take negligible time. The scheduled devices then perform computation locally or offload computation tasks to an edge server, which takes several slots to finish. For device n, n ∈ N , let Dl,n be the number of slots required to finish local computing. The offloading stage takes Dt,n slots to send the raw sensing data to the edge server, followed by edge computing that lasts De,n slots. Result feedback delay is ignored. These three are random variables with finite expectations denoted as Dl,n, Dt,n, and De,n, respectively. Furthermore, the latency in each communication or computation stage is assumed to be independent. We use three binary indicators ul,n(k), ut,n(k), and ue,n(k) to indicate the stage device n is in at time slot k. For example, ul,n(k) = 1 if device n is performing local computing at time slot k. Otherwise, ul,n(k) = 0. Similarly, ut,n(k) and ue,n(k) are associated with the offloading and edge computing stages, respectively. Consider non-preemptive policies, we require that ul,n(k)+ut,n(k)+ue,n(k) ≤ 1. When the summation is zero, device n is idle. Let M be the number of orthogonal sub-channels. If device n is scheduled to offload sensing data to the edge server, it will occupy one idle channel for Dt,n consecutive slots to complete the transmission. On the edge server side, we assume that it is equipped with multi-core hardware and can process multiple computation tasks in parallel [3]. Therefore, each offloaded computation task is served immediately upon arrival, and there is no queuing delay. Let dn(k) be the latency since the time slot when the sensing data is collectd. If ul,n(k) + ut,n(k) + ue,n(k) = 1, which means that device n is performing status update, dn(k) = dn(k − 1) + 1. Otherwise, dn(k) = 0. When computation is finished, a new control action is generated and returned to the device. Generally, the quality of the control action depends on the freshness of the sensing data used to compute it. To capture this freshness, AoI is defined as the time elapsed since the generation time of the sensing data used to compute the current control action. The AoI of device n at time slot k is denoted as hn(k). As shown in Fig. 2, AoI evolves as: hn(k) =    dn(k − 1) + 1, if the computation is finished at slot k − 1, hn(k − 1) + 1, otherwise. (1) It is pointed out in [14] that, for LTI system, the control quality can be cast as a function of AoI if the sampling process is independent of the content of the underlying process. Following this finding, we model the relationship between control Trans. Edge 1 AoI k Local Local Computation Transmission Edge Computation 2 3 4 5 6 7 8 9 10 11 12 Update Round 2 Sensing Sensing Fig. 2: Evolution of AoI. quality and AoI as a penalty function fn(·), representing the degradation in performance due to information staleness. It is required that the penalty increases with AoI. Furthermore, to avoid ill cases, we also require that the expected penalty with latency Dl,n, Dt,n, De,n is finite. We focus on energy consumption on the device side. For local computing, device n takes El,n Joule per slot. When offloading sensing data to the edge server, device n consumes Et,n Joule per slot. Let En(k) be the energy consumption at time slot k, it consists of two components En(k) = El,nul,n(k) + Et,nut,n(k). The average energy consumed per time slot by device n should be no larger than En. Let vector h(k) ≜ (h1(k), h2(k), . . . , hN (k)) represent the AoI of all devices at time slot k. Similarly, d(k),ul(k),ut(k),ue(k) are vectors of corresponding variables. The state of the whole status update system is Θ(k) ≜ (h(k), d(k),ul(k),ut(k),ue(k)). The history up to time slot k is denoted as H(k) ≜ {Θ(i)|i ≤ k}. A scheduling policy π takes in the history H(k) and decides the new value of ul(k) and ut(k). Note that policy π is a centralized policy because it needs h(k) and d(k) to make decision. To obtain this information, we assume each device will report at the start and end of its computation. Because each device is not always doing computation and this action information is tiny compared to raw sensing data, we ignore this extra cost to implement policy π. Our objective is to propose a scheduling policy that minimizes the time-averaged AoI penalty, subject to energy consumption constraints and communication constraints, as expressed in P1. P1 : min π∈Π X n∈N lim sup K→∞ 1 K Eπ 'X K k=1 fn(hn(k))# s.t. ut,n(k), ue,n(k), ul,n(k) ∈ {0, 1}, ∀k ≥ 1, ∀n ∈ N , ut,n(k) + ue,n(k) + ul,n(k) ≤ 1, ∀k ≥ 1, ∀n ∈ N , X n∈N ut,n(k) ≤ M, ∀k ≥ 1, lim sup K→∞ 1 K Eπ 'X K k=1 En(k) # ≤ En, ∀n ∈ N . (2) Here, Π is the set of non-preemptive policies. This problem can be formulated as a Constrained Markov Decision Process (CMDP) with Θ(k) representing the state of the system. 4 However, solving this problem exactly is computationally prohibitive. The first reason is that the state space grows exponentially with the number of devices. The second reason is that there are multiple constraints, which renders the standard iteratively tightening approach for CMDP invalid [31]. Therefore, in the following, we begin by investigating the lower bound of the AoI penalty. Building on this, we propose a low-complexity scheduling policy that draws inspiration from the lower bound problem. III. LOWER BOUND OF THE AOI PENALTY In this section, we aim to derive a nontrivial lower bound on the AoI penalty given system parameters. This not only aids in evaluating policy performance but also provides valuable insights into how to design a scheduling policy. A. Lower Bound Derivation We first study the AoI penalty of a single device and then extend the result to multiple devices. For simplicity, the subscript n is dropped temporarily. The time horizon can be divided into disjoint time intervals delineated by the event of computation completion, with each interval being referred to as an update round, as shown in Fig. 2. Let h + l and h + t be the peak age in local computing round and edge computing round respectively. Both are random variables depending on the policy π. Furthermore, we introduce ρl(π) and ρt(π) to denote the portions of energy spent on local computing and offloading under policy π respectively. The following lemma presents an alternative expression for the average AoI penalty in P1, Lemma 1. Given policy π ∈ Π, the average AoI penalty is, lim sup K→∞ 1 K Eπ 'X K k=1 f(h(k))# = ρt(π)E EtDt Eπ[F(h + t ) − F(Dt + De − 1)] + ρl(π)E ElDl Eπ[F(h + l ) − F(Dl − 1)], (3) where F(h) ≜ X h x=0 f(x). (4) Proof. See Appendix A. Considering the following optimization problem P2, P2 : min π ρtE EtDt Eπ(π)[F(h + t ) − F(Dt + De − 1)] + ρl(π)E ElDl Eπ[F(h + l ) − F(Dl − 1)] s.t. ρt(π)E EtDt (Eπ[h + t ] − (Dt + De − 1)) + ρl(π)E ElDl (Eπ[h + l ] − (Dl − 1)) = 1. (5) Lemma 2 shows that it provides a lower bound for P1, Lemma 2. The minimum value of P2 is no larger than that of P1. Proof. See Appendix A. Because f(x) only takes values at discrete point, we introduce extended penalty function ˜f(x) to facilitate analysis. ˜f(x) is obtained by interpolating f(x) such that: 1) ˜f(x) is an increasing function, 2) ˜f(x) = f(x), when x ∈ N. As a result, we have X h i=0 f(i) ≥ Z h 0 ˜f(x)dx. (6) Let F˜(h) be the integral of ˜f(x) over [0, h]. Then, F˜(h) ≤ F(h). Because ˜f(x) is increasing, F˜(h) is convex. By Jensen’s inequality, we have Eπ[F(h + l )] ≥ F˜(Eπ[h + l ]), Eπ[F(h + t )] ≥ F˜(Eπ[h + t ]). (7) Let x ≜ Eπ[h + l ] and y ≜ Eπ[h + t ]. Plugging (7) into P2 leads to the following optimization problem P3, P3 : min x≥0,y≥0 ρt(π)E EtDt  F˜(y) − Eπ[F(Dt + De − 1))] + ρl(π)E ElDl  F˜(x) − Eπ[F(Dl − 1)] s.t. ρt(π)E EtDt (y − (Dt + De − 1)) + ρl(π)E ElDl (x − (Dl − 1)) = 1. (8) P3 relaxes the feasible region of Eπ[h + l ] and Eπ[h + l ] to nonnegative number, and thus it is a relaxation of P2. The optimal solution is xopt = yopt = 1 + ρl(π)E(Dl−1) ElDl + ρt(π)E(Dt+De−1) EtDt ρl(π)E ElDl + ρt(π)E EtDt . (9) For simplicity, the optimal solution (9) is denoted as G(ρl , ρt). Then, with energy proportions ρl , ρt, the penalty lower bound in single-device case is ρtE EtDt  F˜(G(ρl , ρt)) − Eπ[F(Dt + De − 1))] + ρlE ElDl  F˜(G(ρl , ρt)) − Eπ[F(Dl − 1)] . (10) Now we zoom out to consider the entire system, and let ρl ≜ (ρl,1, ρl,2, . . . , ρl,N ), ρt ≜ (ρt,1, ρt,2, . . . , ρt,N ). 5 Consider optimization problem P4 min ρl,ρt X n∈N  ρl,nEn El,nDl,n + ρt,nEn Et,nDt,n  F˜ n(Gn(ρl,n, ρt,n)) − X n∈N  ρt,nEn Et,nDt,n E[Fn(Dt,n + De,n − 1)] + ρl,nEn El,nDl,n E[Fn(Dl,n − 1)] s.t. X n∈N ρt,nEn Et,n ≤ M, ρl,n + ρt,n ≤ 1, ∀n ∈ N , ρl,n ≥ 0, ρt,n ≥ 0, ∀n ∈ N . (11) The first constraint is obtained by relaxing the communication constraint, which originally states that at most M devices can offload simultaneously. It is now relaxed as the timeaverage number of transmissions, which should not exceed M. Therefore, the optimal value of P4 provides a lower bound of the time average AoI penalty. B. Lower Bound Analysis In this subsection, we first show that P4 is a convex optimization problem, and then study properties of the optimal solution based on KKT conditions. Lemma 3. The optimization problem P4 is convex. Proof. See Appendix B. For simplicity, let’s introduce the following auxiliary variables an = En El,nDl,n , bn = En Et,nDt,n , cn = Dl,n − 1, dn = Dt,n + De,n − 1, vn = Eπ[Fn(Dl,n − 1)], wn = Eπ[Fn(Dt,n + De,n − 1)], xn = ρl,n, yn = ρt,n. (12) Let α, β, γ, ν be Lagrange multipliers. the Lagrangian function is L(x, y, α, β, γ, ν) =β T (x + y − 1) − γ T x − ν T y + X n∈N (anxn + bnyn)F˜ n(Gn(xn, yn)) − X n∈N (anvnxn + bnwnyn) + α  X n∈N ynEn Et,n − M ! . (13) Because P4 is convex, the optimal solution and Lagrange multipliers satisfy KKT conditions. Let x ∗ , y ∗ , α∗ , β ∗ , γ ∗ , ν ∗ be the corresponding optimal solution. Applying KKT conditions to (13) provides the following property, Theorem 1. The optimal solution specified by KKT conditions satisfies Wt,n(ht,n) Et,nDt,n − Wl,n(hl,n) El,nDl,n − γ ∗ n − ν ∗ n En = α ∗ Et,n , (14) where ht,n = Gn(x ∗ n , y∗ n ) − (Dt,n + De,n − 1), (15) hl,n = Gn(x ∗ n , y∗ n ) − (Dl,n − 1), (16) and Wt,n(x) = x ˜fn(x + Dt,n + De,n − 1) − (F˜ n(x + Dt,n + De,n − 1) − E[Fn(Dt,n + De,n − 1)]), (17) Wl,n(x) = x ˜fn(x + Dl,n − 1) − (F˜ n(x + Dl,n − 1) − E[Fn(Dl,n − 1)]). (18) Proof. See Appendix C. Here, ht,n represents the expected AoI when device n is scheduled to offload, and hl,n represents the expected AoI when device n is scheduled to do local computing. An intuitive illustration of Wt,n and Wl,n is shown in Fig. 3. Taking Wl,n as an example, h is the AoI when the scheduling decisions are made, and d is the computing latency. When x = h, the first term in (18) is the summation of regions I, II, and III, and the second term is the summation of regions I and II. Thus, Wl,n is the colored region III. With this geometrical interpretation, the influence of latency and penalty function is reduced to the area of the colored region in Fig. 3. AoI Penalty 𝑑 − 1 ℎ ℎ + 𝑑 − 1 𝑓(ℎ) 𝑊(ℎ) I II III Fig. 3: An illustration of W(x). Remark 1. Rethinking (14), the first term Wt,n(h) Et,nDt,n is the priority of doing status update by offloading when AoI is h. And the second term Wl,n(h) El,nDl,n corresponds to the priority of doing local computing. The third term is related to two Lagrange multipliers: γ ∗ n and ν ∗ n . Due to complementary slackness, γ ∗ n = 0 if ρ ∗ l,n > 0. For the same reason, ν ∗ n = 0 if ρ ∗ t,n > 0. Note that γ ∗ n and ν ∗ n can not be larger than 0 simultaneously. When γ ∗ n and ν ∗ n equal 0, (14) is reduced to 1 Dt,n Wt,n(ht,n) − Et,n El,nDl,n Wl,n(hl,n) = α ∗ . (19) 6 Taking α ∗ as the price of using the channel to offload, Equation (19) can be used to determine whether to perform local computing or offload updates. IV. SCHEDULING POLICY Based on (14), which characterizes the expected AoI at scheduling instants, a natural and intuitive policy is to schedule local computing for device n when its AoI is hl,n and to schedule offloading when the AoI is ht,n. However, the challenge of obtaining the values of hl,n and ht,n, as well as the parameters γn, νn, and α at runtime, renders this policy impractical. Nevertheless, the insights provided by (14) indicate that a scheduling policy should steer the AoI at scheduling instants towards values that align with (14). This helps to design scheduling policy for the original problem P1. We first introduce an auxiliary variable Qn, and rearrange (14) as  Wt,n(ht,n) Dt,n − Et,nQn  + Et,n El,n  Wl,n(hl,n) Dl,n − El,nQn  = α + Et,n En (γn − νn). (20) If Qn satisfies Wl,n(hl,n) Dl,n − El,nQn = 0, (21) then Wt,n(ht,n) Dt,n − Et,nQn = α + Et,n En (γn − νn). (22) If the value of Qn is known, (21) and (22) provide a heuristic scheduling policy. Firstly, for the edge computing part, since the function Wt,n(x) is increasing, we can sort idle devices in descending order based on the left-hand side of (22) with ht,n replaced by hn(k) and select no more than m(k) devices to offload, where m(k) is the number of idle channels at time slot k. Then, if device n is still idle and hn(k) satisfies that Wl,n(hn(k)) Dl,n ≥ El,nQn, (23) it will be scheduled to do local computing. By adopting this approach, we can bring the AoI at scheduling instants closer to the values specified by (14). Qn plays the role of threshold in this policy, such that devices will not update so frequently that the energy constraints are violated. In other words, Qn is determined by energy constraints. Although it is hard to calculate the exact value of Qn, we can approach it at runtime. Based on this insight, we use tools from Lyapunov optimization [29] and introduce virtual queue Qn(k): Qn(k + 1) ≜ max{Qn(k) − En + En(k), 0}, (24) Qn(k) corresponds to the energy consumption until time slot k. If update is too frequent, Qn(k) will increase and prevent further update. As system evolves, Qn(k) approximates Qn. Let Nidle(k) be the set of devices that are idle at the beginning of time slot k. The two auxiliary sets are defined as Cl(k) ≜  n     Wl,n(hn(k)) Dl,n ≥ V El,nQn(k), n ∈ Nidle(k)  , Ct(k) ≜  n     Wt,n(hn(k)) Dt,n ≥ V Et,nQn(k), n ∈ Nidle(k)  , where V is a parameter used to smooth the fluctuation of Qn(k). The set Cl(k) consists of devices eligible for local computing, while Ct(k) consists of devices eligible for edge computing. The intersection of these two sets may not be empty. To simplify the expression of scheduling policy, we introduce index In(k) as In(k) =    Wl,n(hn(k)) Dl,n − V El,nQn(k), if n ∈ Cl(k) − Ct(k), Wt,n(hn(k)) Dt,n − V Et,nQn(k), if n ∈ Ct(k) − Cl(k), Wt,n(hn(k)) Dt,n − Wl,n(hn(k)) Dl,n if n ∈ Cl(k) ∩ Ct(k). +V (El,n − Et,n)Qn(k), (25) Based on the insights from (14), we propose a Max-Weight scheduling policy πMW, which makes scheduling decisions at each time slot as shown in algorithm 1. In this algorithm, the scheduler first decides which devices to offload based on their values of In(k), which is derived from (22). Subsequently, those devices that are still idle will perform local computing if they fall within the set Cl(k), as dictated by (21). Algorithm 1 Max-Weight scheduling Input: The number of idle channels m(k). 1: Sort devices in Ct(k) in descending order according to the value of In(k). The result is (n1, n2, . . . , nS). S is the total number of devices in this set. 2: s ← 1 3: while s ≤ S do 4: if m(k) > 0 and Ins (k) ≥ 0 then 5: ut,ns (k) ← 1, m(k) ← m(k) − 1 6: else if ns ∈ Cl(k) ∩ Ct(k) then 7: ul,ns (k) ← 1 8: end if 9: s ← s + 1 10: end while 11: for n ∈ Cl(k) − Ct(k) do 12: ul,n(k) ← 1 13: end for The term V Qn(k) plays the role of Qn in (21) and (22). Since we want lim supk→∞ V Qn(k) and lim infk→∞ V Qn(k) to be close to Qn, it is expected that a smaller V enjoys a better performance, because a small V can smooth fluctuations in the value of Qn(k). This conjecture is substantiated in Section V. Theorem 2 demonstrates that algorithm 1 maximizes a term that is linear in ul,n and ut,n, as follows: 7 Theorem 2. Algorithm 1 makes scheduling decisions ul(k) and ut(k) to maximize the following, X n∈Nidle(k)  Wl,n(hn(k)) Dl,n − V El,nQn(k)  ul,n(k) + X n∈Nidle(k)  Wt,n(hn(k)) Dt,n − V Et,nQn(k)  ut,n(k). (26) Proof. See Appedix D. The following theorem establishes that, under mild assumptions, policy πMW satisfies the energy constraints in (2). Theorem 3. For any n ∈ N , if there exists D∗ n such that Dl,n, Dt,n and De,n are smaller than D∗ n , then lim sup K→∞ 1 K EπMW 'X K k=1 En(k) # ≤ En, ∀n ∈ N . (27) Proof. Let ki be the time slot at which device n starts its i-th round of local computing or offloading. Because the delay is bounded, we have Q(ki+1) ≤ Q(ki) + max(El,n, Et,n)D∗ n . (28) We will prove that there exists L such that lim sup i→∞ Qn(ki) ≤ L. (29) Given ki , there exists si such that Qn(ki) ≥ Qn(k1) + (si − 1) max(El,n, Et,n)D∗ n , Qn(ki) < Qn(k1) + si max(El,n, Et,n)D∗ n . (30) Let s ∗ = sup{si , i ≥ 1}. If s ∗ is finite, then (29) holds trivially. Otherwise, consider an i ∗ such that Qn(ki ∗ ) − tEn ≥ max  Wl,n(2D∗ n + t) V El,n , Wt,n(2D∗ n + t) V Et,n  , (31) and tEn + max  Wl,n(2D∗ n + t) V El,n , Wt,n(2D∗ n + t) V Et,n  ≤ Qn(k1) + (si ∗ − 1) max(El,n, Et,n)D∗ n , (32) where t ≜ l max(El,n,Et,n)D∗ n En m . (31) means that device n is idle for t time slots at least, after the completion of the i ∗ -th status update. Therefore, Qn(ki ∗+1) ≤ Qn(ki ∗ ) + max(El,n, Et,n)D∗ n − tEn. (33) According to the definition of t, (33) yields that Qn(ki ∗+1) ≤ Qn(ki ∗ ). If Qn(ki ∗+1) falls in the range specified in (30) with si ∗ , repeating the analysis above gives that Qn(ki ∗+2) ≤ Qn(ki ∗+1). If Qn(ki ∗+1) < Qn(k1) + (s ∗ i − 1) max(El,n, Et,n)D∗ n , due to (28), we have Qn(ki ∗+2) ≤ Qn(k1) + si ∗ max(El,n, Et,n)D∗ n . (34) Based on induction, we conclude that Qn(kj ) ≤ Qn(k1) + si ∗ max(El,n, Et,n)D∗ n , ∀j ≥ i ∗ , (35) and thus (29) holds. Recall the definition of Qn(k) in (24), we have for all k ≥ 1: 1 K X K k=1 En(k) ≤ Qn(K + 1) K − Qn(1) K + En. (36) Taking expectations of the above and letting K → ∞ yields: lim sup K→∞ 1 K EπMW 'X K k=1 En(k) # ≤ En. (37) One important distinction between our work and other studies that use Max-Weight policy for scheduling, such as [9], is that the set of idle devices in our problem varies with time. Thus, conventional approaches based on strongly stable queue techniques cannot be applied to our problem directly. Although a general performance guarantee is difficult to establish, the following proposition provides insight into the performance gap for a special case: Proposition 1. Let J πMW be the average AoI penalty under policy πMW. When the penalty function is fn(x) = αnx p , p > 0, and Dl,n = Dt,n = 1, De,n = 0, ∀n ∈ N , J πMW satisfies  J πMW p + 1p+1 ≤ J ∗  B p + J πMW p , (38) where J ∗ is the lower bound from (11), and B is defined as B ≜ V 2 X n∈N (max(El,n, Et,n) − En) 2 . (39) Proof. See Appendix E. Remark 2. When p = 1, the penalty function is in linear form, and the target becomes the weighted time average age. Let p = 1 in (38), we obtain the following inequality: (J πMW − 2J ∗ ) 2 ≤ 4J ∗ (B + J ∗ ), (40) which yields, J πMW J ∗ ≤ 2 + 2r B J ∗ + 1. (41) This suggests that the weighted average age achieved by the Max-Weight policy is bounded within approximately four times of the lower bound. V. NUMERICAL RESULTS In this part, we evaluate the performance of the proposed policy under various settings. In addition to extensive simulations on synthetic data, we also apply this policy to a video tracking task and carry out experiments on ILSVRC17-VID dataset. 8 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 Mean Local Computation Time of Type-II device (slots) 0.26 0.28 0.30 0.32 0.34 0.36 Time Average Penalty Max-Reduction Policy Max-Weight Policy Lower Bound (a) Composite 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 Mean Local Computation Time of Type-II device (slots) 30 32 34 36 38 40 42 Time Average Penalty Max-Reduction Policy Max-Weight Policy Lower Bound (b) Linear 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 Mean Local Computation Time of Type-II device (slots) 70 75 80 85 90 Time Average Penalty Max-Reduction Policy Max-Weight Policy Lower Bound (c) Square Fig. 4: Performance comparison. TABLE I: Simulation settings. Parameter Type-I Type-II Local Comp. Delay (slots) U(1, 15) U(1, x) Transmission Delay (slots) U(1, 3) U(3, 7) Edge Comp. Delay (slots) U(1, 2) U(1, 2) Local Comp. Energy (J/slot) 10 10 Transmission Energy (J/slot) 1 1 Energy Budget (J/slot) 0.4 0.4 A. Simulation Results Scheduling decisions depend on various factors, including the form of penalty function, computation delay, transmission delay, etc. To facilitate experiments, the set of devices is divided into 2 types. Part of the simulation settings are listed in Table I, where U(a, b) means taking values uniformly in the set {a, a + 1, . . . , b}. In the first simulation, the delay distribution of Type-II devices’ local computing delay follows U(1, x) with x increasing from 10 to 20. Different kinds of penalty functions are considered in the simulation, including linear function, square function, and a special type of composite function, as shown in Table II. By varying the distribution of Type-II devices’ local computing delay, we obtain the results shown in Fig. 4. The number of devices is 30. Half of them are Type-I, and the left are Type-II. The number of orthogonal channels is 3. MaxReduction policy [32] is considered for comparison. In MaxReduction policy, the terms Wl,n(hn(k)) Dl,n and Wt,n(hn(k)) Dt,n in (25) are replaced by the expected penalty reduction after scheduling. The lower bound is obtained by solving the optimization problem P4 numerically. The simulation horizon is 106 slots. V is set to be 0.01 for composite penalty function, and 1 for both linear and square penalty functions. The performance of the proposed Max-Weight policy is close to the lower bound. It should be noted that the lower bound is derived by using Jenson’s inequality, and thus the estimation error between the lower bound and the minimum average AoI penalty gets larger for higher-order penalty functions. It is also interesting to check whether the proposed policy does steer AoI to be aligned with (14). Considering the case where the local computing delay of Type-II devices follows TABLE II: List of penalty functions. Function Type-I Type-II Linear x 2x Square 0.1x 2 0.2x 2 Composite 1 − (0.02x + 1)−0.4 1 − (0.14x + 1)−0.4 TABLE III: Coefficient of variation under different penalty functions. Coefficient of Variation Penalty Function Composite Linear Square Max-Weight 0.129 0.062 0.047 Max-Reduction 0.787 0.680 0.539 0 5000 10000 15000 20000 25000 30000 Time Step (slots) 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Time Average Energy Consumption V = 0.1 V = 0.1 V = 1 V = 10 E Fig. 5: The average energy consumption under different V with square penalty function. U(1, 10), we estimate the value of α by plugging the peak AoI value after each computation into (19) and calculate the average for each device. And thus we obtain 30 points, each corresponding to one device. We then calculate the mean value and standard deviation over these 30 devices. The Coefficient of Variation (CV) is listed in Table III, which is the ratio of the standard deviation to the mean. The result shows that the CV values of Max-Weight policy are one order of magnitude smaller than that of the Max-Reduction policy. To investigate the influence of parameter V , we first check the average energy consumption, as shown in Fig. 5. These curves are obtained by running the Max-Weight policy and calculating the moving average of energy consumption. We choose the square penalty function case and plot the first 30000 time slots. The local computing time for Type-II devices is 9 V = 0.1 V = 1 V = 10 40 50 60 70 80 90 Time Average Penalty Lower Bound Uniform Distribution Poisson Distribution Geometric Distribution Fig. 6: The average penalty under different V and delay distributions. U(1, 10). The cyan dashed line corresponds to the energy budget 0.4. The first observation is that all three curves converge to the horizontal cyan line, this is in line with Theorem 3. Another observation is that smaller V results in slower convergence to the expected value. This might be because a larger V means fewer rounds to reach the desired Qn value. However, the convergence speed comes at the price of performance loss. As shown in Fig. 6, increasing V from 0.1 to 10 leads to larger average penalty. This is because that a larger V increases the fluctuation of the virtual queue Q(k), as discussed in Section IV. The influence of delay distribution is also studied in Fig. 6. Fixing the mean value, we run simulations when delay follows uniform distribution, Poisson distribution and geometric distribution respectively. The performance under geometric distribution is the worst. This might be because that the geometric distribution has the largest variance among the three in this case. B. Experimental Results To show the usage of the proposed policy, we choose an object tracking application for demonstration. Tracking object is key to many visual applications [7], [33]–[35]. Given an object’s initial position, the tracker tracks this object as it moves. In this process, tracking error accumulates, and tracking performance would decrease if the tracker has not been refreshed. Fig. 7 gives an example of the tracking process. The red dash box is the position of the target car, and the blue box is the tracking result. After 30 video frames, the blue box drifts away from the true position. After 30 frames Fig. 7: The tracking performance degrades as the object moves. To refresh the tracker, object detection algorithms [36], [37] is called to obtain the current position of the target object. The detection can be done on-device or offloaded to an edge server. Thus, object tracking task can be naturally cast as a status update process, where status update refers to the object detection step. In this case, AoI is defined as the number of video frames since the latest frame used for object detection. To evaluate tracking performance, we first calculate the IoU (Intersection over Union). It represents the area of the intersection over that of the union. Let B1 be the tracking position and B2 be the actual position, IoU is defined in (42). Tracking performance is measured by the probability of the IoU larger than a given threshold IoUth: P (IoUcurr ≥ IoUth), where IoUcurr is the IoU of the current frame. IoU = Area(B1 ∩ B2) Area(B1 ∪ B2) . (42) In this experiment, we choose CSRT algorithm [38] for video tracking, which is faster than DNN-based methods. To isolate influence from the detection algorithm, it is assumed that the detection algorithm can always return the accurate position. We first do profiling on ILSVRC17-VID dataset to evaluate the tracking performance as a function of AoI. The IoU thresholds IoUth are set to be 0.5 and 0.75, representing different requirements for tracking accuracy. 0 10 20 30 40 50 60 70 80 90 AoI (frames) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Successful Tracking Probability Thres=0.5 Thres=0.5(fitted) Thres=0.75 Thres=0.75(fitted) Fig. 8: Profiling result of the successful tracking probability as a function of AoI. 30% of the videos in the dataset are chosen for profiling, i.e., 540 videos. For each video, we start from frame 1, initialize the CSRT tracker with bounding boxes, and let it track the following 90 frames. Then, the tracker is refreshed with the actual positions in the 91-st frame and repeats the procedure. Fig. 8 shows the profiling result, where these two curves can be fitted by functions of the form of (ax + 1)−b . Table.IV shows the fitted parameters. Thus, the penalty function is modeled as 1 − (ax + 1)−b with the penalty being the tracking failure probability. This experiment is done on a simulator we build on the server. We set the number of tracking devices to be 20, half of which are labeled as Type-I device with IoUth = 0.5. The 10 TABLE IV: Fitted parameters with different IoU thresholds. IoU Requirement a b 0.5 0.02149158 0.45788114 0.75 0.14155363 0.45766638 other half are labeled as Type-II device with IoUth = 0.75. As for parameter settings, we set both two types’ local computing delay follows Gaussian distribution N (200, 30)ms [7], truncated above 0. The local computing power is set to be 2.5W. For transmission part, the Type-I’s transmission delay follows distribution N (30, 10)ms, and Type-II’s transmission delay follows distribution N (60, 20)ms. The transmission power is set to be 250mW. The energy budget is set to be 300mW. For the computation delay on the edge side, we test the inference time of Faster-RCNN network [39] with ResNet50 [40] as backbone on a Linux server with TITAN Xp GPU. The computation time distribution is shown in Fig. 9. 40 60 80 100 120 140 Computation Latency (ms) 0.00 0.01 0.02 0.03 0.04 0.05 Probability Density Fig. 9: Computation time distribution of detection. Two polices based on video content are adopted for comparison. The first is NCC (Normalized Cross Correlation) policy [41]. NCC refers to the cross-correlation between two regions. A small cross-correlation value suggests that the detected object has significant change, and thus the tracker is likely to be inaccurate. Thus, NCC value is plugged into (25) as done in Max-Reduction policy. The second is CIB (Current IoU Based-) policy. In CIB policy, we assume the scheduler knows the IoU between the tracking position and the actual position. The IoU value is plugged into (25) for scheduling. Note that CIB policy requires knowledge of the actual position and thus cannot be implemented in real scenario. We just use it for comparison. The parameter V is set to be 0.01. To evaluate the performance on ILSVRC17-VID dataset, we randomly take 300 videos from the videos that are not used for profiling. Fig. 10a compares the success tracking probability of Max-Weight policy, NCC and CIB. As shown in this figure, Max-Weight policy outperforms the other two for both two types of devices, and improves the total successful tracking probability by 27% compared with NCC. In Fig. 10b, the cumulative distributions of IoU under these two policies are Type-I Type-II Total 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Successful Tracking Probability 0.75 0.4 0.57 0.87 0.46 0.66 0.89 0.57 0.73 NCC Policy CIB Policy Max-Weight Policy (a) Successful tracking probability results 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 IoU 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Cumulative Distribution NCC Max-Weight CIB NCC Policy CIB Policy Max-Weight Policy (b) Cumulative distribution of IoU Fig. 10: Performance comparison between two policies. presented. As we can see, Max-Weight policy enjoys better tracking performance. It is surprising to observe in Fig. 10 that CIB policy is worse than the Max-Weight policy, as CIB uses knowledge of the actual IoU. This phenomenon might be due to two reasons. First, CIB policy doesn’t take transmission and computation delay into consideration, and this might lead to bad resource allocation. Second, CIB policy only uses current IoU, while the profiling curves in Fig. 8 incorporate long-term information. This motivates us to investigate how to represent content semantics from a time perspective. VI. CONCLUSION To support emerging real-time applications with computation-intensive status update, it is critical to efficiently manage communication and computation resources in the network system to provide as fresh status information as possible. To fully utilize computation resources, we considered a hybrid computation framework where computation tasks can be processed on-device or offloaded to an edge server. Task-specific timeliness requirement was modeled as penalty functions of AoI. We first analyzed the minimum average AoI penalty and formulated an optimization problem to compute the penalty lower bound. Based on the lower bound, we proposed indices to quantify the priorities of local computing 11 and edge computing respectively. Combining energy virtual queue with these indices, we proposed a Max-Weight scheduling policy, inspired by the optimal conditions of the lower bound problem. Extensive simulations showed that our proposed policy has close-to-optimal performance under different penalty functions. We also applied the proposed policy to object tracking tasks on ILSVRC17-VID dataset and improved the tracking accuracy compared with scheduling polices based on video content information.",
		"summary": "Intelligent real-time applications, such as video surveillance, demand intensive computation to extract status information from raw sensing data. This poses a substantial challenge in orchestrating computation and communication resources to provide fresh status information. In this paper, we consider a scenario where multiple energy-constrained devices served by an edge server. To extract status information, each device can either do the computation locally or offload it to the edge server. A scheduling policy is needed to determine when and where to compute for each device, taking into account communication and computation capabilities, as well as taskspecific timeliness requirements. To that end, we first model the timeliness requirements as general penalty functions of Age of Information (AoI). A convex optimization problem is formulated to provide a lower bound of the minimum AoI penalty given system parameters. Using KKT conditions, we proposed a novel scheduling policy which evaluates status update priorities based on communication and computation delays and taskspecific timeliness requirements. The proposed policy is applied to an object tracking application and carried out on a large video dataset. Simulation results show that our policy improves tracking accuracy compared with scheduling policies based on video content information. ",
		"id": "UUID27"
	},
	{
		"document": "I. INTRODUCTION The Internet of Things (IoT) is emerging as a critical technology to connect a large number of resource-constrained devices such as sensors and actuators as well as appliances to the Internet [1]. Many industries, including smart grids, healthcare, vehicular telematics, smart cities, security and public safety, agriculture, and industrial automation, extensively use IoT networks [2]. Active research is being conducted on designing effective networking protocols to handle the growing number of IoT devices. The design of medium access control (MAC) protocols for IoT networks is particularly challenging because of their unique characteristics [3]. For instance, (i) network access must be provided to a large number of IoT devices, (ii) most IoT devices are battery-powered and have limited power availability, and (iii) the quality of service (QoS) requirements in IoT applications differ from those in human-to-human (H2H) communications [3]. One of the key components of a MAC protocol for IoT networks is a node cardinality estimation protocol that rapidly estimates the number of active devices (i.e., the devices that currently have some data that needs to be transferred to the base station) in every time frame [3]. These estimates can be used to determine P.S. Page, V.S. Borkar, and G.S. Kasbekar are with the Department of Electrical Engineering, Indian Institute of Technology (IIT) Bombay, Mumbai, India. A.S. Siyote is with TIH Foundation for IoT and IoE, IIT Bombay, Mumbai, India. Their email addresses are pranavpage33@gmail.com, borkar.vs@gmail.com, gskasbekar@ee.iitb.ac.in, and anand.siyote@tihiitb.org, respectively. The contributions of V.S. Borkar and G.S. Kasbekar have been supported by the grant RD/0222-EETIHBY-014. the optimal values of various MAC protocol parameters such as contention probability, contention period duration, data transmission period duration, etc. [4]–[6]. Node cardinality estimation protocols also have a large number of applications apart from their use in MAC protocol design. They are used in [7] to periodically estimate the numbers of vehicles moving on various congested routes; the estimated information can be used to dynamically adapt the ON/ OFF periods of traffic lights based on vehicle density. Consider a farm with sensors installed to track a number of variables such as temperature and soil moisture. Before gathering the actual data from the active sensors, a mobile base station (MBS), e.g., one mounted on an unmanned aerial vehicle (UAV), navigates over the agricultural area and stops at designated locations to estimate the number of active sensors [8]. This increases the effectiveness of data collection because the MBS can optimally determine the amount of time it needs to spend at each stop when it subsequently returns to the same locations to collect the actual data and because it can inform the active sensors when to be available for sending the data based on the estimates. During or after natural calamities such as floods and earthquakes, MBSs hover above the affected area to estimate the number of people who need help. These estimates are used to plan disaster relief efforts and efficiently distribute supplies [9]. Also, numerous RadioFrequency IDentification (RFID) systems use node cardinality estimation protocols for inventory management, tag identification, missing tag detection, etc. [10]–[19]. Extensive research has been conducted on the problem of node cardinality estimation in IoT networks and RFID systems. Most of this research is focused on node cardinality estimation in a homogeneous network, wherein the network consists of only one type of nodes [20]–[41]. In addition, some work has been carried out on node cardinality estimation in a heterogeneous network, that is, a network consisting of T types of nodes, where T ≥ 2 is an integer [2], [4], [5], [42]– [48]. Different types of nodes in a heterogeneous network may have different hardware and software capabilities such as different processor speeds, transmission power, memory, battery life, etc.; also, different types of nodes may have different QoS requirements [43]. In a heterogeneous network with T types of nodes, execution of an estimation protocol designed for a homogeneous network T times to obtain separate estimates of the cardinality of each node type is inefficient and improved protocols that rapidly obtain these estimates have been proposed in [2], [4], [5], [42]–[48]. However, all the protocols designed so far for node cardinality estimation in homogeneous and heterogeneous networks use simple arXiv:2310.18664v1 [cs.NI] 28 Oct 2023 2 techniques, which are sub-optimal, for computing the node cardinality estimates. Very little research has been conducted so far on applying machine learning based techniques to compute node cardinality estimates; in particular, to the best of our knowledge, the powerful Privileged Feature Distillation (PFD) technique [49] has not been used in prior work for node cardinality estimation in wireless networks. This is the space in which we contribute in this paper. In this paper, we propose a novel methodology for node cardinality estimation in wireless networks such as the IoT and RFID sytems, which uses the PFD technique and works using a neural network with a teacher-student model [49]. The teacher is trained using both privileged and regular features, and the student is trained using predictions from the teacher and regular features [49]. The concept of a privileged feature [49] arises in scenarios where a particular feature z is available during the training phase but not during the testing or inference phase. The term “privileged” refers to the notion that this feature possesses additional information during the training process that can potentially aid in improving the prediction accuracy or performance. By identifying privileged features and incorporating them into the training process, it is possible to leverage the additional information they provide to potentially improve the model’s predictive capabilities, when those features are not available during the testing phase. Distillation [50] refers to the standard practice of labeling the training dataset using teacher predictions, and using these as supervision targets in the training of the student model. PFD has been successfully applied in various machine learning problems including speech recognition [51], medical imaging [52], and image super-resolution [53]. We review some background concepts pertaining to PFD in Section IV-C. The main contributions of this paper are as follows. In Section III-A, we formulate the problem of estimating the number of active nodes in a homogeneous wireless network, while minimizing the mean squared error (MSE) between the actual number of active nodes and the algorithm’s estimate. In Section III-B, we generalize this problem formulation to the problem of estimating the number of active nodes of each type in a heterogeneous wireless network with T types of nodes, where T ≥ 2 is an integer, while minimizing the MSE. In Section V-A, we propose a novel algorithm, which uses the PFD technique and a neural network with a teacher-student model, for node cardinality estimation in a homogeneous network. In Section V-B, we generalize this algorithm for estimating the cardinality of each node type in a heterogeneous network with T types of nodes. In Section VI, we show via extensive simulations that the proposed PFD based algorithm for a homogeneous (respectively, heterogeneous) network achieves a much lower MSE than the state-of-theart simple RFID counting (SRCs) protocol [54] (respectively, T-SRCs protocol), even though both the algorithms take the same number of time slots for executing the node cardinality estimation process. The rest of this paper is organized as follows. A review of related prior literature is provided in Section II. The system model and problem formulation are described in Section III. Some relevant background is given in Section IV. The proposed algorithms and other algorithms for comparison are described in Section V. Simulation results are provided in Section VI. Finally, conclusions and directions for future research are provided in Section VII. II. RELATED WORK In Section II-A (respectively, Section II-B), we provide a review of related prior literature on node cardinality estimation in wireless networks (respectively, PFD). A. Node Cardinality Estimation The estimation of active node cardinalities is considered crucial in the design of a MAC protocol for IoT networks. This importance has led to extensive research being conducted on this topic [26]–[35]. These studies focus not only on estimating the number of active devices in a homogeneous IoT network, but also on using these estimates to determine the contention probabilities that optimize the throughput of their respective MAC protocols for IoT networks. To estimate the number of active nodes in the current time frame, the estimation scheme proposed in [27] uses the estimates obtained in the previous frame as well as the sub-optimal Dynamic Access Class Barring (D-ACB) factors from the previous frame. In [28], a modified Carrier Sense Multiple Access/ Collision Avoidance (CSMA/CA) protocol intended for IoT networks was introduced. The size of the backoff window for the current time frame is chosen by this protocol by considering the size of the previous backoff window and the previously computed estimates of the active node cardinality. This procedure incorporates historical estimates to improve the effectiveness of the backoff mechanism. Note that both [27] and [28] relied on the estimates obtained in previous frames to compute their estimates in the current frame. This iterative approach allows the utilization of past information to improve the accuracy and effectiveness of the estimation process. A new technique for dynamic access control and resource allocation for random-access channels based on an estimation scheme was introduced in [29]. The only input used in the estimation procedure in [29] for computing the estimates was the number of open slots. The 6-Dimensional Markov Chain (6-DMC) estimation method was introduced in [30]. The numbers of devices that are delay tolerant (DTDs) and delay sensitive (DSDs) are estimated using this approach. The estimation methods in [30], [31], and [32] are based on 6- DMC, Maximum Likelihood Estimation (MLE), and IoT-OSA (an extension of the opportunistic splitting technique). The satellite random access (RA) MAC protocol is provided in [55]. In this protocol, throughput is maximized by computing an estimate of the number of Return Channel Satellite Terminals (RCSTs). The number of collisions observed in earlier frames affects the length of the current frame in the model described in [55]. The approach described in [33] estimates the number of nodes that cause collisions. In LongTerm Evolution (LTE) networks, this estimation enables an effective partitioning of nodes into a predetermined number of groups while minimizing intra-group collisions. Dynamic Backoff (DB), a new method for resolving channel contention, was first described in [34]. Based on the estimated number of 3 competing active devices, this approach modifies the size of the backoff window used to manage channel contention during data transfer. The scheme proposed in [34] also dynamically modifies each frame size based on the projected number of devices, making it adaptable to shifting network circumstances and device activity. The node cardinality estimation problem in IoT networks is similar to the tag cardinality estimation problem in the context of RFID technology. In the latter situation, an RFID reader estimates the number of tags, like a base station does when estimating the number of active nodes in an IoT network. Schemes for estimating the number of tags in an RFID system were proposed in [20]–[25], [36]–[41], [54]. Node cardinality estimation schemes for heterogeneous IoT networks and RFID systems have been proposed in [2], [4], [5], [42]–[48]. A specialized MAC protocol for a heterogeneous IoT network, catering to three types of IoT devices, has been introduced in [4], [5]. It incorporates a rapid estimation protocol to determine active node counts, and uses them to optimize the contention probabilities in the MAC protocol. An efficient node cardinality estimation solution with two components– snapshot collection and accurate estimation– has been given in [46]. It focuses on improving joint cardinality estimation in distributed RFID systems, allowing queries across multiple tag sets at different locations and times with controlled error. It has applications in tracking product flows in logistics. Simulations show a significant time cost reduction while maintaining accuracy. Enhancement of RFID technology’s cardinality estimation function in two ways– joint estimation across tags at different locations and times and category-level tracking– was proposed in [47]. It introduces an anonymous protocol that efficiently estimates joint category-level information, preserving tag anonymity and enabling applications such as monitoring diverse products in distributed supply chains. Multi-category RFID tag estimation has been addressed in [48], aiming to swiftly and accurately count tags within each category. It introduces the “Simultaneous Estimation for Multi-category RFID Systems” (SEM) approach, leveraging Manchester coding to decode combined signals, allowing simultaneous estimation across categories while maintaining pre-defined accuracy. SEM significantly improves estimation speed compared to existing protocols. Rapid estimation of the cardinalities of active nodes of different types in heterogeneous IoT networks with T node types, where T ≥ 2 is an arbitrary integer, has been addressed in [42]– [45]. However, the PFD technique has not been applied to the problem of node cardinality estimation in either a homogeneous or heterogeneous wireless network in any of the above prior works. B. Privileged Feature Distillation (PFD) The concept of learning with privileged features was introduced in [56], and a framework called “learning using privileged information” (LUPI) was proposed. Privileged information is the primary approach used by LUPI to discriminate between simple and complex cases. These methods are closely related to Support Vector Machines (SVM); e.g., the “SVM+” algorithm, which creates slack variables from privileged features and learns an SVM based on regular features with those slack variables, is proposed in [56], [57]. A pairwise SVM algorithm for ranking that uses privileged features to differentiate between easy and hard pairs is proposed in [58]. The privileged features are employed in the version presented in [59] to produce importance weighting for various training samples. A popular technique for knowledge transfer is model distillation [50], often from a large model to a smaller model [60], [61]. Recent studies [62]–[64], and even those where the teacher model and student model have the same structure [65], [66], have demonstrated remarkable empirical success in ranking tasks. “Generalised distillation” (GenD) is the term for the method first suggested in [67] for using distillation to learn from privileged features. This offers a comprehensive perspective on distillation and LUPI. GenD and its derivatives [51], [53], [68] train an expert model using just privileged features, after which the student model is trained to replicate the expert’s predictions. Recently, PFD was presented in [69], where the teacher model accepts input from both regular and privileged features. Due to their emphasis on privileged feature exploitation rather than model size reduction, PFD and GenD are different from traditional model distillation. On a nonpublic data collection, the improved performance of PFD for recommendation systems is empirically demonstrated in [69]. Despite the aforementioned empirical accomplishment, there remains a lack of understanding of privileged characteristics distillation. Prior research [70] demonstrates that LUPI speeds up convergence under the strict premise that the best classifier can be realised using just privileged information. GenD has a quick convergence rate, as shown by [67]. This is different from PFD since it assumes that the function class complexity of the student model is significantly larger than that of the teacher model. The study by [71] on GenD under semi-supervised learning reveals that the benefits come from reducing the complexity of student function classes. However, it does not quantify this reduction, and the theory does not explain why exploiting privileged traits is advantageous. Prior proposals include other uses of privileged features. To enhance picture classification performance, [72] learns a more varied representation using privileged information. Distillation strategies have been proposed by [53], [73] for more effective feature extraction from regular features. A more recent study [74] examined the possibility of training a model using both regular and privileged features to improve the internal representation of regular features. However, to the best of our knowledge, this paper is the first to use the technique of PFD to address the problem of node cardinality estimation in wireless networks. III. SYSTEM MODEL AND PROBLEM FORMULATION In Section III-A (respectively, Section III-B), we describe the system model and problem formulation for the case of a homogeneous network (respectively, heterogeneous network). 4 Type 1 nodes Base Station Type 2 nodes Type 3 nodes Figure 1: The figure shows an example heterogeneous network with T = 3 types of nodes in the range of a BS. A. Homogeneous Network 1) System Model: Consider a population of nodes such that each node is in the range of a single stationary base station (BS). We consider a node as active when it has data to send to the BS. Time is divided into frames of equal durations. To effectively design MAC protocols for data upload to the BS, the number of active nodes in a time frame must be estimated. We study the case, which often arises in practice, when there exists some correlation between the number of nodes active in a time frame and the number of nodes active in the next time frame. E.g., the number of active nodes may evolve as a discrete-time Markov chain. 2) Problem Formulation: The aim of this work is to design algorithms that minimize the mean squared error (MSE) between the actual number of active nodes in a time frame and the algorithm’s estimate of this number, while also reducing the number of time slots needed to produce the estimate. The objective of minimizing the MSE for a homogeneous network of nodes is as follows: alg′ = argminalgE ' limτ→∞ 1 τ τX−1 t=0 (ˆn alg t − n truth t ) 2 # . (1) Here, n truth t is the true number of nodes active in time frame t, while nˆ alg t is the estimate given by the algorithm alg in time frame t. The expectation is with respect to the different realizations of the random process (n truth t , t = 0, 1, 2, . . .). The objective is to find the algorithm alg′ that achieves the minimum in the RHS of (1). B. Heterogeneous Network 1) System Model: In the heterogeneous case, there exist T types of nodes, where T ≥ 2 is an integer, in the range of a BS, as shown in Figure 1 for the case T = 3. For example, nodes of different types may correspond to nodes that send emergency traffic such as fire alarms, nodes that contain moisture sensors, nodes that contain temperature sensors, etc. We represent the numbers of active nodes of different types by n truth t , a 1 × T vector, where n truth t [b], b ∈ {1, · · · , T}, is the number of active nodes of type b in time frame t. 2) Problem Formulation: Similar to the homogeneous case, for the heterogeneous case, the objective is to minimize the expected time-averaged squared Euclidean distance between the estimates computed by the algorithm and the time series, n truth t , of true numbers of active nodes: alg′ = argminalgE ' limτ→∞ 1 τ τX−1 t=0      nˆ alg t − n truth t       2 2 # . (2) Here, nˆ alg t is a 1 × T vector, where nˆ alg t [b], b ∈ {1, · · · , T}, is the estimate of the number of active nodes of type b in time frame t found by the algorithm alg. The objective is to find the algorithm alg′ that achieves the minimum in the RHS of (2). IV. BACKGROUND In this section, we review some concepts that are used in the rest of the paper. A. Simple RFID Counting (SRCs) Protocol SRCs is a node cardinality estimation protocol for a homogeneous network [54], which finds an estimate, nˆ, of the number of active nodes, n, to within given accuracy requirements ϵ and δ, i.e, the following relation is satisfied: P(|nˆ − n| ≤ ϵn) ≥ 1 − δ. The SRCs protocol (Algorithm 3) [54] uses the Lottery Frame (LoF) protocol (Algorithm 1) to generate a rough estimate of the number of active nodes, followed by a Balls and Bins (BB) trial (Algorithm 2) that uses the rough estimate given by LoF. The LoF protocol uses a trial length of llof = ⌈log2 nall⌉ time slots, where nall is the maximum number of active nodes in the network. The SRCs protocol conducts multiple, say num lof, LoF trials and computes an average of the rough estimates generated in the trials, n ′ . The choice of the length of the BB trial l depends on the relative error tolerated, ϵ, and is taken as l = 65 (1−0.04ϵ) 2 [54]. The number of LoF trials, num lof, in SRCs is taken to be of the order O(log 1 δ ). For example, for δ = 10−3 , num lof = 3 is used. The output of the BB trial is the final SRCs estimate, nˆ. The frame structure of SRCs is shown in Figure 2. Algorithm 1 Lottery Frame Protocol [54] 1: Choose trial length llof = ⌈log2 nall⌉ 2: Each active node independently transmits in slot i = 1, . . . , llof − 1 with probability 2 −i and in slot llof with probability 2 −(llof −1) 3: Trial ends when first empty slot (slot in which no node transmits), say slot j, is seen 4: return n ′ = 1.2897 × 2 j B. 3-Stage Scheme-Balls and Bins (3-SS-BB) Protocol 3-SS-BB is a protocol for finding an estimate, nˆb, of the number of active nodes, nb, of each type b ∈ {1, . . . , T} in a heterogeneous network with T types of nodes (see Figure 1) [42], [43]. It is an extension of the BB trial (Algorithm 2) to 5 1 2 i−1 i ℓ p ℓ 1 2 i llof 1 2 i−1 1 2 i 1 2 1 2 2 1 2 l lof −1 1 2 l lof −1 Probability of transmission in each slot by each node Phase 1 (llof = ⌈(log2nall)⌉), num lof trials Phase 2 Slot Number i−1 ℓ−1 llof −1 1 2 i llof 1 2 i−1 1 2 i 1 2 1 2 2 1 2 l lof −1 1 2 l lof −1 i−1 llof −1 1 2 i llof 1 2 i−1 1 2 i 1 2 1 2 2 1 2 l lof −1 1 2 l lof −1 i−1 llof −1 p ℓ p ℓ p ℓ p ℓ p ℓ Figure 2: The figure shows the frame structure of the SRCs protocol. Algorithm 2 Balls and Bins Protocol [54] 1: Given rough estimate n ′ , each active node independently participates in the trial of length l slots with probability p = min(1, 1.6l/n′ ) 2: Each participating node transmits in a slot chosen uniformly at random from the l slots 3: z = number of empty slots in trial 4: if z > 0 then 5: return ln (z/l) ln (1−p/l) 6: else 7: return arbitrary number 8: end if a heterogeneous network. It assumes that rough estimates, n ′ b , b ∈ {1, . . . , T}, of the numbers of active nodes of the T types are initially available; e.g., these estimates may be generated by separately conducting LoF trials for each node type as in the first stage of SRCs. 3-SS-BB uses a trial with l blocks; within each block, there are T − 1 slots. Each active node of type b independently participates in the trial of l blocks with probability pb = min(1, 1.6l/n′ b ). Each participating node of type b chooses a block out of the l blocks uniformly at random Algorithm 3 SRCs Protocol [54] 1: Conduct num lof LoF trials and compute the following rough estimate: n ′ = 1.2897 × 2 Pnum lof m=1 (j(m)−1)/num lof, where j(m) is the first empty slot in the m′ th LoF trial 2: Run a BB trial of length l in which each node participates with probability p = min(1, 1.6l/n′ ) 3: Count the number of empty slots, say z, in the trial 4: if z > 0 then 5: return ln (z/l) ln (1−p/l) 6: else 7: return arbitrary number 8: end if β 0 0 β 0 0 0 0 0 0 0 0 0 0 0 Symbol Combinations 1 2 3 T − 2 T − 1 Slots Types 1 2 3 α α α α α 0 β 0 0 0 β T − 1 T β 0 4 0 0 Figure 3: The figure shows the symbol combinations used by different types of nodes under the 3-SS-BB protocol. 0 indicates no transmission. and sends, in its chosen block, the symbol combination of length T − 1 slots given in the row corresponding to type b in Figure 3. For example, each node of type 1 transmits the pattern (α, α, · · · , α), while each node of type 2 transmits (β, 0, · · · , 0), where α and β are distinct symbols (bit patterns) and 0 indicates no transmission. If there are two or more transmissions in a slot, the result of the slot is c (collision). Thus, a slot can have four possible outcomes: {0, α, β, c}. Estimates nˆb, b ∈ {1, . . . , T}, are generated based on the outcomes of the (T − 1)l slots using the algorithm provided in [42], [43]. 3-SS-BB is summarised in Algorithm 4. Algorithm 4 3-SS-BB 1: Given rough estimate n ′ b , each active node of type b independently participates in the trial of l blocks with probability pb = min(1, 1.6l/n′ b ) 2: Each participating node chooses a block out of the l blocks of the trial uniformly at random 3: Each node of type b sends, in its chosen block, the symbol combination of length T − 1 slots given in the row corresponding to type b in Figure 3 4: In each of the resultant (T −1)l slots, the outcome is one of {0, α, β, c} 5: Estimates nˆb, b ∈ {1, . . . , T}, are generated based on the outcomes of the (T − 1)l slots C. Privileged Features Distillation (PFD) In some problem settings, there exist some features that are not available during testing, but are available offline for training. Instead of discarding these features, one approach is to train a ‘teacher’ model on the privileged features, say xprivileged [49]. The teacher model is then used in the training of a different ‘student’ model [49]. The student is trained only on the features, say xgeneral, that are available during testing, but the loss of the model is designed (see (5)) as a convex combination of the student loss and the teacher loss. In (3), L teacher refers to the teacher loss corresponding to the loss described by the loss function L(·, ·), operating on the prediction of the teacher and the target y. The teacher 6 network is represented by g teacher, and the teacher’s prediction is g teacher(xprivileged). Similarly, in (4), the data loss L data is the loss between the student’s prediction g student(xgeneral) on the general features, xgeneral, and the target y. The mixing ratio between the data loss and the teacher loss is α ∈ [0, 1]. The student does not interact with the privileged features, nor with the teacher’s predictions, but only with the loss between the teacher’s prediction and the target. L teacher = L(g teacher(xprivileged), y) (3) L data = L(g student(xgeneral), y) (4) L student = αL data + (1 − α)L teacher (5) V. ALGORITHMS In Section V-A (respectively, V-B), we describe our proposed PFD based algorithm, and other algorithms for comparison, for homogeneous (respectively, heterogeneous) wireless networks. A. Homogeneous Network 1) Proposed Algorithm: Consider the model and problem formulation described in Section III-A. The entire population of nodes in the range of the base station is of a single type. Recall from Section IV-A that the SRCs protocol consists of a LoF-based phase 1 and a BB-based phase 2. The LoF phase obtains a rough estimate of the number of nodes, n ′ , which the BB phase uses to obtain a refined estimate. In each time frame t, the proposed neural network (NN) based algorithm (see Algorithm 5) executes only phase 2 (BB) and obtains the trial result. If the trial consists of lBB slots, then a vector of size lBB is generated via BB. This vector consists of the outcome (no transmission, success (one transmission), or collision) in each of the lBB slots of the BB trial. The trained model takes this vector as input, along with the estimate of the number of active nodes generated by the model in the previous time frame, and estimates the value of the number of active nodes in the current time frame. The NN is a student model trained using PFD as explained in Section V-A2; so henceforth, the trained model will be represented by the notation Stu. In time frame 0, the proposed NN method conducts a set of LoF trials to give the initial rough estimate nˆ ′ 0 , which is then used as the rough estimate (see step 1 in Algorithm 2) in a BB trial. The NN then uses the result of the BB trial, which is a vector of length lBB, say v0, and the rough estimate nˆ ′ 0 , to generate its own estimate in time frame 0, nˆ Stu 0 , using the student network g Stu. Subsequently, in each time frame t = 1, 2, . . . , num iters, where num iters is the total number of time frames, a BB trial is conducted with rough estimate nˆ Stu t−1 (Stu’s estimate of the previous time frame), which generates a vector of length lBB, say vt, as a result. The NN g Stu operates on (vt, nˆ Stu t−1 ) to produce its estimate nˆ Stu t in time frame t. The motivation for using the estimate of the previous time frame, nˆ Stu t−1 , as the rough estimate for the BB trial of the current time frame t arises from the fact that some correlation exists between the nodes active in the previous time frame and the nodes active in the current time frame. We exploit this fact to reduce the number of time slots used by not executing LoF Figure 4: The figure shows the neural network architecture of a student (Stu) model used in the case of a homogeneous network. trials to obtain the rough estimates for the BB trials of time frames t = 1, 2, 3, . . .. Algorithm 5 Proposed NN Method 1: At t = 0, conduct LoF trials to give the initial rough estimate nˆ ′ 0 ; then conduct BB trial to generate v0 2: nˆ Stu 0 = g Stu(v0, nˆ ′ 0 ) 3: for t = 1, · · · , num iters do 4: Conduct BB trial with rough estimate nˆ Stu t−1 , giving result vt 5: nˆ Stu t = g Stu(vt, nˆ Stu t−1 ) 6: end for The architecture of the NN used is shown in Figure 4. The input dense layer of length L has Rectified Linear Unit (ReLU) activation, while the other two L/2 dense layers (hidden layers) have sigmoid activation. The output layer of length 1 has linear activation. A description of the activation functions is provided in [75]. The architecture has been designed for regression specifically and for ease of training, according to insights from [76]. 2) Training: The estimation of the number of active nodes in each time frame is to be done by a model that uses data recorded in a real online scenario. This implies that each element of the results of the trial vt would comprise of three possibilities: {no transmission, success, collision}. This would be a difficult problem to tackle, and would need more information than the online model has to perform well. Thus, we use PFD, where a teacher model is trained on privileged data not available in the online scenario, in particular, the number of nodes transmitting in each slot of the BB trial and the true value of the number of active nodes in the previous time frame. A student model is trained on the actual data seen during the online scenario. In particular, the objective is to train a student model, which is the actual Stu NN used in Algorithm 5, to perform online estimation of the number of active nodes, nˆ Stu t , in 7 each time frame t using the general feature vector vt and the previous time frame’s estimate nˆ Stu t−1 . The feature vector vt for a BB trial of length lBB has dimensions 1×3lBB since the outcome of each of the lBB slots is represented using one-hot encoding and there are 3 possibilities for a slot– no transmission, success, and collision. Also, L = 3lBB + 1 in Figure 4 since the NN takes as input the vector vt and the estimate of the previous time frame. In the first step, the teacher model is to be trained. The teacher model is fed with privileged information. Let Vt be a 1×lBB vector denoting the results of the BB trial in time frame t and containing privileged information. In particular, for i ∈ {1, . . . , lBB}, Vt[i] is the number of active nodes transmitting in slot i of the BB trial, which is privileged information since when a collision occurs, i.e., two or more nodes transmit in a slot, the number of transmitting nodes is not known in practice. Due to the nature of the algorithm, where the NN’s output in the previous time frame is part of the input vector for the current time frame, if a network is trained on each time frame one-by-one, it overfits the current sample. It is thus able to predict the next few steps accurately, but the same model will ‘forget’ the samples seen a few time frames ago. Hence, the procedure of getting a model to be a genie to ‘track’ the evolution of the time series, logging the dataset, shuffling it, and training a new model to mimic the genie is followed. The model trained offline is able to go through the data multiple times and out-of-order. This calls for a step of data generation– creating a dataset with ((Vt, ntruth t−1 ), ntruth t ) as the (feature, target) tuple for time frame t. The dataset is then shuffled and a new uninitialized teacher network is trained on this dataset. For data generation, recall that the BB trial requires a rough estimate of the number of active nodes. We use a genie network for this purpose, as explained in Algorithm 6. The genie teacher is trained on the BB trial in each time frame t after generating the prediction nˆ gT r t , which is used as the initial rough estimate for the BB trial in time frame t + 1. The genie teacher network is represented by g gT r. The estimate by the genie teacher for the trial at time t is nˆ gT r t , and is given by the following equation: nˆ gT r t = g gT r(Vt, ntruth t−1 ). (6) The teacher training is explained in Algorithm 6. In GEN TEACHER TRAINING DATA, a new network gT r is initialized, LoF trials are conducted to give an initial rough estimate n ′ 0 , and a BB trial is conducted with rough estimate n ′ 0 . The resulting privileged information V0 is used for prediction by gT r to generate nˆ gT r 0 . Subsequently, in each time frame t, a BB trial is run with rough estimate nˆ gT r t−1 , and the privileged vector Vt is fed to the gT r network. The feature vector (Vt, ntruth t−1 ) and the target n truth t are stored in the teacher T r training data. The genie gT r is trained on the current data point with the loss L gT r t given by: L gT r t = (ˆn gT r t − n truth t ) 2 . (7) In TRAIN TEACHER OFFLINE, the generated T r training data is shuffled to avoid overfitting. The dataset is split into train and test, and a new teacher network T r is trained on the dataset using the MSE loss given by: L T r t = L(ˆn T r t , ntruth t ) = (ˆn T r t − n truth t ) 2 . (8) Algorithm 6 Teacher Training 1: function GEN TEACHER TRAINING DATA 2: Randomly initialize gT r’s weights 3: At t = 0, conduct LoF trials to give the initial rough estimate nˆ ′ 0 ; then conduct a BB trial to generate V0 4: nˆ gT r 0 = g gT r(V0, nˆ ′ 0 ) 5: for t = 1, . . . , num iters do 6: Run BB trial with rough estimate n ′ = ˆn gT r t−1 to generate Vt 7: nˆ gT r t = g gT r(Vt, ntruth t−1 ) 8: Save ((Vt, ntruth t−1 ), ntruth t ) in T r training data 9: Fit g gT r(.) to ((Vt, ntruth t−1 ), ntruth t ) using loss L gT r t = (ˆn gT r t − n truth t ) 2 10: end for 11: return T r training data 12: end function 13: function TRAIN TEACHER OFFLINE(T r training data) 14: Randomly initialize teacher T r 15: Shuffle and split dataset into train and test 16: Train T r with loss L T r t = (ˆn T r t − n truth t ) 2 17: return T r 18: end function The student model does not see the privileged information (number of active nodes transmitting in each slot, true value of the number of active nodes in the previous time frame). It only sees the result of each slot (no transmission, success, or collision). Thus, the student has (vt, nˆ Stu t−1 ) to calculate nˆ Stu t , which is given by: nˆ Stu t = g Stu(vt, nˆ Stu t−1 ). (9) Instead of the standard regression MSE loss, distillation loss is used, which includes the loss between the teacher’s prediction and the truth, and is given by: L Stu t = (1 − α)L(ˆn T r t , ntruth t ) + αL(ˆn Stu t , ntruth t ), (10) where α is the mixing ratio. Similar to the teacher model’s training, the results of the trials executed in different time frames are recorded offline, and a new model is trained on the recorded data with random shuffling. This concludes the training of the student model. The student training is explained in Algorithm 7. In GEN STUDENT TRAINING DATA, similar to the teacher training protocol, a genie student network gStu is initialized. To initialize the protocol, LoF trials are conducted and the initial estimate nˆ ′ 0 is generated. gStu uses the result of the BB trial of time frame t, viz., vt, and the estimate of the previous trial nˆ gStu t−1 to compute nˆ gStu t . The feature vector for the student (vt, nˆ gStu t−1 ), the target n truth t and the feature vector for the teacher (Vt, ntruth t−1 ) are stored in the Stu training data. gStu is then fit on the current sample with the distillation loss, which is a combination of the data loss and the teacher loss. In TRAIN STUDENT OFFLINE, a new student model Stu is 8 Algorithm 7 Student Training 1: function GEN STUDENT TRAINING DATA(α) 2: Randomly initialize gStu’s weights 3: At t = 0, conduct LoF trials to give the initial rough estimate nˆ ′ 0 ; then conduct a BB trial to generate v0 4: nˆ gStu 0 = g gStu(v0, nˆ ′ 0 ) 5: for t = 1, . . . , num iters do 6: Run BB trial with rough estimate n ′ = ˆn gStu t−1 7: nˆ gStu t = g gStu(vt, nˆ gStu t−1 ) 8: Save the tuple ((vt, nˆ gStu t−1 ), ntruth t ) in Stu training data; also save Vt for T r prediction 9: Fit g gStu(.) to ((vt, nˆ gStu t−1 ), ntruth t ) with loss L gStu t = α(ˆn gStu t − n truth t ) 2 + (1 − α)(ˆn T r t − n truth t ) 2 10: end for 11: return Stu training data 12: end function 13: function TRAIN STUDENT OFFLINE(T r, Stu training data, α) 14: Randomly initialize Stu’s weights; load pre-trained teacher T r 15: Shuffle and split Stu training data into train and test 16: For a particular BB trial with results (vt, Vt) conducted at time t, calculate the Stu and T r predictions nˆ Stu t = g Stu(vt, nˆ gStu t−1 ) and nˆ T r t = g T r(Vt, ntruth t−1 ) 17: Train Stu with loss L Stu t = α(ˆn Stu t −n truth t ) 2 + (1− α)(ˆn T r t − n truth t ) 2 18: return Stu 19: end function initialized and trained on the recorded data and with the pretrained teacher T r’s predictions. The general information vt and the privileged information Vt are used for Stu and T r inference, giving nˆ Stu t and nˆ T r t , respectively, as the Stu and T r estimates. Stu’s weights are adjusted for minimizing the distillation loss L Stu t = α(ˆn Stu t − n truth t ) 2 + (1 − α)(ˆn T r t − n truth t ) 2 . Intuitively, a high α gives less importance to the T r loss and vice versa. 3) Other Algorithms for Comparison: As a benchmark for comparison with the student model, in each time frame, the SRCs protocol (Algorithm 3) is executed, with the number of time slots used being the same as the length of the BB trial used by the NN (student) model. There is an inherent disadvantage to the SRCs protocol since it does not use knowledge (e.g., estimate of the number of active nodes) from the previous time frame, unlike the NN method. To analyse whether the NN method estimates the number of active nodes better than an SRCs based algorithm that uses knowledge of an estimate of the number of active nodes in the previous time frame, we compare the former against the algorithm BB-Aware, which is described in Algorithm 8. BB-Aware uses SRCs in time frame 0 to generate a rough estimate nˆ BB−Aware 0 . In each subsequent time frame t, BBAware conducts a BB trial of length lBB−Aware and with rough estimate nˆ BB−Aware t−1 (estimate of number of active nodes in the previous time frame) and computes the estimate nˆ BB−Aware t by counting the number of empty slots in the trial (as in BB in Algorithm 2). Figure 5: The figure shows the neural network architecture of a student (Stu) model used in the case of a heterogeneous network. For a fair comparison, each algorithm takes the same number of time slots to execute in each time frame. For example, if the total number of time slots in a time frame is to be 100, then the NN method performs a BB trial of length 100, the SRCs protocol performs 3 LoF trials of length 8 each, and a BB trial of length 76, while the BB-Aware method performs a BB trial of length 100. Algorithm 8 BB-Aware 1: In time frame t = 0, execute SRCs to get nˆ BB−Aware 0 2: for t = 1, . . . , num iters do 3: Conduct a BB trial of length lBB−Aware, with rough estimate nˆ BB−Aware t−1 4: return BB-Aware estimate nˆ BB−Aware t 5: end for B. Heterogeneous Network 1) Proposed Algorithm: Consider the model and problem formulation described in Section III-B. In this case, the coverage area of a base station contains T types of nodes. The problem is to estimate n truth t , a 1 × T vector. The approach followed is largely similar to that described in Section V-A1. Recall from Section V-A1 that in the homogeneous case, a BB trial is conducted in each time frame; instead, in the heterogeneous case, in each time frame, 3-SSBB (explained in Algorithm 4) is conducted, and the results of all the slots are converted into a feature vector. The architecture of the NN used is shown in Figure 5. The input dense layer of length L has relu activation, while the other two L/2 dense layers have sigmoid activation. The output layer of length T has linear activation. A description of the activation functions is provided in [75]. 2) Training: The teacher is trained on the feature vector V T t , which contains the number of nodes of each type participating in each of the l blocks of 3-SS-BB, and n truth t−1 ; note 9 that the size of V T t , n truth t−1  is (l+1)T. The student is trained on v T t , which contains the result of each slot (0, α, β or c (see Algorithm 4)) in each of the l blocks of 3-SS-BB in one-hot encoding format, and ˆn Stu t−1 , which is the vector of estimates of the numbers of active nodes of the T types produced by the student in time frame t−1; note that the size of v T t , ˆn Stu t−1  is 4(T −1)l+T. The training of the teacher and student proceed as per the procedures in Algorithms 6 and 7, respectively, with the feature vectors being as in the heterogeneous case. 3) Other Algorithms for Comparison: As a benchmark for comparison with the student model, in each time frame, SRCs is independently run T times (henceforth referred to as TSRCs)– once for each type of node. Similarly, the BB-Aware algorithm in Algorithm 8 is adapted to the heterogeneous case to give the algorithm T-BB-Aware, wherein in each time frame, BB-Aware is independently run T times– once for each type of node. For a fair comparison, each algorithm takes the same number of time slots to execute in each time frame. In particular, the lengths of the BB trials in 3-SS-BB and T-SRCs are related as in the following equation: l3−SS−BB(T − 1) = (lSRCs + nLoF lLoF )T. (11) The length of a trial in 3-SS-BB is initially fixed and the length of a trial for SRCs (lSRCs ) is computed using (11) and rounded to the nearest integer. The lengths of the BB trials in 3-SS-BB and T-BB-Aware are related as in the following equation: l3−SS−BB(T − 1) = lBB−AwareT. (12) The length of a trial in 3-SS-BB is fixed and the length of the BB trials in T-BB-Aware, lBB−Aware, is computed using (12) and rounded to the nearest integer. VI. PERFORMANCE EVALUATION We describe the simulation setup in Section VI-A. In Section VI-B (respectively, Section VI-C), we provide our simulation results for the case of a homogeneous (respectively, heterogeneous) network. A. Simulation Setup The evolution of the number of active nodes of a given type over different time frames is modeled by a Discrete-Time Markov Chain (DTMC) with N states {0, 1, . . . , N −1}, with a transition probability matrix (TPM) P = [pi,j ], where the transition probabilities are given by the following equation: pi,j =    q, if i = j, 1 − q, if i = 0, j = 1, 1 − q, if i = N − 1, j = N − 2, 1 − p − q, if i ̸= 0, N − 1 and j = i − 1, p, if i ̸= 0, N − 1 and j = i + 1. (13) We consider the case when p = (1 − q)/2, which indicates that it is equally likely to go from a state i to states i + 1 and i − 1. A visual representation of P is shown in Figure 6a. In order to model more sudden changes, we consider the k-step transition probability matrix (P k ), which allows changes in (a) P for q = 0.2 (b) P 5 for q = 0.2 Figure 6: Figures (a) and (b) show the transition probability matrices P and P 5 , respectively, for q = 0.2. the number of active nodes from one time frame to the next one by more than 1. An example is shown in Figure 6b. In the homogeneous case, let n truth t be the number of active nodes in time frame t. The system evolves as in the following equation: P[n truth t = j|n truth t−1 = i] = P k [i, j], (14) where P k [i, j] is the (i, j)’th element of the matrix P k . Also, in the heterogeneous case, the number of active nodes of each type evolves as in (14), with the DTMCs for different types of nodes being independent. Throughout the simulations, for the homogeneous case, the maximum number of active nodes in a time frame is taken to be 64 and for the heterogeneous case, the maximum number of active nodes of each type in a time frame is taken to be ⌊192/T⌋, where T is the number of types. B. Homogeneous Network Throughout the paper, an “epoch” refers to one complete pass through the entire training dataset during training. We plotted the evolution of the training and test loss over 25 10 Figure 7: The figure shows the training of the student using the teacher over epochs with lBB = 100, α = 0.1, k = 5, a teacher dataset of 104 time frames and a student dataset of 104 time frames. Figure 8: The figure shows the normalized mean squared errors (MSE) achieved by the NN, SRCs, and BB-Aware methods, for a student network with lBB = 100, α = 0.1, k = 5, num lof = 3, llof = 8, a teacher dataset of 104 time frames and a student dataset of 104 time frames. epochs, when the student is trained using the teacher. We observed that around 500 epochs, the test loss increases significantly, while having insignificant variation in the training loss. The training of the student was thus stopped at that point. Figure 7 shows the evolution of the training and test loss over 500 epochs, when the student is trained using the teacher. The mixing ratio used in the training is α = 0.1. Figure 8 shows the normalized mean squared errors (MSE) in the active node cardinality estimates computed by the trained student NN, the SRCs protocol, and the BB-Aware method, in different time frames in a homogeneous network. It is seen that the student NN achieves much lower normalized MSEs than both the other methods. Experiment 1 : Changing the length of the BB trial To study the variation of the error on changing the length of the BB trial, a different student network was trained for each length of trial. Each trained student network was then evaluated and the performance averaged over 20 runs of 2000 Figure 9: The figure shows the performance of different methods versus the number of time slots per time frame, with α = 0.1, k = 5, num lof = 3, llof = 8, a teacher dataset of 104 time frames and a student dataset of 104 time frames. Figure 10: The figure shows the performance of the student network versus the number of jumps in the transition probability matrix, for a student network with lBB = 100, α = 0.1, a teacher dataset of 104 time frames and a student dataset of 104 time frames. time frames each. The normalised MSE was then compared with those of the SRCs protocol and BB-Aware protocol. The result is shown in Figure 9. It is seen that the errors of all the methods decrease with an increase in the length of trial, which is to be expected because a longer trial provides more information about the number of active nodes than a short trial, as there are less collisions. The NN performs better than SRCs and BB-Aware consistently. Hence, for achieving the same normalized MSE, a NN can make use of a shorter trial than the SRCs and BB-Aware protocols. Conclusion: An increase in the length of the BB trial causes the errors of NN, SRCs and BB-Aware to decrease, with NN outperforming SRCs and BB-Aware for every length. Experiment 2 : Changing the number of jumps k To study how the methods perform when the system evolves faster or slower, the DTMC according to which the number of active nodes in a time frame evolves (see Section VI-A) is changed by changing the number of jumps taken in one time 11 Figure 11: The figure shows the MSE achieved by different algorithms versus the number of jumps with lBB = 100, α = 0.1, num lof = 3, llof = 8, a teacher dataset of 104 time frames and a student dataset of 104 time frames. The same student model trained on 5 jumps is used for the estimation for every value of the number of jumps. frame. Specifically, for a DTMC with transition probability matrix P k , by changing the number of jumps k, one can model a faster or slower changing time series. For each situation, a different student network is trained and the performance is evaluated by averaging over 20 runs. Figure 10 shows that the normalized MSE achieved by the NN decreases with an increase in the number of jumps. For the same number of time frames that the student is trained on, a DTMC with higher k offers more variation, and thus the NN is trained better and its achieved error decreases. Conclusion: As the NN model is trained on more ‘adverse’ scenarios when the number of jumps in the DTMC is higher (more outliers), the error decreases with an increase in the number of jumps. Experiment 3 : Testing the same NN model with fast or slow time series If a single trained model is evaluated with different DTMCs (different values of k), the error does not vary much, as seen in Figure 11. Note that in faster varying systems (higher k), the estimate from the previous time frame, which is used as the rough estimate for the BB trial in the current time frame, is more unreliable. The fact that despite this, the error achieved by the NN does not increase much in k suggests that the student network has learned to map the results of the current trial to the number of active nodes well, rather than relying heavily on the estimate of the previous time frame. BB-Aware performs worse under a faster time series (higher k), which is expected since it uses its estimate from the previous time frame as the rough estimate for the BB trial of the current time frame and the correlation between the true values of the number of active nodes in the previous time frame and in the current time frame decreases as k increases. Figure 11 also shows that for all values of k, NN significantly outperforms BB-Aware and SRCs. Conclusion: The NN model learns the dependence between the vt vector and the target n truth t , and does not simply repeat Figure 12: The figure shows the variation of the test loss with the mixing ratio α for a student network with lBB = 100, k = 5, a teacher dataset of 104 time frames and a student dataset of 104 time frames. nˆ Stu t−1 ; also, for all values of k, it significantly outperforms BBAware and SRCs. Experiment 4 : Varying the mixing ratio α Next, we study the dependence of the test loss on the mixing ratio α (see (10)). It can be seen from Figure 12 that the test loss decreases when α is increased up to a certain point, then increases again as α approaches 1. Figure 12 shows that distillation offers a significant benefit over blind training the student (α = 1). Also, the figure shows that the best result (lowest test loss) is achieved for a value of α around 0.5. Conclusion: Distillation offers a significant benefit over regular NN model training. C. Heterogeneous Network Figure 13 shows the training curves for offline training of a teacher model. It is seen that the test loss and training loss both decrease and settle quickly, indicating a relatively simple function to model. In contrast, Figure 14 shows the training curves for offline training of the corresponding student model. As the student model is larger than the teacher model, it is slower to train. The function mapping the student input to the target is also significantly complex. This leads to the model overfitting to the training data, and the test loss remains roughly constant, while the training loss decreases. After the training is complete, the trained student model is deployed in an online scenario. Figure 15 shows the performances of the student model, T-SRCs and T-BB-Aware methods versus the time frame number for 1000 time frames. It can be seen that the NN model significantly outperforms T-SRCs as well as T-BB-Aware. Experiment 1 : Changing the length of the trial The length of the trial (l3−SS−BB) is changed, and a different student model is trained on the generated data for each value of l3−SS−BB. The other two methods, viz., T-SRCs and T-BB-Aware, are also configured to use the same total number of time slots per time frame to produce estimates as the NN method. As expected, in Figure 16, the error decreases for the NN, T-SRCs, and T-BB-Aware methods as 12 Figure 13: The figure shows the training of the teacher in the heterogeneous network case with l3−SS−BB = 100, T = 3, k = 5, and a teacher dataset of 104 time frames. Figure 14: The figure shows the training of the student in the heterogeneous network case, with l3−SS−BB = 100, T = 3, α = 0.1, k = 5, a teacher dataset of 104 time frames and a student dataset of 104 time frames. Figure 15: The figure shows the normalized mean squared errors (MSE) achieved by the NN, T-SRCs, and T-BB-Aware methods, with l3−SS−BB = 100, T = 3, α = 0.1, k = 5, num lof = 3, llof = 8, a teacher dataset of 104 time frames and a student dataset of 104 time frames, and an evaluation run of 2000 time frames. Figure 16: The figure shows the average MSE achieved by different algorithms versus the number of time slots taken by each algorithm in a time frame, with T = 3, α = 0.1, k = 5, num lof = 3, llof = 8, a teacher dataset of 2 × 104 time frames and a student dataset of 2 × 104 time frames. the number of time slots increases; this is because the number of collisions decreases under each method. The NN method significantly outperforms both the T-SRCs and T-BB-Aware methods, which indicates that to achieve the same average error, the NN method can deliver with a shorter trial than both T-SRCs and T-BB-Aware. T-BB-Aware outperforms T-SRCs, as it uses a longer trial and uses information (estimates of numbers of active nodes of different types) from the previous trial. Conclusion: The NN model requires a far shorter trial than T-SRCs and T-BB-Aware to offer a specified average MSE. Experiment 2 : Changing the number of types of nodes Keeping the total number of nodes across all types present in the network the same (equal to 192), the number of types of nodes (T) is now varied. The maximum number of active nodes of each type in a time frame is taken to be ⌊192/T⌋. The errors achieved by different methods are shown in Figure 17. It is interesting to note that the NN method consistently gives low average MSE, even though the complexity of the mapping problem increases with T. Also, the figure shows that the NN method significantly outperforms both the T-SRCs and T-BBAware methods. Conclusion: The NN method can adapt well to a higher number of types of nodes (T) while achieving significantly lower error than T-SRCs and T-BB-Aware for all values of T. VII. CONCLUSIONS AND FUTURE WORK We have proposed a novel methodology for node cardinality estimation in homogeneous as well as heterogeneous wireless networks, which uses the PFD technique and works using a neural network with a teacher-student model. Using extensive simulations, we have shown that the neural networks trained using PFD significantly outperform state-of-the-art node cardinality estimation algorithms. In particular, for a fixed number of time slots per time frame, the proposed PFD based algorithms achieve much lower average normalised 13 Figure 17: The figure shows the average MSE achieved by different algorithms versus the number of types of nodes (T) with l3−SS−BB = 75, α = 0.1, k = 5, num lof = 3, llof = 8, a teacher dataset of 2 × 104 time frames and a student dataset of 2 × 104 time frames. MSE than SRCs and T-SRCs. Moreover, the proposed PFD based algorithms also outperform the SRCs based BB-Aware and T-BB-Aware methods, which use information from the previous time frame and hence have longer BB trials, in homogeneous and heterogeneous wireless networks, respectively. Our work demonstrates that PFD is a promising approach for effectively solving the problem of node cardinality estimation in wireless networks. In this paper, we have assumed that the base station is stationary. A direction for future research is to extend the techniques proposed in this paper to the case where a mobile base station moves around, making multiple stops, for node cardinality estimation in a large region in which a homogeneous or heterogeneous wireless network is deployed.",
		"summary": "The Internet of Things (IoT) is emerging as a critical technology to connect resource-constrained devices such as sensors and actuators as well as appliances to the Internet. In this paper, we propose a novel methodology for node cardinality estimation in wireless networks such as the IoT and RadioFrequency IDentification (RFID) systems, which uses the privileged feature distillation (PFD) technique and works using a neural network with a teacher-student model. The teacher is trained using both privileged and regular features, and the student is trained with predictions from the teacher and regular features. We propose node cardinality estimation algorithms based on the PFD technique for homogeneous as well as heterogeneous wireless networks. We show via extensive simulations that the proposed PFD based algorithms for homogeneous as well as heterogeneous networks achieve much lower mean squared errors in the computed node cardinality estimates than state-of-the-art protocols proposed in prior work, while taking the same number of time slots for executing the node cardinality estimation process as the latter protocols. ",
		"id": "UUID28"
	},
	{
		"document": "INTRODUCTION A Wireless Sensor Network (WSN) consists of small devices which are in general called as nodes. These nodes are embedded with sensor devices, actuator devices, a computational unit for processing the sensor data, and optionally communication unit for communicating the processed data to remote locations. The WSNs finds its applications in environmental monitoring. Researchers found their interest in WSNs with the advent and rapid advancement in technological aspects of wireless technology and embedded systems. Various components in WSN nodes perform crucial tasks by making nodes able to communicate with each other to transmit data obtained by their sensors. Communication may take place internally (i.e., between nodes) or externally (i.e., between the node and a centralized system). The various communication modes in WSNs had lead to the By ABSTRACT Internet of Things (IoT) consists of a wide variety of devices with limited power sources. Due to the adhered reason, energy consumption is considered as one of the major challenges in the IoT environment. In this research article, an attempt is made to optimize the existing Routing Protocol (RPL) towards a green technology. It focuses on finding the most significant parameter in the RPL using Taguchi Design of Experiments. It emphasizes the effects of five input factors, such as Network Size, Mobility Speed, DIO_DOUBLING, DIO_MIN_INTERVAL, and Redundancy Constant on only one output parameter Power Consumption. The findings show that DIO_MIN_INTERVAL is the leading factor that has a significant effect on the power consumption in RPL. After determining the most significant factor that affects the power consumption, measures can be taken to optimize the performance of RPL by applying some optimization techniques. COOJA simulator is used to carry out the simulations required for this research article. Keywords: IoT, RPL, Design of Experiments (DoE), Taguchi DoE, Cooja Simulator. CHANDRA SEKHAR SANABOINA * * Research Scholar, Jawaharlal Nehru Technological University Kakinada, Andhra Pradesh, India. ** Professor, Department of Computer Science and Systems Engineering, Andhra University, Visakhapatnam, Andhra Pradesh, India. IDENTIFICATION OF MOST SIGNIFICANT PARAMETER FOR OPTIMIZING THE PERFORMANCE OF RPL ROUTING PROTOCOL IN IoT USING TAGUCHI DESIGN OF EXPERIMENTS Date Received: 28/09/2018 Date Revised: 15/10/2018 Date Accepted: 21/11/2018 PALLAMSETTY SANABOINA ** RESEARCH PAPERS i-manager’s Journal on Software Engineering, Vol. 13 lNo. 1 lJuly - September 2018 31 experiments was first proposed by R. A. Fisher. As the name suggests, the full factorial design of experiments identifies all the possible combinations for the given set of factors and hence results in a very large set of experiments. In fact, all the combinations of the experiments need to be conducted. As an example, consider industrial experiments where almost every factor is considered to be important. In such cases, the full factorial design of experiments results in a large number of experiments which incur a lot of experimental costs. To put a check on the experimental costs, proposals were made to reduce the number of experiments to a practical level but still, give efficient and accurate results. This new proposal with a limited number of experiments is branded as the partial fraction design of experiments. The drawback with this proposal is that it does not have general guidelines for its application. It is an informal way of conducting experiments and no one can guarantee and authenticate the results obtained from this method. The next proposal for conducting the experiments is based on a special set of general design guidelines which was proposed by Taguchi. These guidelines help the researchers to conduct experiments covering a wide area of applications. Taguchi DoE was developed in the 1960s in Japan andwasscatteredtothewestduringthe1980s.This Taguchi style has its own advantages and disadvantages (Taguchi, 1986; Dehnad, 1989). Orthogonal arrays are a special set of arrays used in this method, which specifies the conduction of a minimal number of experiments, but can give full fledged information of all the factors that affect the parameter under consideration. The DoE using the orthogonal array is, in most cases, efficient when compared to many other statistical designs. The minimum number of experiments that are required to conduct the Taguchi method can be calculated based on the degrees of freedom approach. This article mainly focuses on finding the most significant parameter in RPL for the optimization of power consumption. 1. Routing Protocol for Low-Power and Lossy Networks (RPL) The ROLL (Routing Over Low Power and Lossy Network) is a working group created by Internet Engineering Task Force (IETF) for the purpose of analyzing the routing requirements Unit through the wireless communication channels among the nodes. Finally, these data are sent to the Gateway. The inception of WSNs research took place in the 1980s, but from 2001 onwards the use of WSNs was extensively found in the industries. The reason for this might be due to the advent of miniature technologies like System on Chip (SoC) and Very Large Scale Integrated Circuits (VLSI), which reduced the size of sensors, processors, and other components to an unbelievably small size. IEEE organization has defined a standard for this fact. The IEEE 802.15.4 covers low data rate wireless personal area networks. Based on this standard, ZigBee Alliance has published the ZigBee standard that can be used in WSNs. The idea of the Internet of Things (IoT) was developed in parallel to WSNs. The term internet of things was coined by Ashton (2009). It refers to the process of giving a unique address to each and every object (Things) available in the real world thus making them available over the internet. These things can vary from large entities like airplanes, Industrial plants, machines, cars to small entities like spectacles, pens, etc., or even they can be the body parts of human beings, animals, and plants. Wireless communication technologies play a major role in communication between things. Even though not strictly confined to this wireless technology it can also extend to wired communications. According to Cisco (Evans, 2011), 50 billion things will be connected to the Internet in 2020, thus overshadowing the data generated by humans. This is limited by the birth rate in 2020. In this study, for optimizing the existing Routing Protocol (RPL) towards a green technology and finding the most significant parameter in RPL, Taguchi approach was used. Taguchi Design of Experiments (DoE) Design of Experiments (DoE) is a multi-purpose technique (Box et al., 2005). Its usage is not limited to physical experiments, but can be applied to simulation experiments (Law & Kelton, 2000), to the investigation of calculated results of complex analytical expressions whose parameters are methodically varied or to other decision problems, where the effects of several factors are examined. DoE is also known as the factorial design of RESEARCH PAPERS 32 i-manager’s Journal on Software Engineering, Vol. 13 lNo. 1 lJuly - September 2018 Imin, Imax and Redundancy Constant (K) (Winter et al., 2012). 1.1.1 Imin Parameter It is the minimum interval of time between two successful DIOs. In general, DIO control messages are periodically transmitted so that optimal use of resources can be achieved. The periodical transmission also reduces the redundant control traffic. An algorithm keeps control of the transmission of the DIO messages which is named as a Trickle Timer algorithm. It has minimum and maximum values, which are dictated by Imin and Imax parameters, respectively. The trickle timer starts at the lowest possible value Imin and gets doubled each time a DIO message is successfully transmitted until it reaches a maximum possible value of Imax. Calculation of Imin is given by the formula. Imin = 2 ^ RPL_DIO_INTERVAL_MIN where RPL_DIO_INTERVAL_MIN is an RPL parameter in Contiki Operating System. 1.1.2 Imax Parameter This parameter is used to limit the number of times the Imin can be doubled. The value of Imax is determined by the RPL parameter DIO Interval Doublings (In Contiki: RPL_DIO_INTERVAL_DOUBLINGS) and computed as:  Imax = Imin * 2 ^ RPL_DIO_INTERVAL_DOUBLINGS 1.1.3 Redundancy Constant The average transmission probability of a node in the network depends on the number of neighbors and the redundancy constant. The usage of a fixed redundancy constant in the network causes an unbalanced transmission load and may cause early depletion of energy sources of nodes with fewer neighbors. Increasing the redundancy constant increases the transmission probabilities in the network (Coladon et al., 2015). Redundancy constant (k) is a natural number greater than 0 and is used to suppress the DIO transmission (Hazrat, 2012). 1.1.4 DIO Interval Minimum This parameter controls the rate of DIO transmission and therefore crucial for control traffic overhead, energy consumption, convergence time, etc. The more quickly of applications including industrial automation, home and building automation, and smart grid. The main objective of ROLL working group was to design and develop the routing solutions for IP-based Low power and Lossy Networks (LLN) that have the support of a variety of link layers. An LLN is made up of embedded devices that have limited memory, low energy (battery power), and low processing capability. RPL is a proactive routing protocol based on IPv6 distance vector protocol. Construction of DODAG is the first step in the routing process and routing the data packets through the DODAG is the second step. The name DODAG suggests its inherent property that all edges are oriented towards the destination and no cycles exist. Every node in the DODAG is designated with a rank, which specifies the relative position of that particular node with respect to the DODAG root node. It is obvious that the rank of the node strictly increases from the root towards the leaf nodes. The rank is computed depending on the DODAG's Objective Function (OF): hop counts, link metrics (ETX, i.e. the expected number of transmissions required to successfully transmit and acknowledge a packet on the link or LQI, i.e. the Link Quality Indicator) or other constraints. To build and maintain its logical topology (route, parents, neighbors table), RPL uses IPv6 control messages (Tsvetkov & Klein, 2011). 1.1 Trickle Algorithm Power consumption is a major trade in resourceconstrained environments. As per the research findings, the major source of energy consumption in RPL is due to the transmission of the control packets. Careful transmission of the control packets will lead to green technology (i.e., less power consumption). There exists an algorithm that controls/limits the transmission of a number of control packets. The time interval between two consecutive Data Input Output (DIO) messages is considered to be the DIO minimum interval. This DIO minimum interval doubles after each successful transmission of the DIO message and is limited by a maximum value, which is determined by DIO interval doublings. Three parameters that are configurable and has a major influence on the performance of the RPL are RESEARCH PAPERS i-manager’s Journal on Software Engineering, Vol. 13 lNo. 1 lJuly - September 2018 33 designing a set of guidelines for conducting the experiments was named after him and is popularly known as Taguchi Design of Experiments. Two vital tools are utilized in Taguchi design: Orthogonal Arrays (OAs) and Signal-toNoise Ratio (SNR). OAs are a special type of arrays which give a combination of the factors with a limited number of experiments, but could confer full information that affects the performance parameter (Mohamed et al., 2010). The bottom line of OAs lies in choosing various levels of combinations of the input factors for each experiment. Degrees of freedom approach is used to find the optimal minimum number of experiments that need to conduct the Taguchi DoE. For any OA, the degrees of freedom should be greater than or equal to the number of factors under consideration (i.e., input parameters) (Mohamed et al., 2008). (1) The variation which affects the change in performance of RPL is computed through Signal-to-Noise Ratio (SNR). The SNR is used to measure the performance metrics as well as the significant parameters through Analysis of Variance (ANOVA). Three classes of the performance metric in the analysis of SNR are employed: larger-the-better, smallerthe-better, and the nominal-the-best. 2.1.1 Smaller-The-Better In this research article, the smaller-the-better metric is chosen for performance evaluation of the chosen parameters of RPL. This metric is used to analyze the power consumptions (Roy, 2001). The SNR is computed as: (2) where r is the number of simulation repetitions under the same design point, p is the power, and y is the response value. 2.1.2 Larger-The-Better Larger-the-better metric is used for analyzing the strength, efficiency, S/N ratio, and throughput of the routing protocols. (3) the DIOs are transmitted the more quickly the network gets converted, but at the expense of Control Traffic overhead and more energy consumption. This parameter is influential on the performance of the whole protocol performance. A careful tweaking of this parameter is necessary for improved performance keeping in view the different application areas of WSN and environmental conditions. 1.1.5 DIO Doublings Maintaining the steady network conditions and low traffic is a crucial part in RPL. Hence, the DIO min interval is doubled after each successful DIO transmission and the parameter is defined as DIO doubling. Its low value can cause the control traffic to flow even if not needed. Consequently, it is essential to configure this parameter precisely. 1.1.6 Network Size The number of nodes in a network defines the network density. Network size (N/W) is one of the factors that affect the performance of the RPL routing protocol. In this study, different network densities are evaluated to discover the importance of nodes and how they can influence RPL behavior (Kumar et al., 2011). 1.1.7 Mobility Speed There are many aspects of routing in IoT which are harder to deal with when nodes are mobile because the issues like energy efficiency, PDR, and connectivity become more difficult to optimize. Solutions for mobility usually rely on updating routing information frequently, which is a critical factor for IoT with constrained resources. Therefore the evaluation of RPL with mobile nodes with different specifications provide good suggestions and improvements for further enhancements of the routing protocol (Hazrat, 2012). 2. Methodology The methodology used in this research article for performance evaluation of RPL is explained in this section. 2.1 Concepts of Taguchi DoE This is a new method of conducting experiments which are based on well-defined guidelines. Taguchi's effort of RESEARCH PAPERS 34 i-manager’s Journal on Software Engineering, Vol. 13 lNo. 1 lJuly - September 2018 RESEARCH PAPERS experiments because of its highly unpredictable behavior. The process parameters are directly proportional to the number of experiments (i.e., the number of experiments to be conducted increases with the increase in the number of input factors/parameters). The Taguchi method finds a solution to this problem. It makes use of a unique type of arrays called orthogonal arrays which studies the entire parameter space with a limited number of experiments. The steps required for designing a Taguchi experiment is illustrated in Figure 1. 2.2.1 Selection of the Independent Factors Identifying the factors that most influence the output parameter has significant importance. This knowledge is essential prior to conducting the experiment. The input to the experiment is generally obtained from the project which benefits in compiling a widespread list of factors. 2.2.3 Deciding the Number of Levels After identifying the independent factors, the user has to decide upon the number of levels (number of values) for each factor. The choice of different levels is dependent on the effect of the output parameter due to different levels of the input parameters. 2.2.3 Selection of Orthogonal Array The degrees of freedom plays an important role in 2.1.3 Nominal-The-Best In this metric, the signal value is fixed (nominal value), and the variance around this value can be considered the result of Signal-to-Noise factors. 2  N = 10 log (a) (4) p 10 where ais the mean value. 2.1.4 ANOVA The Analysis of Variance (ANOVA) is used to determine the relative effect of all factors on the signal metric (i.e, power consumption) and to determine which factor has the highest effect. Parameters in ANOVA are calculated by the following equations. The Sum of Squares (SS ) from SNR (Voice et al., T 2007; Sahithi & Setty, 2018) is given as follows: (5) th where N is the Signal to Noise Ratio (SNR) of the i i experiment conducted. The sum of squared deviations due to each factor (SS) is j calculated by using the following formula: (6) where L is the number of levels and N is the average SNR of ji th th a j factor at j level. Also, the sum of squares of error (SSc) is given by, (7) where q represents the number of factors, SS represents the j sum of squared deviations for each factor. The contribution of each factor p in percentage is given by the formula: j  p = (SS / SS ) * 100 (8) j J T Where p value gives the significance level for each factor. j The f-test can also be used to determine which factor has the most significant effect on the performance metric. The large F-ratio indicates the strong effect of the factor. The Fth value for j can be evaluated as:  F = (SS / Df) / (SS / Df ) (9) j j c c 2.2 Designing Taguchi Technique It becomes impossible to utilize the conventional design of Figure 1. Taguchi's Methodology i-manager’s Journal on Software Engineering, Vol. 13 lNo. 1 lJuly - September 2018 35 RESEARCH PAPERS performance of the process). Analysis of Variance (ANOVA) can be conducted to decide upon which independent factor has highest percentage contribution. 2.2.7 Interference It is clear from the above experimental analysis that smaller the value of the sum of the square of an independent factor the smaller will be its influence of the parameter of consideration and more the value of the sum of the square of an independent factor the more will be its influence on the parameter under consideration. It is good to calculate the ratio of the individual sum of the square of a factor to the total sum of squares of all the factors. This ratio emphasizes the percentage contribution of the independent factors on the parameter under consideration. 3. Simulation Environment In this section, the simulation environment which was used for the performance evaluation of RPL is explained. 3.1 Contiki Operating System The Operating System that is best suited for the wireless sensor network operating system is Contiki. It has the inbuilt kernel, libraries, the program loader, and various set of processes in the kernel as well as for the user (Dunkels et al., 2004). This operating system is most widely used in the networked embedded systems and smart environment, which involves a large number of smart objects. It provides a lot of means that assist in writing the applications for smart objects. Libraries provide a means which assists in linked list manipulation, memory allocation, and communication with the external world. Perhaps, Contiki is the first embedded Operating System that provided IP communication. ‘C’ is the programming language that is used to develop the Contiki operating system. All the applications in Contiki were also developed using C programming language and hence it supports portability to different architectures like Texas Instruments MSP430. Processes in Contiki are implemented as event handlers and hence Contiki can be treated as an event-driven operating system. The main components of the Contiki operating systems are core and loaded programs. ·Core: It consists of Contiki Kernel, the language rundeciding the orthogonal array. A minimum number of experiments that are to be conducted depends on the selection of orthogonal array. It is important to note that the degrees of freedom available should be less than the minimum number of experiments that must be run. 2.2.4 Allocating the Independent Factors to every Segment Independent factors are to be assigned to the vertical column and the order of assignment is very crucial. The factors are to be assigned at right columns in case of mixed level factors (i.e., different factors having different levels). Deciding upon the actual level of values for each design factor is essential before conducting the experiment. The percentage contribution and the importance of the independent factors change based on the number of values (levels) assigned to them. Setting the proper values to various independent factors is the designer's liability. 2.2.5 Conducting the Experiments Various experiments suggested by the orthogonal array as per the level combinations are to be performed. It is mandatory that all the experiments suggested by the OA should be conducted. The dummy factor and interaction columns are needed only at the time of analyzing the data and are to be omitted during the conduction of experiments. At the end of each experiment, the parameter under consideration is to be noted down. 2.2.6 Analysis of the Data Segregation of individual effect of independent factors is essential as each experiment is the combination of various factors with different levels. Segregation is the process of summing up the values of the parameter under consideration for different levels. After finding the mean value of each level of an independent factor, the sum of the square of deviation of each of the mean value from the total mean value is to be calculated. Whether the parameter under considerations is sensitive to the change in level setting is decided by the sum of square deviation. The zero value or close to zero value of the sum of square deviation indicates that the design factors are insignificant (i.e., factors are not influencing the 36 i-manager’s Journal on Software Engineering, Vol. 13 lNo. 1 lJuly - September 2018 RESEARCH PAPERS 5 parameters and number of levels, L27 equals to 3 factors (Level ) = 243 experiments have to be conducted if it is a Full-Factorial method. In a fixed level (3 level design), two orthogonal arrays L and 9 L are available. Selecting L needs only 9 experiments to 27 9 be conducted which may not give sufficient results for correct analysis. Therefore, L OA is selected in this 27 research article and 27 experiments were conducted using Cooja simulator with different levels of the factors. Table 2 depicts the combination of different experiments and each combination of the experiment can be termed as Design-Point and each Design-Point corresponds to a simulation scenario. Table 3 gives an idea of the simulation environment used to conduct experiments. This research emphasizes a small simulation area of size 100 m 100 m. Simulations (DesignPoint) are carried out for a period of 600000 ms (600 s). The experiments were repeated thrice to check whether the simulation parameters selected (i.e, simulation area 100 m 100 m and simulation time 600000 ms) were optimum or not. The three set of results for various design points generated the same results thus concluding that the selected simulation parameters are apt and hence steady state has been achieved. As each simulation scenario is executed, the CPU Power, LPM Power, Listen Power, Transmit Power, and Overall Power is computed as output performance metrics. But for evaluation purpose of this research article, only overall average power is considered because overall average power is treated as: Overall Average Power @CPU + LPM + Radio Listen + Radio  Transmit Power 4. Results and Data Analysis Figure 2 depicts the main effects plot of S/N ratio for power. ´ ´ time, the program loader, and a communication stack with device drivers for the communication. ·Loaded Programs: It consists of binary image files of sensor motes which are used in Cooja simulator (Dunkels et al., 2004). 3.2 Cooja Simulator An inbuilt simulator that is available in Contiki sensor network operating system is Cooja. It is based on Java (Osterlind et al., 2006). The simulator is entirely developed using Java, but some sensor node software is written in ‘C’ programming language. One of the outstanding features of Cooja is that it allows simultaneous simulations at Network Level, Operating System Level, and Machine Code Instruction Level (Osterlind et al., 2006). Cooja has the capability of running Contiki programs compiled for native CPU or MSP430 emulator. Plugins like Simulation, Visualizer, Radio Logger, and Timeline are used to implement interactions with the simulated nodes. All the simulation data is stored in an XML file, which has an extension of CSC (Cooja Simulation Configuration). It consists of all the information related to nodes, node position, random seed, radio medium used, Simulation environment, etc., The Cooja simulator has the full control of the simulated systems because of the approach if followed during simulation. It makes use of functions for analyses and control of the Contiki system. As an example, consider a situation where the simulator informs the Contiki system to handle an event or fetches the entire Contiki system memory for analysis. The drawbacks of the Cooja simulator is the use of JNI. It leads to some of the side effects. Another important drawback is the dependency on external tools like compilers, linkers, and their run-time arguments. 3.3 Experimental Design and Simulation In this article, the performance of RPL is evaluated based on five factors: Network Size, Mobility Speed, DIO_MIN_INTERVAL, DIO_DOUBLING, and Redundancy Constant as described in Table 1. As listed, each parameter is examined at three different levels (i) Level-1, (ii) Level-2, and (iii) Level-3. Based on the number of Label Factor Level 1 Level 2 Level 3 A Network Size (NS) 20 Nodes 30 Nodes 40 Nodes B Mobility Speed (MS) 5 m/s 15 m/s 25 m/s C DIO_MIN_INTERVAL (MIN) 8 12 16 D DIO_DOUBLING (DOUBLING) 4 8 12 E REDUNDANCY_ CONSTANT (RC) 6 10 14 Table 1. Experimental Parameters and their Levels i-manager’s Journal on Software Engineering, Vol. 13 lNo. 1 lJuly - September 2018 37 RESEARCH PAPERS as follows: ·Network Size: The power consumption for an IoT network is high in case of 20 nodes and there is a drastic decrease in power consumption at a network size of 30 and a little increase in the power as the network size increases to 40 nodes. ·Power Consumption: The decrease in power consumption as the number of nodes increases in a network is due to the decrease in the number of DIO messages exchanges between nodes. As the number of nodes increases within a given area, it becomes easy to form a DODAG with less number of DIO exchanges. This happens up to a certain limit thereafter there will be congestion in the network which leads to the number of DIO messages to collide with each other. This results in retransmission of more number of DIO messages, which in turn results in an increase in power consumption. ·Mobility Speed: The mobility speed has the same effect as that of the network size. MIN value is inversely proportional to the power consumption (i.e, less MIN value higher the power consumption and more mobility speed lesser the power consumption). The higher power consumption at fewer mobility speeds is due to the exchange of more DIO messages for forming the DODAG. As the mobility speed increases the DIO message exchanges will be reduced and hence the lesser power consumption. ·DIO_MIN_INTERVAL: Lower the min-interval values more will be the power consumption. As the min values are In the plots given, Y-axis indicates the dependent variables (Mean SNR in this case) and X-axis indicates Independent variables (NS, MS, MIN, DOUBLING, and RC). The main effects plot gives the effect of a single independent variable on a dependent variable ignoring all other independent variables. The main effects plot is an easy decision tool that helps in understanding the data means. The analysis of the main effects plot for power is described Design Point Level of Factors A B C D E 1 20 5 8 4 6 2 20 5 8 4 10 3 20 5 8 4 14 4 20 15 12 8 6 5 20 15 12 8 10 6 20 15 12 8 14 7 20 25 16 12 6 8 20 25 16 12 10 9 20 25 16 12 14 10 30 5 12 12 6 11 30 5 12 12 10 12 30 5 12 12 14 13 30 15 16 4 6 14 30 15 16 4 10 15 30 15 16 4 14 16 30 25 8 8 6 17 30 25 8 8 10 18 30 25 8 8 14 19 40 5 16 8 6 20 40 5 16 8 10 21 40 5 16 8 14 22 40 15 8 12 6 23 40 15 8 12 10 24 40 15 8 12 14 25 40 25 12 4 6 26 40 25 12 4 10 27 40 25 12 4 14 Table 2. Experimental Layout using L27 Orthogonal Array Routing Protocol RPL Simulation Time 600000 ms Simulation Area 100 m ´ 100 m Network Size 20, 30, 40 Nodes Mobility Speed 5, 15, 25 m/s MIN 8, 12, 16 DOUBLING 4, 8, 12 RC 6, 10, 14 Table 3. Simulation Parameters Figure 2. Main Effects Plot for SN ratio for Power 38 i-manager’s Journal on Software Engineering, Vol. 13 lNo. 1 lJuly - September 2018 RESEARCH PAPERS observed that the plots are almost parallel to each other and hence RC does not have any interaction with NS. Same is the case with RC and the other independent variables (MS, MIN, DOUBLING). As the plots are almost parallel, there are no interactions with RC and MS, MIN, DOUBLING. As another example consider Row 1 and Column 3 (Interaction between levels of NS and various levels of MIN). It is clear from the graph that at low values of MIN, the power consumption is more and at higher values of MIN, the power consumption is less. From the same graph, observe that the plots are almost parallel for the values 30 and 40 of NS. Hence there are no interactions between them. Continuing with the discussion consider Row 3 and Column 2 (interaction of different levels of MIN and various levels of MS). The plots are parallel and hence no interactions. Table 4 gives the results for various design points (simulation scenarios) for L27 OA. Table 5 depicts the ANOVA table/Welch's test table where: ·DF: The amount of interaction present in the data can be treated as total Degrees of Freedom (DF). It shows how much information that a particular factor is used. ·Seq SS: Sequential sums of squares are measures of variation for different components of the model. ·Ads MS: Adjusted mean square measures how much variation a term explains regardless of the order they were entered. ·F-Values: The F-Value is the test statistic used to determine whether the term is associated with the response. A sufficiently large F-value indicates that the term is more significant. ·P-Value: It is the probability that measures the evidence against the NULL hypothesis. Lower probability provides stronger evidence against the NULL hypothesis. P-Value with the significance level (α=0.05) is used to find the significant factor. Case i: If P-Value £a: The difference between the sum of the means are statistically significant. Case ii: If P-Values > a: The differences between the means are not statistically significant. From Table 6 one can observe that factor C (MIN) is having increased there is a drastic decrease in the power consumption (i.e, at MIN=12). Further increase in the MIN value decreases the power consumption. ·DIO_DOUBLING: When doubling interval values increases the effect on power consumption decreases. In a similar way when doubling values decreases power consumption will be increased. ·Redundancy Constant: This is something difficult from the after-input parameter so far decreased. In this research article, it has a direct relationship with power consumption. From the above main effects plot, one can observe that Network Size, Mobility Speed, and DIO_MIN, DIO_DOUBLING are having an uneven relationship with power consumption (i.e, power consumption increases with a decrease in this parameter values and vice versa), where redundancy constant is having a direct relation with power consumption (i.e., power consumption increases with an increase in redundancy constant). Figure 3 depicts the interaction plot of power which is a graphical tool to understand the interactions. In simple terms, it depicts the effect of an independent variable on the level of another independent variable. The analysis of Figure 3 is as follows: The independent variables are said to have interaction if they are not parallel and do not have interaction if they are parallel to each other. For example, consider Row 1 and Column 5 in Figure 2: it gives the interaction of the independent variables (RC) on different levels (20, 30, and 40 nodes) of another independent variable NS. It is Figure 3. Interaction Plot for Power i-manager’s Journal on Software Engineering, Vol. 13 lNo. 1 lJuly - September 2018 39 RESEARCH PAPERS largest delta value is given rank 2, and so on. Hence from the table, the rows of MIN, DOUBLING, MS, NS, and RC are having ranges as 1, 2, 3, 4, and 5, respectively thus signifying the orderofsignificant factorwith respect topower. Conclusion In this research article, performance evaluation is done on RPL with five chosen parameters by using Taguchi Design of Experiments. Cooja simulator is used for the evaluation in this research article. From the results discussed above, the power consumption in RPL has different significant factors, such as Network Size, Mobility Speed, DIO-MIN-INTERVAL, DIODOUBLING, and Redundancy Constant out of which DIOMIN is considered to be the significant factor (i.e., DIO-MIN value has a significant effect on power consumption in RPL). Future Work One can focus on the application of optimization techniques such as Soft Computing/Hard Computing techniques on the significant factors identified in this research article and power consumption can be reduced thus paving a way to green technologies. In fact, an automated tool based on this can be developed, which means necessary changes in the value of DIO-MIN are automatically based on the input values, such as NS, MS, etc., for optimizing the RPL protocol.",
		"summary": "Internet of Things (IoT) consists of a wide variety of devices with limited power sources. Due to the adhered reason, energy consumption is considered as one of the major challenges in the IoT environment. In this research article, an attempt is made to optimize the existing Routing Protocol (RPL) towards a green technology. It focuses on finding the most significant parameter in the RPL using Taguchi Design of Experiments. It emphasizes the effects of five input factors, such as Network Size, Mobility Speed, DIO_DOUBLING, DIO_MIN_INTERVAL, and Redundancy Constant on only one output parameter Power Consumption. The findings show that DIO_MIN_INTERVAL is the leading factor that has a significant effect on the power consumption in RPL. After determining the most significant factor that affects the power consumption, measures can be taken to optimize the performance of RPL by applying some optimization techniques. COOJA simulator is used to carry out the simulations required for this research article. ",
		"id": "UUID29"
	},
	{
		"document": "I. INTRODUCTION During daily commutes, drivers assert themselves in running negotiations with other road users in order to reach their destinations quickly and safely. Right-of-way expectations inform these assertions between road users. Consider the passing lane shown in Fig. 1. Agent A2 (blue) initially follows behind agent A1 (red), and we may intuitively perceive A1 as the leader. If instead A2 overtakes A1, the scenario seems to imply a reversal of leadership, with A2 in front and A1 behind, as in the inset of Fig. 1. However, this intuition is vague and premature. If A2 tailgates A1 or otherwise behaves aggressively, A1 might speed up or yield to A2 out of caution. However, aggressive behavior does not necessarily indicate leadership, as A1 could also react to A2 tailgating by slowing down and relying on the knowledge that A2 will not risk a collision. Here, any simple intuition of the leadership dynamics falls short. Depending on each driver’s safety and comfort tolerances, either A1 or A2 may be the leader. Hence, deciphering leadership dynamics requires understanding common expectations, agent incentives, and other agents’ actions. Successfully doing so can improve autonomous intent and behavior prediction for motion planning, as demonstrated by [1]. We turn to optimal decision making and game theory for tools to analyze interactive scenarios. Stackelberg games [2], also known as leader-follower games, stand out because they model interactions with clear leadership hierarchies. In a Stackelberg game, a leader selects its strategy to influence the follower’s response. Each strategy in a Stackelberg solution satisfies leadership conditions that describe how the leader’s 1Aerospace Engineering and Engineering Mechanics, University of Texas at Austin {hamzah, dfk}@utexas.edu This work was supported by the National Science Foundation under Grant No. 2211548. Fig. 1. Agents A1 (red) and A2 (blue) initially proceed along the same lane of a two-way road at similar speeds. While A2 is behind A1, the SLF infers that A1 is the leader. During A2’s passing maneuver, the SLF captures the leadership probability shifting to A2. The dashed line in the inset indicates the probabilities at the current time. We display the current expected measurements generated by the measurement model h. The blue coloring indicates that most particles in the SLF believe A2 is the leader. behavior induces the follower to act. Additionally, solving Stackelberg games results in trajectories that we can use for model-predictive control. Using these attractive properties, we propose a leadership inference technique for multi-agent scenarios like that of Fig. 1. To this end, we first contribute Stackelberg Iterative Linear-Quadratic Games (SILQGames), an algorithm for solving general Stackelberg games, and we show that it converges for games with nonlinear dynamics and general costs. Second, we propose the Stackelberg Leadership Filter (SLF) to infer leadership over time in two-agent interactions based on observations of the interacting agents. We validate that it infers the correct Stackelberg leader in two-agent games and report results on simulations of driving scenarios. II. RELATED WORK Leadership Inference. Many prior works develop leadership inference techniques, particularly for robotic swarms and animal sociology. As an abstract concept, leadership is challenging to measure [3], [4]. Leadership models prespecify particular agent(s) as leaders that influence group motion. Swarm applications [5]–[7] often assume the Reynolds flocking model [8]. Animal sociology applications define leadership models based on principal component analysis [9] or stochastic inference [10] with hand-selected domainspecific features. By contrast, we explicitly frame these interactions in terms of optimal decision making and game theory and therefore utilize the Stackelberg leadership model. Defining a Stackelberg game requires prespecifying a leader and solving one produces equilibrium trajectories for each agent. Hence, by associating a particular leader with solution arXiv:2310.18171v1 [cs.MA] 27 Oct 2023 trajectories in a principled manner, the Stackelberg leadership model allows for modeling leadership over long-horizon interactions without hand-crafted heuristics. Stackelberg Games for Motion Planning. Recent advances [11]–[13] investigate Stackelberg models of leadership for interactive scenarios involving self-driving vehicles. In particular, Tian et al. [1] incorporate leadership as a latent variable by solving open-loop Stackelberg games and comparing expected leader and follower behaviors with observed agent behaviors. Our method generalizes this underlying approach to Stackelberg leadership by modeling a joint distribution over game state and leadership. We solve feedback Stackelberg games for richer access to leadership information. Solving Dynamic Games. Identifying computationally efficient game-solving techniques with theoretical guarantees of finding equilibria remains an open area of research. Most existing game-solving algorithms consider Nash games, which find equilibria for which each actor is unilaterally optimal given fixed opponent strategies. These algorithms [14]–[18] generally use Newton-based schemes based on iterative and dynamic programming algorithms that have been widespread for decades [19], [20]. We note two axes on which such approaches differ: first, these approaches solve either open-loop Nash games [14]–[17] or feedback Nash games [14], [18]. Second, these algorithms either reduce the game to a simpler problem [17] or directly solve the game [14]–[16], [18]. In particular, Fridovich-Keil et al. [18] introduce Iterative Linear-Quadratic Games (ILQGames), an iterative method that approximates solutions to nonlinear dynamic, nonquadratic cost feedback Nash games by repeatedly solving linear-quadratic (LQ) approximations until convergence. Convergence analysis of these methods is subtle, as described in depth by Laine et al. [21]. We utilize a similar approach as ILQGames to solve feedback Stackelberg games. III. PROBLEM FORMULATION Two agents, A1 and A2 (e.g., autonomous cars), operate in a shared space with state xt ∈ R n at each timestep t ∈ T ≡ {1, 2, . . . , T} with sampling period ∆t. Agent Ai has controls u (i) t ∈ R m(i) , and the state evolves according to xt+1 = ft  xt,u (1) t ,u (2) t  . (1) We denote the sequence of states as x1:T and the sequence of Ai’s controls as u (i) 1:T . We assume that ft is continuous and continuously differentiable in xt,u (1) t ,u (2) t . Ai’s objective, J (i)  x1:T ,u (1) 1:T ,u (2) 1:T  ≡ X T t=1 g (i) t  xt,u (1) t ,u (2) t  , (2) describes its preferences in a given scenario. We model the objective (2) as the sum of stage costs g (i) t , assumed to be twice differentiable in xt,u (1) t ,u (2) t . Each agent Ai minimizes its objective with respect to its controls u (i) 1:T . A. Background: Feedback Stackelberg Games Stackelberg games model leadership as a mismatch of information. Intuitively and without loss of generality, the leader A1 commits to a strategy and communicates it to the follower A2. Given this relationship, the leader carefully selects its strategy in order to influence the follower. Formally, a Stackelberg equilibrium {u (1∗) 1:T ,u (2∗) 1:T (u (1∗) 1:T )} is a tuple of optimal control trajectories for both agents. The function u (2∗) 1:T (u (1) 1:T ) highlights that A2’s optimal strategy depends on the leader’s (possibly non-optimal) chosen strategy. Using an abuse of notation, we omit the state argument of the objective J (i) , and define γ(u (i) t ) ≡ [u (i) 1:t−1 ,u (i) t ,u (i∗) t+1:T ]. We define the set of all optimal follower responses at time t, U (2∗) t  u (1) t  ⊂ R m(2) , as U (2∗) t  u (1) t  ≡ argmin u (2) t J (2)  γ  u (1) t  , γ  u (2) t  . (3) We assume |U (2∗) t (u (1∗) t )| = 1, i.e., that an optimal leader strategy results in a unique optimal follower response at each time t. Under this assumption, the set of control trajectories for all agents forms a feedback Stackelberg equilibrium if, at every time t ∈ T, the optimal trajectories satisfy J (1)  γ  u (1∗) t  , γ  u (2∗) t  = (4) min u (1) t max u (2) t ∈U (2∗) t  u (1) t  J (1)  γ  u (1) t  , γ  u (2) t  . At equilibrium, the leader uses Stackelberg condition (4) to guide the follower towards its least bad option for the leader. Stackelberg games are generally non-cooperative, meaning that agents do not coordinate but plan based on observations of the game state. Agents in open-loop games observe only the initial game state, whereas in feedback games, agents adjust their control inputs after observing the state at each time step, producing complex, temporally-nested game constraints (3) and (4). LQ Stackelberg games have analytic solutions given strictly convex costs [22, Eq. 7.14-15]. We denote S i T (xt) as the T-horizon Stackelberg game solved from state xt with leader Ai . For a more detailed treatment of Stackelberg equilibria and solving LQ Stackelberg games, refer to Bas¸ar and Olsder [22, Ch. 3, 7]. B. Stackelberg Leadership Filtering We seek to describe a filter that identifies a leadership belief for Ai based on observations. To this end, let us first define Ht ∈ {1, 2} to be a binary random variable (RV) indicating the leader at time t. Next, we state our assumptions about the observability of the game. We assume the state xt is observable via noisy measurement zt ∼ N (h(xt; Ht), Σt) with known covariance matrix Σt ≻ 0 and measurement model h. We also assume that control inputs u (i) t for each agent Ai are directly observable. Next, recall that each agent has an objective that describes its preferences. For this work, we assume all agent objectives {J (i)} are known a priori. In general settings, we note that techniques exist [23]–[25] to infer agent objectives from noisy observations, though further work may be required to confirm the computational tractability of simultaneously inferring leadership and objectives. Finally, we formally define the leadership belief for Ht as b(Ht) = p{Ht|z1:t}. IV. INFERRING LEADERSHIP We propose Stackelberg Iterative Linear-Quadratic Games (SILQGames), which iteratively solves nonlinear dynamic, general cost (non-LQ) Stackelberg games with continuous and differentiable dynamics and costs. We use SILQGames in the Stackelberg Leadership Filter (SLF, Fig. 2) as part of the Stackelberg leadership model. Our method infers the leading agent of a two-agent interaction from observations. A. Iteratively Solving Stackelberg Games At a high level, SILQGames (Alg. 1) iteratively solves LQ approximations of Stackelberg games (lines 4 to 8), updates the control trajectories using the solutions to these approximated games (line 9), and terminates if the updated trajectory satisfies a convergence condition (lines 10 to 12). Upon successful convergence, the resulting trajectory constitutes an approximate Stackelberg equilibrium. This type of approach also yields approximate equilibrium solutions in the Nash case, although establishing precise error bounds remains an open problem [21]. We expect a similar result for SILQGames, though it is beyond the scope of this work. Inputs. SILQGames accepts an initial state x1 and a leader Ai . It accepts a set of all agents’ nominal control trajectories {u (i),k=0 1:T }. We produce a nominal state trajectory x 0 1:T by applying the nominal controls from x1 (line 1). LQ Game Approximation. At each iteration k, we first linearize the dynamics (lines 4 and 5) and take second-order Taylor series approximations of the costs (lines 6 and 7) about the previous iteration’s state and control trajectories, x k−1 1:T ,u (1),k−1 1:T ,u (2),k−1 1:T : At = ∇xft, (5a) Q (i) t = ∇2 xxg (i) t , (5b) R ij t = ∇2 u(j)u(j) g (i) t , (5c) B (i) t = ∇u(i) ft, (5d) q (i) t = ∇xg (i) t , (5e) r ij t = ∇u(j) g (i) t . (5f) We define the state and control variables for our LQ game approximation as deviations from the previous state and control trajectories: δx k 1:T = x k 1:T − x k−1 1:T and δu (i),k 1:T = u (i),k 1:T − u (i),k−1 1:T . We then approximate the game as an LQ problem with linear dynamics and quadratic costs δx k t+1 ≈ Atδx k t + X i∈{1,2} B (i) t δu (i),k t , (6a) g (i) t (·, ·, ·) ≈ 1 2 h 2g (i) t  x k−1 t ,u (1),k−1 t ,u (2),k−1 t  + (6b)  Q (i) t δx k t + 2q (i) t ⊺ δx k t + X N j=1  R ij t δu (j),k t + 2r ij t ⊺ δu (j),k t i . We exclude mixed partials ∇xu(i) , ∇u(i)u(j) due to their rarity in cost structures of relevant applications, but they can be included if needed. In practice, Q (i) t and R ij t may not be positive definite. Recall that LQ Stackelberg games have unique global solutions given strictly convex costs. Thus, we enforce positive definiteness, and thus convexity, in the quadratic cost estimates by adding a scaled identity matrix νI to all Q (i) t and R ij t terms. This addition increases each eigenvalue by ν ∈ R+ [27, Ch. 3], so a sufficiently large choice of ν guarantees convexity. Finally, we solve the LQ game analytically (line 8) [22, Eq. 7.14-15]. Algorithm 1 Stackelberg Iterative Linear-Quadratic Games Input: leader Ai , initial state x1, nominal strategies n u (i),0 1:T o Output: converged strategies n u (i),k−1 1:T o 1: x 0 1:T ← applyGameDynamics  x1, n u (i),0 1:T o 2: αk ← α1 3: for iteration k = 1, 2, . . . , Miter do 4: F1:T ≡ {A, B(i)}t=1:T (5a, 5d) 5: ← linearizeDynamics  x k−1 1:T , n u (i),k−1 1:T o 6: G1:T ≡ {Q(i) , q(i) , Rij , rij}t=1:T (5b, 5c, 5e, 5f) 7: ← quadraticizeCosts  x k−1 1:T , n u (i),k−1 1:T o 8: P (i),k 1:T , p (i),k 1:T ← solveLQStackelberg({F1:T , G1:T }) 9: x k 1:T , n u (i),k 1:T o ← stepToward(P (i),k 1:T , p (i),k 1:T , αk) (7) 10: if ∥x k 1:T − x k−1 1:T ∥∞ ≤ τ then 11: return x k−1 1:T , n u (i),k−1 1:T o 12: end if 13: αk+1 ← max(αmin, βαk) 14: end for Strategy Update. After approximating the game as LQ and solving it, we update the control strategy (line 9). The analytic solution to the LQ game consists of gain and feedforward terms P (i),k 1:T , p (i),k 1:T which constitute an affine feedback control law that produces strategy δuˆ (i),k t = −P (i),k t δx k t − p (i),k t . Following standard procedures in Iterative Linear Quadratic Regulation (ILQR) [19], we define update rule u (i),k t = u (i),k−1 t − P (i),k t δx k t − αkp (i),k t , (7) where αk ∈ (0, 1] is an iteration-varying step size parameter. As αk approaches 0, the new iterate u (i),k t approaches the previous iterate u (i),k−1 t . Likewise, as αk approaches 1, we adjust our previous iterate by the full step δuˆ (i),k t . In single-agent settings, methods like ILQR commonly apply a line search for step size selection. However, this approach requires a detailed description of complex, temporally-nested feedback game constraints (3) and (4). Instead, SILQGames decays the step size (line 13) with configurable decay factor β ∈ (0, 1) and minimum step size αmin. Initial step size α1 = 1 unless otherwise specified (line 2). Convergence Criterion. Optimization algorithms commonly use first-order optimality conditions [27, Ch. 12] to test for convergence, and incorporating a line search guarantees monotone improvement in such a convergence metric. As with a line search, however, using first-order optimality conditions becomes unwieldy due to the feedback game constraints (3) and (4). In practice, we define a convergence criterion as a function of the current and next iterate’s states: Conv x k 1:T , x k−1 1:T  =    x k 1:T − x k−1 1:T     ∞ . (8 Fig. 2. Each particle in the Stackelberg leadership filter has context c k t = [˜x k t , Hk t ] ⊺, where continuous RV xt ∈ Rn describes the state and discrete RV Ht ∈ {1, 2} indicates the leader. 1. At t − 1, we have a prior distribution over the filter context. For Ht−1, the prior is Bernoulli distributed. 2. The continuous state transitions according to game dynamics ft−1. Leadership state evolves stochastically based on a two-state Markov chain. 3. We play a Stackelberg game from each particle’s previous state and extract the game state at the current time t as the expected measurement. 4. The algorithm uses a standard particle filter measurement update [26, Ch. 4]. Resampling eliminates unlikely particles and reweights the particle set towards those that are similar to the measurement. Finally, we marginalize over the continuous state and produce a probability of leadership. We compute x k 1:T based on the proposed controls resulting from update step (7). We say SILQGames converges if the metric value falls below a threshold τ . SILQGames stops after a maximum number of iterations Miter, irrespective of convergence. We expect SILQGames to converge, though we do not expect monotone decrease in the convergence criterion as a large step size may occasionally overshoot the Stackelberg equilibrium. Oscillations in the convergence metric can occur when step sizes are consistently too large and may indicate that αmin or β should be reduced. Please refer to our results in Section V-A for further details. Computational Complexity. The per-iteration computational complexity analysis of [18] holds almost identically for SILQGames. Since the number of agents is a constant (i.e., 2), the per-iteration complexity of SILQGames is O(n 3 ). The entire algorithm runs in O(kn3 ), where k ≤ Miter is the number of iterations to convergence. B. Leadership Filtering The Stackelberg Leadership Filter (SLF) estimates the likelihood that each agent is the leader of a two-agent interaction given noisy measurements z1:T . Let filter context ct = [xt, Ht] ⊺ consist of continuous game state xt and leader Ht. Following conventional Bayesian filtering practices and denoting all agent controls wt ={u (1) t ,u (2) t } for brevity, the SLF refines prior context belief b(ct−1) with update rule b(ct) ∝ p{zt|xt} Z ct−1 p {ct|ct−1, wt−1} b(ct−1)dct−1, (9) In (9), the context transition probability term p{ct|ct−1, wt−1} = p{xt, Ht|xt−1, Ht−1, wt−1} describes the likelihood of context ct given the previous context ct−1 and each agent’s controls. Furthermore, the measurement likelihood p{zt|xt} quantifies an expected measurement based on how well the new state xt matches the observation zt. Thus, we compute the leadership belief at time t by marginalizing b(ct) = b(xt, Ht) over xt: b(Ht) = Z xt b(xt, Ht)dxt. (10) Next, we make several assumptions to simplify the context transition probability. First, we assume conditional independence of xt and Ht given ct−1 and wt−1. While these values often evolve together, we can make this assumption if the state responds slowly to changes in leadership. In particular, if we select a sufficiently small sampling period ∆t, then any change in the state xt when Ht ̸= Ht−1 requires multiple time steps to observe. After this simplification, p {ct|ct−1, wt−1}=p {xt|ct−1, wt−1} p {Ht|ct−1, wt−1}. (11) The term p{xt|ct−1, wt−1} indicates that xt depends on the previous leader and the previous state and controls through the dynamics ft−1. The second term p{Ht|ct−1, wt−1} models how Ht depends on the previous state and controls. In the passing scenario, for example, we could encode a prior relationship between Ht and whichever vehicle is in front. However, establishing this type of prior is difficult [3], [4], so we leave it to user discretion if such knowledge is available. We do not encode any such relationship in this work. Instead, we make a second, stronger assumption that the leadership transition process for Ht occurs independently of state xt−1 and agent controls wt−1. With these two simplifications, our context transition probability becomes p {ct|ct−1, wt−1} = p {xt|ct−1, wt−1} p{Ht|Ht−1}. (12) We model the leadership transition probability p{Ht|Ht−1} as a two-state Markov Chain with transition likelihood p{Ht ̸= Ht−1|Ht−1} = ptrans. This assumption applies to a limited set of scenarios, though as mentioned, users may provide more complicated leadership transition models. Selecting a Filter. Due to the computational intractability of exactly evaluating Bayesian update rule (9), we use a particle filtering approach. Particle k has context c k t = [˜x k t , Hk t ] ⊺ . Particle filters use a measurement model to compute the expected observation for a state xt [26, Ch. 4]. Our measurement model h(˜x k t ; Hk t ) solves a Stackelberg game to generate simulated solution trajectories conditioned on the particle’s leader. In the measurement update, we compare a subset of the solution to the ground truth observations and update the likelihood of leadership using (9) and (10). We resample with replacement to eliminate unlikely particles when the effective number of particles, a metric that measures how well the particles represent the distribution, becomes low. We infer the leading agent based on the similarity of expected measurements, generated from Stackelberg games, to observations (a) Median ℓ∞ convergence metric, with 10th and 90th percentiles. (b) Stackelberg solution positions. Fig. 3. We run 100 SILQGames simulations on the non-LQ shepherd and sheep game. The simulations converge in a mean of 1133 and a standard deviation of 367 iterations. (a) shows the number of unconverged simulations and (b) shows the solution for one instance. of the ground truth. Since Stackelberg equilibria satisfy leadership condition (4), converged solutions let the filter observe leadership indirectly via the measurement model. The Stackelberg Measurement Model. We construct a measurement model that relates the leader Hk t−1 in particle k at time t − 1 with the expected state measurement at time t; in particular, we model the expected measurement from each particle as the output of Stackelberg game S Hk t−1 Ts (˜x k t−1 ), played from the previous particle state over horizon Ts. Experiments determine that we must configure Ts carefully, neither too short to provide relevant leadership information nor too long as to cause excessive latency. We call the solutions to these games Stackelberg measurement trajectories and select the state at time t, zˆ Hk t−1 t , as the expected measurement. In practice, playing a Stackelberg game from previous state xt−1 requires each particle to maintain xt−1 as additional context. To identify nominal strategies u (i),k=0 t−1:t+Ts−1 for the call to SILQGames within the measurement model, we require the user to define GetNominalTrajectory(Ts, c k 1:t−1 , w1:t−1) in a manner appropriate to the application, i.e., using previous particle contexts, a heuristic, etc. We describe one such heuristic in the appendix. After producing a measurement trajectory, we attach measurement uncertainty Σt to each state in it. Depending on the application, this step may incorporate uncertainty from sensors, processing, variation over time, etc. V. EXPERIMENTS & RESULTS We first introduce the two-agent LQ shepherd and sheep game [28] and a nonlinear, nonquadratic variant. We use these to validate SILQGames and the SLF. Finally, we run the SLF on realistic driving scenarios. The LQ Shepherd and Sheep Game. In the shepherd and sheep game, each agent state x (i) t = [p (i) x,t, v (i) x,t, p (i) y,t, v (i) y,t] ∈ R 4 includes 2D position and velocity in the horizontal and vertical directions and each agent controls its planar acceleration u (i) t ∈ R 2 . Agents’ states evolve according to (linear) double-integrator dynamics in each direction, p (i) x,t+1 = p (i) x,t + v (i) x,t∆t; v (i) x,t+1 = v (i) x,t + u (i) x,t∆t; p (i) y,t+1 = p (i) y,t + v (i) y,t∆t; v (i) y,t+1 = v (i) y,t + u (i) y,t∆t. The game state combines the agent states xt = [x (1) t , x (2) t ] ⊺ . Agents’ costs g (1) t  xt,u (1) t ,u (2) t  =  p (2) x,t2 +  p (2) y,t2 + ∥u (1) t ∥ 2 2 , (13) g (2) t (. . .) =  p (1) x,t − p (2) x,t2 +  p (1) y,t − p (2) y,t2 + ∥u (2) t ∥ 2 2 , (14) are quadratic in state and controls and incentivize “shepherd” A1 to minimize “sheep” A2’s distance to the origin (i.e., the barn) and A2 to minimize its distance to A1. Since the game is LQ, an analytic Stackelberg solution exists. Nonlinear, Nonquadratic (Non-LQ) Game. We form a nonlinear, nonquadratic variant of (13), (14) by using unicycle dynamics and modifying (13). Each agent state x (i) t = [p (i) x,t, p (i) y,t, ψ(i) t , v (i) t ] ⊺ ∈ R 4 contains 2D position, heading, and velocity and each agent Ai controls yaw rate ω (i) t ∈ R and longitudinal acceleration α (i) t ∈ R. Game state xt = [x (1) t , x (2) t ] ⊺ , where agents’ states evolve according to p (i) x,t+1 = p (i) x,t + ∆tv(i) t cos ψ (i) t ; p (i) y,t+1 = p (i) y,t + ∆tv(i) t sin ψ (i) t ; ψ (i) t+1 = ψ (i) t + ∆tω(i) t ; v (i) t+1 = v (i) t + ∆tα(i) t . The nonquadratic cost g (1′ ) t  xt,u (1) t ,u (2) t  = g (1) t (·, ·, ·) − log  s − p (2) x,t (15) − log  p (2) x,t − s  − log  s − p (2) y,t − log  p (2) y,t − s  adds log barrier terms to (13) which force A1 to keep A2’s position (p (2) x,t , p (2) y,t ) bounded within an origin-centered square of side length 2s. This nonquadratic cost remains convex. If A2 begins near the log boundary, then A1’s cost starts high, and A1 may display more aggressive control. A. SILQGames Validation To test convergence for non-LQ games, we run 100 simulations of SILQGames on the non-LQ shepherd and sheep game. In each simulation, we fix A1’s initial position at (2 m, 1 m) and vary A2’s initial position along the perimeter of a radius-√ 5 m circle. Both agents begin stationary and face toward the origin. The nominal strategies apply zero input. We specify additional parameters in the appendix. Analysis. The results in Fig. 3a indicate that all simulations converge. The median value of the convergence metric, shown with 10% and 90% percentile bounds, exhibits a generally decreasing trend. These results are consistent with our previous discussion on convergence, as SILQGames converges in every simulation, though without monotone decrease in the convergence criterion. In Fig. 3b, we report the solution for a particular (arbitrarily chosen) simulation. Both agents’ motion follows the incentive structure of the game: the distance between the two agents decreases, as does the distance from A2 to the origin. As expected, A1 exerts more control effort than A2 due to A2’s leadership role and A1’s incentive to constrain A2’s position. Finally, we note that A1’s motion (a) Mean leader probability with standard deviation. (b) Measurement trajectories at t = 0.54 s. (c) Measurement trajectories at t = 2.04 s. Fig. 4. We run 100 SLF simulations on analytic solutions to the LQ shepherd and sheep game. (a) indicates that the SLF initially misidentifies the leader but then identifies the leader correctly as A1 before becoming uncertain due to noise. (b) and (c) are associated with a particular simulation and show the Stackelberg measurement trajectories at t = 0.54 s and t = 2.04 s, respectively. The color of a particle’s measurement trajectory indicates leading agent Hk t−1 , and the insets show the expected measurements for each particle, the actual measurement, and the ground truth. changes sharply towards the end of its trajectory. Here, the unicycle comes to a stop and moves in reverse. These results demonstrate that, for a game with nonlinear dynamics and convex, nonquadratic costs, SILQGames converges to a solution that appears consistent with the dynamics and costs. Timing. We collect elapsed times for each iteration of 100 SILQGames simulations on AMD Ryzen 9 5900x 12-core processors. An iteration of SILQGames runs in a mean of 0.49 s with a standard deviation of 0.29 s. B. Leadership Filter Validation We validate the leadership filter on analytic solution trajectories of horizon Tsim for the LQ shepherd and sheep game played with leader LGT = A1. Since we generate the ground truth x GT 1:T with a known leader, a perfect filter should infer the true leader with consistently high confidence. Our results indicate that the SLF produces an observable signal for Stackelberg leadership, but (as one can expect) noise and measurement model configuration significantly affect performance. We simulate noisy game state measurements zt ∼ N (x GT t , Σ) and assume the SLF knows Σ. We list parameter values in the appendix. Analysis. In our results, the SLF produces the expected leadership probability for part of the simulation horizon. From 1.5−3.5 s in Fig. 4a, the SLF correctly infers A1 as the leader with high likelihood. We can interpret these leadership results using the measurement trajectories. Examining the expected measurements in Fig. 4c at 2.04 s, we note that the observations in this time range more closely match the measurement models generated with A1 as leader, which the SLF interprets as indicating leadership by A1. However, we also see complex behavior in Fig. 4a. First, the SLF initially misidentifies the leader as A2, as shown by Fig. 4b, because the Stackelberg measurement trajectories do not capture leadership information over the whole simulation horizon. Specifically, the measurement trajectories {h(˜x k t−1 , Hk t−1 )} are straight lines that roughly reduce the state costs of the shepherd and sheep, but do not capture the granularity of motion from the ground truth due to higher control costs over the short horizon Ts ≪ Tsim. Second, the SLF is completely uncertain after 4.5 s. Near the origin, the contribution of process noise to the motion outweighs the contribution of the dynamics, and the measurement noise is too uncertain to clarify the state. Thus, the measurement and process noise obfuscate the dynamics. From these results, we see that the SLF requires parameter Ts to be of sufficient length to capture the influence of leadership on the measurement trajectories. We note that the SLF is sensitive to noise as it infers leadership indirectly by comparing the observed motion with the expected motion of a Stackelberg leader. Thus, too little process noise may lead particles to converge to an incorrect trajectory, and too much may reduce the signal-to-noise ratio. Timing. The mean overall runtime for 100 simulations of an LQ game with 501 steps is 10.91 s with standard deviation 1.64 s. The mean and standard deviation SLF cycle runtimes for 50 particles and a 75-step measurement horizon are 0.82 s and 0.35 s. Self-driving vehicle applications require sub100 ms perception cycle latency [29], so our implementation is not real-time. However, straightforward though nontrivial optimizations can reduce latency below 100 ms, as demonstrated by [30], [31], which use fast particle filters with measurement models that involve solving dynamic games. C. Realistic Driving Scenarios We formulate passing and merging scenarios using realistic ground truth trajectories without a clear leader. We demonstrate that the SLF responds to changes in leadership, handles objectives that imperfectly model agent behavior, and that the results match right-of-way expectations. The dynamics and cost terms demonstrate that the SLF does not require LQ assumptions and works for nonconvex costs. Our results further indicate that SILQGames, used within the SLF, converges under these conditions. Each agent’s state evolves according to unicycle dynamics. The simulation runs for T steps at period ∆t = 0.05 s. We model stage cost g (i) t as a weighted sum of incentives g (i) j,t , g (i) t = M X (i) j=1 w (i) j g (i) j,t . (16) Weights {w (i) j } ⊂ R + specify the relative priorities of subobjectives. We define M(i) = 6 terms to incentivize driving behaviors corresponding to legal or safety considerations. g (i) 1,t = d(x (i) t , x (i),goal t ) (17a) g (i) 2,t = − log(∥p (i) t − p (j) t ∥ 2 2 − dc) ∀i ̸= j (17b) g (i) 3,t = − log(vm − |v (i) t |) − log(∆ψm − |ψ (i) t − ψr |) (17c) g (i) 4,t = (ω (i) t ) 2 + (α (i) t ) 2 (17d) g (i) 5,t = − log(∥p (i) t,llb − p (i) t ∥ 2 2 ) − log(∥p (i) t,rlb − p (i) t ∥ 2 2 ) (17e) g (i) 6,t = exp(−(1/2)(p (i) t,cl − p (i) t ) ⊺C −1 (p (i) t,cl − p (i) t )) (17f) Eq. (17a) requires a small distance between vehicle state x (i) t and goal state x (i),goal t . For this scenario, d(·, ·) is a weighted Euclidean distance. Eq. (17b) requires a minimum safety radius dc between the vehicles. Eq. (17c) requires obeying speed limit vm and avoiding excessive heading deviation ∆ψm from road direction ψr . Eq. (17d) incentivizes low control effort. Eq. (17e) enforces left and right lane boundaries p (i) t,llb, p (i) t,rlb, based on lane width ℓw. Eq. (17f) uses a (nonconvex) Gaussian function with covariance C to discourage crossing the center line p (i) t,cl. We specify these parameter values in the appendix. Lastly, we define the direction of motion as the y-direction and the transverse direction as the −x-direction to maintain a righthand coordinate frame. Passing Scenario. The passing scenario begins with A2 behind A1 and runs for 7.5 s. In the ground truth trajectories (Fig. 1), A2 initially follows A1 for 2.5 s, then passes in the other lane, and ends ahead of A1 in the initial lane. A1 drives along the lane at a constant velocity, applying no controls. We simulate the leadership filter on the passing maneuver. We expect A1 to start with a high leadership probability and for that probability to decrease once the passing maneuver begins, and vice versa for A2. In Fig. 1, the state estimate tracks the ground truth, indicating that the leadership filter captures the game dynamics. Since the SLF produces the expected trends in the state estimates and agents’ probabilities, our results show that Stackelberg leadership can match right-of-way expectations for scenarios without a ground truth leader. Moreover, the SLF responds appropriately to changing leadership dynamics over time. Finally, the result shows that SILQGames can handle nonconvex cost terms. Merging Scenario. The merging scenario involves three sections of road (see Fig. 5): two 30 m-long lanes separated by a barrier at x = 0 m, a merging segment that decreases from width 2ℓw to ℓw over 30 m of length, and a one lane road centered along x = 0 m. Both agents start in their own lanes, though A1 starts behind A2. In the ground truth, A2 merges before A1, which slows down to yield before merging. A2 delays its merge once it enters the merging segment. We construct the game played within the measurement model to incentivize each agent to merge quickly after entering the merging segment, so the cost we define for A2 does not exactly reflect its actual behavior. In Fig. 5, we simulate this merge with the leadership filter. We expect A2 to lead the interaction as it begins ahead and merges first. Given their objectives, we expect the agents’ measurement trajectories to merge quickly, and we see these Fig. 5. In this merge, A2 starts ahead in its lane and A1 yields to A2. We see a high leadership likelihood for A2, as expected because it merges first. The inset indicates the current probabilities with a vertical dashed line. trajectories quickly move toward the center of the merging segment. Nevertheless, the leadership filter’s state estimate tracks the ground truth, including A2’s delayed merge, and the SLF infers A2 as the leader. Thus, the results match our right-of-way expectations despite agent objectives that do not exactly describe the observed ground truth behavior. VI. DISCUSSION & LIMITATIONS We contribute SILQGames, an iterative algorithm to solve Stackelberg games with nonlinear dynamics and nonquadratic costs. Through empirical validation on non-LQ game scenarios, we show it reliably converges. We also introduce the Stackelberg Leadership Filter and apply it to noisy scenarios with known leaders and realistic driving situations. Results highlight the SLF’s ability to estimate leadership in long-horizon interactions with changing leadership and with objectives that do not exactly reflect observed agent behavior. Furthermore, we discuss the robustness of our method to the measurement horizon and noise. Future directions include extending SILQGames to N > 2 agents and overcoming combinatorial scaling challenges from the pairwise definition of Stackelberg leadership. Another critical direction involves establishing theoretical bounds on the number of SILQGames iterations. For the SLF, future work includes enabling real-time application using more efficient estimators and algorithmically adjusting the measurement horizon Ts to observe leadership dynamics over different horizons. Further work may also clarify when Stackelberg leadership appropriately models leadership",
		"summary": "Effectively predicting intent and behavior requires inferring leadership in multi-agent interactions. Dynamic games provide an expressive theoretical framework for modeling these interactions. Employing this framework, we propose a novel method to infer the leader in a two-agent game by observing the agents’ behavior in complex, long-horizon interactions. We make two contributions. First, we introduce an iterative algorithm that solves dynamic two-agent Stackelberg games with nonlinear dynamics and nonquadratic costs, and demonstrate that it consistently converges. Second, we propose the Stackelberg Leadership Filter (SLF), an online method for identifying the leading agent in interactive scenarios based on observations of the game interactions. We validate the leadership filter’s efficacy on simulated driving scenarios to demonstrate that the SLF can draw conclusions about leadership that match rightof-way expectations. ",
		"id": "UUID30"
	},
]